{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddb458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm.auto import tqdm\n",
    "from io import TextIOWrapper\n",
    "import pandas as pd\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a142b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP-файл скачан: C:\\Users\\Admin\\.cache\\huggingface\\hub\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\meta\\all.channels.csv.zip\n",
      "Распаковка в папку: data/posts/ajtkulov/meta\n",
      "Файлы внутри ZIP: ['all.channels.csv']\n",
      "Распаковка завершена\n",
      "\n",
      "Чтение CSV из файла: data/posts/ajtkulov/meta\\all.channels.csv\n",
      "\n",
      "УСПЕХ! Файл прочитан с кодировкой: utf-8-sig\n",
      "\n",
      "Колонки успешно переименованы в: link, name, description, category, message_id\n",
      "Количество каналов: 380,467\n",
      "Количество столбцов: 5\n",
      "Столбцы: ['link', 'name', 'description', 'category', 'message_id']\n",
      "\n",
      "Первые 10 каналов:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>message_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://tgstat.ru/channel/@premium</td>\n",
       "      <td>Telegram Premium</td>\n",
       "      <td>Telegram Premium – a subscription that unlocks...</td>\n",
       "      <td>Telegram</td>\n",
       "      <td>7980100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://tgstat.ru/channel/sOj9iDAtUkMyYWQy</td>\n",
       "      <td>Топор Live</td>\n",
       "      <td>Нейтрально, без пропаганды. Топор Live с быстр...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>4388359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://tgstat.ru/channel/@wewantyoutodothejob</td>\n",
       "      <td>WeWantYou</td>\n",
       "      <td>Канал для поиска исполнителей для разных задач...</td>\n",
       "      <td>Другое</td>\n",
       "      <td>4158678.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://tgstat.ru/channel/@leoday</td>\n",
       "      <td>Леонардо Дайвинчик</td>\n",
       "      <td>Бот знакомств @leomatchbot</td>\n",
       "      <td>Юмор и развлечение</td>\n",
       "      <td>4057819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://tgstat.ru/channel/@novosti_efir</td>\n",
       "      <td>Прямой Эфир • Новости</td>\n",
       "      <td>️Все самое важное в одном канале. Новости Росс...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3886208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://tgstat.ru/channel/@novosti_voinaa</td>\n",
       "      <td>СМИ Россия не Москва</td>\n",
       "      <td>Эруктации информпространства России и ее Окраин.</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3368394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://tgstat.ru/channel/@rian_ru</td>\n",
       "      <td>РИА Новости</td>\n",
       "      <td>Главные Новости РИА t.me/rian_ru</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3220976.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://tgstat.ru/channel/@invest_zonaa</td>\n",
       "      <td>INVEST ZONE</td>\n",
       "      <td>Привет! Я Руслан, с 2017 года торгую рынок кри...</td>\n",
       "      <td>Криптовалюты</td>\n",
       "      <td>3057506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://tgstat.ru/channel/@mash</td>\n",
       "      <td>Mash</td>\n",
       "      <td>Прислать новость, фото, видео, аудио, бересту:...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>2827677.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://tgstat.ru/channel/@crypto_drop_stukach</td>\n",
       "      <td>Дропы от Стукача</td>\n",
       "      <td>Все о крипто раздачах, прибыльных темах и абуз...</td>\n",
       "      <td>Шок-конент</td>\n",
       "      <td>2636192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             link                   name  \\\n",
       "0              https://tgstat.ru/channel/@premium       Telegram Premium   \n",
       "1      https://tgstat.ru/channel/sOj9iDAtUkMyYWQy             Топор Live   \n",
       "2  https://tgstat.ru/channel/@wewantyoutodothejob              WeWantYou   \n",
       "3               https://tgstat.ru/channel/@leoday     Леонардо Дайвинчик   \n",
       "4         https://tgstat.ru/channel/@novosti_efir  Прямой Эфир • Новости   \n",
       "5       https://tgstat.ru/channel/@novosti_voinaa   СМИ Россия не Москва   \n",
       "6              https://tgstat.ru/channel/@rian_ru            РИА Новости   \n",
       "7         https://tgstat.ru/channel/@invest_zonaa            INVEST ZONE   \n",
       "8                 https://tgstat.ru/channel/@mash                   Mash   \n",
       "9  https://tgstat.ru/channel/@crypto_drop_stukach       Дропы от Стукача   \n",
       "\n",
       "                                         description            category  \\\n",
       "0  Telegram Premium – a subscription that unlocks...            Telegram   \n",
       "1  Нейтрально, без пропаганды. Топор Live с быстр...             Новости   \n",
       "2  Канал для поиска исполнителей для разных задач...              Другое   \n",
       "3                         Бот знакомств @leomatchbot  Юмор и развлечение   \n",
       "4  ️Все самое важное в одном канале. Новости Росс...             Новости   \n",
       "5   Эруктации информпространства России и ее Окраин.             Новости   \n",
       "6                   Главные Новости РИА t.me/rian_ru             Новости   \n",
       "7  Привет! Я Руслан, с 2017 года торгую рынок кри...        Криптовалюты   \n",
       "8  Прислать новость, фото, видео, аудио, бересту:...             Новости   \n",
       "9  Все о крипто раздачах, прибыльных темах и абуз...          Шок-конент   \n",
       "\n",
       "   message_id  \n",
       "0   7980100.0  \n",
       "1   4388359.0  \n",
       "2   4158678.0  \n",
       "3   4057819.0  \n",
       "4   3886208.0  \n",
       "5   3368394.0  \n",
       "6   3220976.0  \n",
       "7   3057506.0  \n",
       "8   2827677.0  \n",
       "9   2636192.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Распределение по category (топ-15):\n",
      "category\n",
      "Новости                                                               25909\n",
      "Блоги                                                                 25599\n",
      "Другое                                                                20693\n",
      "Мода и красота                                                        17673\n",
      "Психология                                                            13740\n",
      "                                                                      ...  \n",
      "Политика|||Регион|||Свердловская область                                 19\n",
      "Политика|||Регион|||Самарская область                                    19\n",
      "Путешествия|||Регион|||Приморский край                                   18\n",
      "Новости|||Регион|||Кабардино-Балкарская Республика|||Новости и СМИ       18\n",
      "Политика|||Регион|||Пермский край                                        18\n",
      "Name: count, Length: 150, dtype: int64\n",
      "\n",
      "Уникальные категории сохранены в файл: data/posts/ajtkulov/meta\\unique_categories.csv\n",
      "Количество уникальных категорий: 1748\n",
      "Первые 20 уникальных категорий:\n",
      "                                     category\n",
      "0                                    Telegram\n",
      "1                                     Новости\n",
      "2                                      Другое\n",
      "3                          Юмор и развлечение\n",
      "4                                Криптовалюты\n",
      "5                                  Шок-конент\n",
      "6   Новости|||Регион|||Москва|||Новости и СМИ\n",
      "7                                    Политика\n",
      "8                                   Экономика\n",
      "9                                  Технологии\n",
      "10                                      Блоги\n",
      "11                                    Продажи\n",
      "12                             Видео и фильмы\n",
      "13                                       Игры\n",
      "14                          Софт и приложения\n",
      "15                                    Карьера\n",
      "16                               Букмекерство\n",
      "17                                     Бизнес\n",
      "18                                Образование\n",
      "19                               Для взрослых\n",
      "CSV лежит здесь: data/posts/ajtkulov/meta\\all.channels.csv\n"
     ]
    }
   ],
   "source": [
    "# Параметры\n",
    "repo_id = \"ajtkulov/telegram-ru\"\n",
    "meta_zip_filename = \"meta/all.channels.csv.zip\"          # путь в репозитории\n",
    "output_meta_dir = \"data/posts/ajtkulov/meta\"             # куда сохраняем\n",
    "\n",
    "os.makedirs(output_meta_dir, exist_ok=True)\n",
    "\n",
    "# Шаг 1: Скачивание ZIP-файла\n",
    "try:\n",
    "    zip_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=meta_zip_filename,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    print(f\"ZIP-файл скачан: {zip_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка скачивания: {e}\")\n",
    "    raise\n",
    "\n",
    "# Шаг 2: Распаковка в нужную папку\n",
    "print(\"Распаковка в папку:\", output_meta_dir)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    inner_files = zip_ref.namelist()\n",
    "    print(f\"Файлы внутри ZIP: {inner_files}\")\n",
    "    \n",
    "    if not inner_files:\n",
    "        raise ValueError(\"ZIP-файл пустой\")\n",
    "    \n",
    "    zip_ref.extractall(output_meta_dir)\n",
    "    print(\"Распаковка завершена\")\n",
    "\n",
    "# Шаг 3: Поиск распакованного CSV-файла\n",
    "csv_path = None\n",
    "for file in os.listdir(output_meta_dir):\n",
    "    if file.lower().endswith('.csv'):\n",
    "        csv_path = os.path.join(output_meta_dir, file)\n",
    "        break\n",
    "\n",
    "if not csv_path:\n",
    "    raise FileNotFoundError(\"CSV-файл не найден после распаковки. Проверьте папку.\")\n",
    "\n",
    "print(f\"\\nЧтение CSV из файла: {csv_path}\")\n",
    "\n",
    "# Шаг 4: Чтение с обработкой всех типичных проблем\n",
    "encodings = ['utf-8-sig', 'cp1251', 'utf-8', 'latin1', 'iso-8859-1']\n",
    "df_channels = None\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        df_channels = pd.read_csv(\n",
    "            csv_path,\n",
    "            encoding=encoding,\n",
    "            sep='\\t',                  # табуляция (как у вас)\n",
    "            on_bad_lines='skip',       # пропуск битых строк\n",
    "            low_memory=False,\n",
    "            encoding_errors='replace'  # замена кракозябр\n",
    "        )\n",
    "        print(f\"\\nУСПЕХ! Файл прочитан с кодировкой: {encoding}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Попытка с {encoding} провалилась: {str(e)}\")\n",
    "\n",
    "if df_channels is None:\n",
    "    raise ValueError(\"Не удалось прочитать CSV ни одной кодировкой. Возможно файл повреждён.\")\n",
    "\n",
    "# ← Добавляем названия колонок\n",
    "column_names = [\"link\", \"name\", \"description\", \"category\", \"message_id\"]\n",
    "if len(df_channels.columns) == len(column_names):\n",
    "    df_channels.columns = column_names\n",
    "    print(\"\\nКолонки успешно переименованы в: link, name, description, category, message_id\")\n",
    "else:\n",
    "    print(f\"\\nВнимание! Количество столбцов ({len(df_channels.columns)}) не равно ожидаемому ({len(column_names)}).\")\n",
    "    print(\"Колонки НЕ переименованы. Текущие:\", df_channels.columns.tolist())\n",
    "\n",
    "\n",
    "# Общая статистика\n",
    "print(f\"Количество каналов: {len(df_channels):,}\")\n",
    "print(f\"Количество столбцов: {len(df_channels.columns)}\")\n",
    "print(\"Столбцы:\", df_channels.columns.tolist())\n",
    "\n",
    "# Первые 10 строк\n",
    "print(\"\\nПервые 10 каналов:\")\n",
    "display(df_channels.head(10))\n",
    "\n",
    "# Распределение категорий\n",
    "if 'category' in df_channels.columns:\n",
    "    print(\"\\nРаспределение по category (топ-15):\")\n",
    "    print(df_channels['category'].value_counts().head(150))\n",
    "else:\n",
    "    print(\"\\nКолонка 'category' не найдена\")\n",
    "\n",
    "if 'category' in df_channels.columns:\n",
    "    unique_categories = df_channels['category'].dropna().unique()\n",
    "    unique_df = pd.DataFrame(unique_categories, columns=['category'])\n",
    "    \n",
    "    unique_csv_path = os.path.join(output_meta_dir, \"unique_categories.csv\")\n",
    "    unique_df.to_csv(unique_csv_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\nУникальные категории сохранены в файл: {unique_csv_path}\")\n",
    "    print(f\"Количество уникальных категорий: {len(unique_categories)}\")\n",
    "    print(\"Первые 20 уникальных категорий:\")\n",
    "    print(unique_df.head(20))\n",
    "else:\n",
    "    print(\"\\nНе удалось сохранить категории — колонка 'category' отсутствует\")\n",
    "\n",
    "print(\"CSV лежит здесь:\", csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7631edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Чтение файла: data/posts/ajtkulov/meta/unique_categories.csv\n",
      "Всего уникальных категорий в исходном файле: 1,748\n",
      "Первые 10 категорий:\n",
      "                                    category\n",
      "0                                   Telegram\n",
      "1                                    Новости\n",
      "2                                     Другое\n",
      "3                         Юмор и развлечение\n",
      "4                               Криптовалюты\n",
      "5                                 Шок-конент\n",
      "6  Новости|||Регион|||Москва|||Новости и СМИ\n",
      "7                                   Политика\n",
      "8                                  Экономика\n",
      "9                                 Технологии\n",
      "\n",
      "Запрещённые категории / подстроки (удаляем их):\n",
      " - Новости\n",
      " - Политика\n",
      " - Шок-контент\n",
      " - Шок-конент\n",
      " - Darknet\n",
      " - Для взрослых\n",
      " - Эротика\n",
      " - Право\n",
      " - Религия\n",
      " - Инстаграм\n",
      " - Другое\n",
      " - Telegram\n",
      " - Новости\n",
      " - Политика\n",
      " - Шок\n",
      " - Darknet\n",
      "\n",
      "После фильтрации осталось категорий: 1,417\n",
      "\n",
      "Первые 20 оставшихся категорий:\n",
      "              category\n",
      "3   Юмор и развлечение\n",
      "4         Криптовалюты\n",
      "8            Экономика\n",
      "9           Технологии\n",
      "10               Блоги\n",
      "11             Продажи\n",
      "12      Видео и фильмы\n",
      "13                Игры\n",
      "14   Софт и приложения\n",
      "15             Карьера\n",
      "16        Букмекерство\n",
      "17              Бизнес\n",
      "18         Образование\n",
      "20         Путешествия\n",
      "22     Картинки и фото\n",
      "23      Мода и красота\n",
      "24      Позновательное\n",
      "25              Музыка\n",
      "26        Семья и дети\n",
      "28     Еда и кулинария\n",
      "\n",
      "Отфильтрованный файл сохранён: data/posts/ajtkulov/meta/filtered_categories.csv\n",
      "Теперь в нём только нишевые / полезные категории\n"
     ]
    }
   ],
   "source": [
    "# Ячейка: Фильтрация категорий (убираем ненужные, включая подстроки)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Путь к файлу с уникальными категориями\n",
    "input_csv = 'data/posts/ajtkulov/meta/unique_categories.csv'\n",
    "\n",
    "# Путь для сохранения отфильтрованного файла\n",
    "output_csv = 'data/posts/ajtkulov/meta/filtered_categories.csv'\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\"Чтение файла: {input_csv}\")\n",
    "\n",
    "# Читаем файл (учитывая BOM и кодировку)\n",
    "df = pd.read_csv(input_csv, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Всего уникальных категорий в исходном файле: {len(df):,}\")\n",
    "print(\"Первые 10 категорий:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Список запрещённых категорий и подстрок (расширенный)\n",
    "FORBIDDEN_PATTERNS = [\n",
    "    'Новости', 'Политика', 'Шок-контент', 'Шок-конент', 'Darknet',\n",
    "    'Для взрослых', 'Эротика', 'Право', 'Религия', 'Инстаграм',\n",
    "    'Другое', 'Telegram',  # слишком общие\n",
    "]\n",
    "\n",
    "# Дополнительно: любые категории, содержащие эти слова (даже с |||)\n",
    "FORBIDDEN_SUBSTRINGS = ['Новости', 'Политика', 'Шок', 'Darknet']\n",
    "\n",
    "print(\"\\nЗапрещённые категории / подстроки (удаляем их):\")\n",
    "for p in FORBIDDEN_PATTERNS + FORBIDDEN_SUBSTRINGS:\n",
    "    print(f\" - {p}\")\n",
    "\n",
    "# Фильтрация:\n",
    "# 1. Точное совпадение с запрещённым списком\n",
    "# 2. Содержит любую запрещённую подстроку\n",
    "mask = (\n",
    "    df['category'].isin(FORBIDDEN_PATTERNS) |\n",
    "    df['category'].str.contains('|'.join(FORBIDDEN_SUBSTRINGS), na=False, regex=True)\n",
    ")\n",
    "\n",
    "filtered_df = df[~mask].copy()\n",
    "\n",
    "# Убираем дубликаты и NaN\n",
    "filtered_df = filtered_df.dropna(subset=['category']).drop_duplicates(subset=['category'])\n",
    "\n",
    "print(f\"\\nПосле фильтрации осталось категорий: {len(filtered_df):,}\")\n",
    "\n",
    "print(\"\\nПервые 20 оставшихся категорий:\")\n",
    "print(filtered_df.head(20))\n",
    "\n",
    "# Сохранение отфильтрованного файла\n",
    "filtered_df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nОтфильтрованный файл сохранён: {output_csv}\")\n",
    "print(\"Теперь в нём только нишевые / полезные категории\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b713148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Чтение файла: data/posts/ajtkulov/meta/all.channels.csv\n",
      "Файл успешно прочитан. Строк: 380,467\n",
      "Первый столбец не имеет имени — переименовываем в 'link'\n",
      "\n",
      "Очистка первого столбца...\n",
      "\n",
      "Первые 10 строк после очистки:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>Топор 18+</th>\n",
       "      <th>Самый популярный русскоязычный Telegram канал.</th>\n",
       "      <th>Шок-конент</th>\n",
       "      <th>8179938</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>premium</td>\n",
       "      <td>Telegram Premium</td>\n",
       "      <td>Telegram Premium – a subscription that unlocks...</td>\n",
       "      <td>Telegram</td>\n",
       "      <td>7980100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sOj9iDAtUkMyYWQy</td>\n",
       "      <td>Топор Live</td>\n",
       "      <td>Нейтрально, без пропаганды. Топор Live с быстр...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>4388359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wewantyoutodothejob</td>\n",
       "      <td>WeWantYou</td>\n",
       "      <td>Канал для поиска исполнителей для разных задач...</td>\n",
       "      <td>Другое</td>\n",
       "      <td>4158678.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leoday</td>\n",
       "      <td>Леонардо Дайвинчик</td>\n",
       "      <td>Бот знакомств @leomatchbot</td>\n",
       "      <td>Юмор и развлечение</td>\n",
       "      <td>4057819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>novosti_efir</td>\n",
       "      <td>Прямой Эфир • Новости</td>\n",
       "      <td>️Все самое важное в одном канале. Новости Росс...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3886208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>novosti_voinaa</td>\n",
       "      <td>СМИ Россия не Москва</td>\n",
       "      <td>Эруктации информпространства России и ее Окраин.</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3368394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rian_ru</td>\n",
       "      <td>РИА Новости</td>\n",
       "      <td>Главные Новости РИА t.me/rian_ru</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3220976.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>invest_zonaa</td>\n",
       "      <td>INVEST ZONE</td>\n",
       "      <td>Привет! Я Руслан, с 2017 года торгую рынок кри...</td>\n",
       "      <td>Криптовалюты</td>\n",
       "      <td>3057506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mash</td>\n",
       "      <td>Mash</td>\n",
       "      <td>Прислать новость, фото, видео, аудио, бересту:...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>2827677.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>crypto_drop_stukach</td>\n",
       "      <td>Дропы от Стукача</td>\n",
       "      <td>Все о крипто раздачах, прибыльных темах и абуз...</td>\n",
       "      <td>Шок-конент</td>\n",
       "      <td>2636192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  link              Топор 18+  \\\n",
       "0              premium       Telegram Premium   \n",
       "1     sOj9iDAtUkMyYWQy             Топор Live   \n",
       "2  wewantyoutodothejob              WeWantYou   \n",
       "3               leoday     Леонардо Дайвинчик   \n",
       "4         novosti_efir  Прямой Эфир • Новости   \n",
       "5       novosti_voinaa   СМИ Россия не Москва   \n",
       "6              rian_ru            РИА Новости   \n",
       "7         invest_zonaa            INVEST ZONE   \n",
       "8                 mash                   Mash   \n",
       "9  crypto_drop_stukach       Дропы от Стукача   \n",
       "\n",
       "      Самый популярный русскоязычный Telegram канал.          Шок-конент  \\\n",
       "0  Telegram Premium – a subscription that unlocks...            Telegram   \n",
       "1  Нейтрально, без пропаганды. Топор Live с быстр...             Новости   \n",
       "2  Канал для поиска исполнителей для разных задач...              Другое   \n",
       "3                         Бот знакомств @leomatchbot  Юмор и развлечение   \n",
       "4  ️Все самое важное в одном канале. Новости Росс...             Новости   \n",
       "5   Эруктации информпространства России и ее Окраин.             Новости   \n",
       "6                   Главные Новости РИА t.me/rian_ru             Новости   \n",
       "7  Привет! Я Руслан, с 2017 года торгую рынок кри...        Криптовалюты   \n",
       "8  Прислать новость, фото, видео, аудио, бересту:...             Новости   \n",
       "9  Все о крипто раздачах, прибыльных темах и абуз...          Шок-конент   \n",
       "\n",
       "     8179938  \n",
       "0  7980100.0  \n",
       "1  4388359.0  \n",
       "2  4158678.0  \n",
       "3  4057819.0  \n",
       "4  3886208.0  \n",
       "5  3368394.0  \n",
       "6  3220976.0  \n",
       "7  3057506.0  \n",
       "8  2827677.0  \n",
       "9  2636192.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Очищенный файл сохранён: data/posts/ajtkulov/meta/all_channels_clean.csv\n",
      "Первый столбец теперь содержит только имена каналов (без https и @)\n"
     ]
    }
   ],
   "source": [
    "# Ячейка: Очистка первого столбца (оставляем только имя канала)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Путь к исходному файлу\n",
    "input_file = 'data/posts/ajtkulov/meta/all.channels.csv'\n",
    "\n",
    "# Путь для сохранения очищенного файла\n",
    "output_file = 'data/posts/ajtkulov/meta/all_channels_clean.csv'\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\"Чтение файла: {input_file}\")\n",
    "\n",
    "# Читаем CSV (учитываем возможные проблемы с кодировкой и разделителями)\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        input_file,\n",
    "        encoding='utf-8-sig',\n",
    "        sep='\\t',                  # если табуляция\n",
    "        on_bad_lines='skip',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(f\"Файл успешно прочитан. Строк: {len(df):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка чтения: {e}\")\n",
    "    # Попробуем другую кодировку\n",
    "    df = pd.read_csv(\n",
    "        input_file,\n",
    "        encoding='cp1251',\n",
    "        sep='\\t',\n",
    "        on_bad_lines='skip',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(\"Успешно прочитано с cp1251\")\n",
    "\n",
    "# Проверяем, есть ли первый столбец (по умолчанию он без имени — берём по индексу)\n",
    "if df.columns[0].startswith('https://tgstat.ru'):\n",
    "    print(\"Первый столбец не имеет имени — переименовываем в 'link'\")\n",
    "    df = df.rename(columns={df.columns[0]: 'link'})\n",
    "\n",
    "# Очистка первого столбца\n",
    "def clean_channel_link(link):\n",
    "    if pd.isna(link):\n",
    "        return link\n",
    "    link = str(link).strip()\n",
    "    # Убираем префикс https://tgstat.ru/channel/\n",
    "    if link.startswith('https://tgstat.ru/channel/'):\n",
    "        link = link.replace('https://tgstat.ru/channel/', '')\n",
    "    # Убираем @ в начале, если остался\n",
    "    if link.startswith('@'):\n",
    "        link = link[1:]\n",
    "    return link\n",
    "\n",
    "print(\"\\nОчистка первого столбца...\")\n",
    "df['link'] = df['link'].apply(clean_channel_link)\n",
    "\n",
    "# Показываем результат\n",
    "print(\"\\nПервые 10 строк после очистки:\")\n",
    "display(df.head(10))\n",
    "\n",
    "# Сохранение нового файла\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nОчищенный файл сохранён: {output_file}\")\n",
    "print(f\"Первый столбец теперь содержит только имена каналов (без https и @)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af27f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружен прогресс: 2 файлов уже обработано\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26afe9fa7764402b99d204eed658d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка ZIP-файлов:   0%|          | 0/359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tg.0.zip уже обработан — пропуск\n",
      "\n",
      "tg.1.zip уже обработан — пропуск\n",
      "\n",
      "=== Обработка tg.2.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a6a8adfdc54b698b659bb131a4e21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.2.zip:  18%|#8        | 241M/1.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Обработка \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Шаг 1: Скачивание (потоково, но hf_hub_download скачивает целиком)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m     48\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m     49\u001b[0m         filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     51\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mСкачано в: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Шаг 2: Потоковая обработка ZIP\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1012\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1013\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1014\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1017\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1018\u001b[0m         headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[0;32m   1019\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1020\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1021\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1023\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1024\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\file_download.py:1168\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1168\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1169\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1170\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[0;32m   1171\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1172\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1173\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1174\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1175\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1176\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1177\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[0;32m   1178\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[0;32m   1179\u001b[0m     )\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1181\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\file_download.py:1735\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[0;32m   1728\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n\u001b[0;32m   1729\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1730\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1732\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1733\u001b[0m             )\n\u001b[1;32m-> 1735\u001b[0m         http_get(\n\u001b[0;32m   1736\u001b[0m             url_to_download,\n\u001b[0;32m   1737\u001b[0m             f,\n\u001b[0;32m   1738\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1739\u001b[0m             resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1740\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1741\u001b[0m             expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1742\u001b[0m         )\n\u001b[0;32m   1744\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1745\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\file_download.py:493\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    491\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    495\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\urllib3\\response.py:1091\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1090\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1091\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1094\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\urllib3\\response.py:980\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 980\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[0;32m    982\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\urllib3\\response.py:904\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    901\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\urllib3\\response.py:887\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Параметры (настройте под себя)\n",
    "repo_id = \"ajtkulov/telegram-ru\"\n",
    "output_dir = \"data/posts/ajtkulov\"           # Папка для результатов (CSV/JSONL)\n",
    "cache_dir = os.path.join(output_dir, \"cache\")  # Кэш для ZIP (можно удалять после обработки)\n",
    "progress_file = os.path.join(output_dir, \"progress.json\")  # Для паузы/продолжения\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"csv\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"jsonl\"), exist_ok=True)\n",
    "\n",
    "# Критерии фильтра (пример; настройте)\n",
    "min_text_length = 100\n",
    "keywords = []  # Если пусто — без фильтра по словам; иначе содержит хотя бы одно\n",
    "min_date = \"2020-01-01\"  # Формат YYYY-MM-DD\n",
    "\n",
    "# Список всех файлов\n",
    "file_names = [f\"tg.{i}.zip\" for i in range(359)]  # 0 to 112\n",
    "\n",
    "# Для теста: file_names = file_names[:3]  # Только первые 3\n",
    "\n",
    "# Загрузка прогресса (какие ZIP уже обработаны)\n",
    "processed_files = set()\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as pf:\n",
    "        processed_files = set(json.load(pf))\n",
    "    print(f\"Загружен прогресс: {len(processed_files)} файлов уже обработано\")\n",
    "\n",
    "# Основной цикл: последовательная обработка\n",
    "for filename in tqdm(file_names, desc=\"Обработка ZIP-файлов\"):\n",
    "    if filename in processed_files:\n",
    "        print(f\"\\n{filename} уже обработан — пропуск\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n=== Обработка {filename} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Шаг 1: Скачивание (потоково, но hf_hub_download скачивает целиком)\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=f\"data/{filename}\",\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        print(f\"Скачано в: {file_path}\")\n",
    "        \n",
    "        # Шаг 2: Потоковая обработка ZIP\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            inner_files = [f for f in zip_ref.namelist() if not f.endswith('/') and not f.endswith('.done')]\n",
    "            print(f\"Файлов внутри: {len(inner_files)}\")\n",
    "            \n",
    "            filtered_posts = []  # Временный буфер для этого ZIP (маленький, чтобы не жрать RAM)\n",
    "            \n",
    "            for inner_path in tqdm(inner_files, desc=\"Файлы внутри ZIP\", leave=False):\n",
    "                with zip_ref.open(inner_path) as f:\n",
    "                    # Потоковое чтение строк\n",
    "                    for line in TextIOWrapper(f, encoding='utf-8', errors='ignore'):\n",
    "                        line = line.strip()\n",
    "                        if not line: continue\n",
    "                        \n",
    "                        try:\n",
    "                            obj = json.loads(line)\n",
    "                            text = obj.get('text', '')\n",
    "                            date = obj.get('date', '')\n",
    "                            \n",
    "                            # Фильтр\n",
    "                            if (len(text) >= min_text_length and\n",
    "                                (not keywords or any(kw.lower() in text.lower() for kw in keywords)) and\n",
    "                                date >= min_date):\n",
    "                                filtered_posts.append(obj)\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue  # Пропуск битых строк\n",
    "            \n",
    "            # Шаг 3: Сохранение отфильтрованных из этого ZIP\n",
    "            if filtered_posts:\n",
    "                df = pd.DataFrame(filtered_posts)\n",
    "                batch_name = filename.replace('.zip', '')\n",
    "                \n",
    "                # CSV\n",
    "                csv_path = os.path.join(output_dir, \"csv\", f\"{batch_name}.csv\")\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "                print(f\"CSV: {csv_path} ({len(df):,} постов)\")\n",
    "                \n",
    "                # JSONL\n",
    "                jsonl_path = os.path.join(output_dir, \"jsonl\", f\"{batch_name}.jsonl\")\n",
    "                with open(jsonl_path, 'w', encoding='utf-8') as jf:\n",
    "                    for post in filtered_posts:\n",
    "                        jf.write(json.dumps(post, ensure_ascii=False) + '\\n')\n",
    "                print(f\"JSONL: {jsonl_path} ({len(filtered_posts):,} постов)\")\n",
    "            else:\n",
    "                print(\"Нет подходящих постов в этом ZIP\")\n",
    "        \n",
    "        # Опционально: удалить ZIP после обработки\n",
    "        os.remove(file_path)\n",
    "        print(f\"ZIP удалён: {file_path}\")\n",
    "        \n",
    "        # Сохранить прогресс\n",
    "        processed_files.add(filename)\n",
    "        with open(progress_file, 'w') as pf:\n",
    "            json.dump(list(processed_files), pf)\n",
    "        print(\"Прогресс сохранён\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка с {filename}: {e}\")\n",
    "        # Здесь можно добавить паузу или break, но код продолжит с следующим\n",
    "\n",
    "print(\"\\nОбработка завершена!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46492ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c3471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c3863ef6fb45d7a648e53f21bc3e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка ZIP-файлов:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== tg.0.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055996962b5a4e438b9ffd197b562ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.0.zip:  30%|###       | 682M/2.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачано: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.0.zip\n",
      "Внутри ZIP файлов: 891\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0626275976314c2da7406a72f007e10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Внутренние файлы:   0%|          | 0/891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Достигнут глобальный лимит — прерываем\n",
      "\n",
      "Финальный отбор постов...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc805bca2f64008915b45bc9b03cd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Отбор по каналам:   0%|          | 0/295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Отобрано постов: 1,450\n",
      "Финальный CSV: data/posts/ajtkulov/selected\\selected_posts.csv (1,450 строк)\n",
      "Финальный JSONL: data/posts/ajtkulov/selected\\selected_posts.jsonl (1,450 записей)\n",
      "Форматированный JSON: data/posts/ajtkulov/selected\\selected_posts_pretty.json\n",
      "\n",
      "Готово!\n"
     ]
    }
   ],
   "source": [
    "# ЯЧЕЙКА: Стратифицированный отбор постов (5 на канал, топ по просмотрам, длина 150–400)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm.auto import tqdm\n",
    "from io import TextIOWrapper\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# ==============================================\n",
    "# НАСТРОЙКИ (изменяйте здесь)\n",
    "# ==============================================\n",
    "\n",
    "repo_id = \"ajtkulov/telegram-ru\"\n",
    "\n",
    "output_dir = \"data/posts/ajtkulov/selected\"               # куда сохранять финальный результат\n",
    "cache_dir = os.path.join(output_dir, \"cache\")             # временный кэш для ZIP\n",
    "progress_file = os.path.join(output_dir, \"progress.json\") # какие ZIP уже обработаны\n",
    "stats_file = os.path.join(output_dir, \"channel_top_posts.json\")  # топ-посты по каналам\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Запрещённые категории (не берём посты из этих каналов)\n",
    "forbidden_categories = [\n",
    "    'Новости', 'Новости и СМИ', 'Политика', 'Шок-контент',\n",
    "    # Добавьте сюда всё, что считаете новостным/мусорным\n",
    "]\n",
    "\n",
    "# Желаемое количество постов на канал\n",
    "MAX_POSTS_PER_CHANNEL = 5\n",
    "\n",
    "# Фильтр по длине текста\n",
    "MIN_TEXT_LEN = 150\n",
    "MAX_TEXT_LEN = 400\n",
    "\n",
    "# Глобальный лимит записей (не превысим)\n",
    "MAX_TOTAL_RECORDS = 700000\n",
    "\n",
    "# Список ZIP-файлов (0–112, но можно ограничить)\n",
    "file_names = [f\"tg.{i}.zip\" for i in range(230)]\n",
    "# file_names = file_names[:5]  # ← для теста\n",
    "\n",
    "# ==============================================\n",
    "# Вспомогательные структуры\n",
    "# ==============================================\n",
    "\n",
    "# channel → list of (views, text, obj) — храним только топ-5 по просмотрам\n",
    "channel_top_posts = {}\n",
    "\n",
    "# Загрузка существующей статистики, если есть\n",
    "if os.path.exists(stats_file):\n",
    "    with open(stats_file, 'r', encoding='utf-8') as f:\n",
    "        channel_top_posts = json.load(f)\n",
    "    print(f\"Загружено статистики по {len(channel_top_posts):,} каналам\")\n",
    "\n",
    "# Прогресс: какие ZIP уже обработаны\n",
    "processed_files = set()\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as pf:\n",
    "        processed_files = set(json.load(pf))\n",
    "    print(f\"Уже обработано ZIP-файлов: {len(processed_files)}\")\n",
    "\n",
    "# ==============================================\n",
    "# Основной цикл: чтение и отбор\n",
    "# ==============================================\n",
    "\n",
    "total_selected = 0\n",
    "\n",
    "for filename in tqdm(file_names, desc=\"Обработка ZIP-файлов\"):\n",
    "    if filename in processed_files:\n",
    "        print(f\"{filename} уже обработан — пропуск\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n=== {filename} ===\")\n",
    "    \n",
    "    try:\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=f\"data/{filename}\",\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        print(f\"Скачано: {file_path}\")\n",
    "        \n",
    "        with zipfile.ZipFile(file_path, 'r') as z:\n",
    "            inner_files = [f for f in z.namelist() if not f.endswith('/') and not f.endswith('.done')]\n",
    "            print(f\"Внутри ZIP файлов: {len(inner_files)}\")\n",
    "            \n",
    "            for inner_path in tqdm(inner_files, desc=\"Внутренние файлы\", leave=False):\n",
    "                with z.open(inner_path) as f:\n",
    "                    for line in TextIOWrapper(f, encoding='utf-8', errors='ignore'):\n",
    "                        line = line.strip()\n",
    "                        if not line: continue\n",
    "                        \n",
    "                        try:\n",
    "                            obj = json.loads(line)\n",
    "                            channel = obj.get('channel')\n",
    "                            text = obj.get('text', '')\n",
    "                            views = obj.get('views', 0)  # или 'view_count', 'views_count' — проверьте\n",
    "                            if not isinstance(views, (int, float)):\n",
    "                                views = 0\n",
    "                            \n",
    "                            if not channel or not text:\n",
    "                                continue\n",
    "                            \n",
    "                            # Фильтр по длине\n",
    "                            if not (MIN_TEXT_LEN <= len(text) <= MAX_TEXT_LEN):\n",
    "                                continue\n",
    "                            \n",
    "                            # Если канал уже в запрещённых — пропускаем\n",
    "                            # (здесь нужен df_channels из мета-CSV — подключаем ниже)\n",
    "                            \n",
    "                            # Сохраняем пост для канала\n",
    "                            if channel not in channel_top_posts:\n",
    "                                channel_top_posts[channel] = []\n",
    "                            \n",
    "                            channel_top_posts[channel].append((views, text, obj))\n",
    "                            \n",
    "                            total_selected += 1\n",
    "                            if total_selected >= MAX_TOTAL_RECORDS * 1.5:  # небольшой запас\n",
    "                                print(\"Достигнут глобальный лимит — прерываем\")\n",
    "                                raise StopIteration\n",
    "                                \n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "        \n",
    "        # После обработки ZIP — сохраняем прогресс\n",
    "        processed_files.add(filename)\n",
    "        with open(progress_file, 'w', encoding='utf-8') as pf:\n",
    "            json.dump(list(processed_files), pf, indent=2)  # добавил indent для читаемости\n",
    "        \n",
    "        # Сохраняем промежуточную статистику топ-постов\n",
    "        with open(stats_file, 'w', encoding='utf-8') as sf:\n",
    "            # Форматируем JSON для читаемости\n",
    "            formatted_stats = {}\n",
    "            for channel, posts in channel_top_posts.items():\n",
    "                formatted_stats[channel] = posts\n",
    "            \n",
    "            json.dump(formatted_stats, sf, ensure_ascii=False, indent=2)  # добавил indent\n",
    "        \n",
    "        # Удаляем ZIP, чтобы не занимать место\n",
    "        os.remove(file_path)\n",
    "        print(f\"ZIP удалён: {file_path}\")\n",
    "        \n",
    "    except StopIteration:\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка {filename}: {e}\")\n",
    "\n",
    "# ==============================================\n",
    "# Финальный отбор: 5 постов на канал, топ по просмотрам\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\nФинальный отбор постов...\")\n",
    "\n",
    "selected_posts = []\n",
    "\n",
    "for channel, post_list in tqdm(channel_top_posts.items(), desc=\"Отбор по каналам\"):\n",
    "    # Сортируем по просмотрам descending\n",
    "    post_list.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Берём не более 5\n",
    "    take = min(5, len(post_list))\n",
    "    for _, _, obj in post_list[:take]:\n",
    "        selected_posts.append(obj)\n",
    "        \n",
    "        if len(selected_posts) >= MAX_TOTAL_RECORDS:\n",
    "            break\n",
    "    \n",
    "    if len(selected_posts) >= MAX_TOTAL_RECORDS:\n",
    "        break\n",
    "\n",
    "print(f\"\\nОтобрано постов: {len(selected_posts):,}\")\n",
    "\n",
    "# ==============================================\n",
    "# Сохранение финального результата\n",
    "# ==============================================\n",
    "\n",
    "if selected_posts:\n",
    "    df_final = pd.DataFrame(selected_posts)\n",
    "    \n",
    "    # Сохраняем в CSV\n",
    "    csv_path = os.path.join(output_dir, \"selected_posts.csv\")\n",
    "    df_final.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Финальный CSV: {csv_path} ({len(df_final):,} строк)\")\n",
    "    \n",
    "    # Сохраняем в JSONL (одна строка = один пост)\n",
    "    jsonl_path = os.path.join(output_dir, \"selected_posts.jsonl\")\n",
    "    with open(jsonl_path, 'w', encoding='utf-8') as jf:\n",
    "        for post in selected_posts:\n",
    "            jf.write(json.dumps(post, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Финальный JSONL: {jsonl_path} ({len(selected_posts):,} записей)\")\n",
    "    \n",
    "    # Также сохраняем в красивый JSON файл для удобства чтения\n",
    "    json_path = os.path.join(output_dir, \"selected_posts_pretty.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as jf:\n",
    "        json.dump(selected_posts, jf, ensure_ascii=False, indent=2)\n",
    "    print(f\"Форматированный JSON: {json_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Не удалось отобрать ни одного поста\")\n",
    "\n",
    "print(\"\\nГотово!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "510b5ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка информации о каналах и категориях...\n",
      "Загружено каналов: 380468\n",
      "Загружено категорий для фильтрации: 1417\n",
      "Создан словарь категорий для 380468 каналов\n",
      "Разрешено 1417 категорий\n",
      "Первые 10 разрешенных категорий:\n",
      "  1. Регион|||Амурская область|||Технологии\n",
      "  2. Telegram|||Регион|||Приморский край\n",
      "  3. Образование|||Регион|||Республика Башкортостан\n",
      "  4. Медицина|||Регион|||Ивановская область\n",
      "  5. Здоровье и фитнес|||Регион|||Томская область|||Здоровье и Фитнес\n",
      "  6. Блоги|||Регион|||Ямало-Ненецкий автономный округ\n",
      "  7. Спорт\n",
      "  8. Другое|||Регион|||Белгородская область\n",
      "  9. Регион|||СтавропольѸй край|||Экономика\n",
      "  10. Регион|||Республика Татарстан|||Семья и дети\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm.auto import tqdm\n",
    "from io import TextIOWrapper\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# ==============================================\n",
    "# НАСТРОЙКИ (изменяйте здесь)\n",
    "# ==============================================\n",
    "\n",
    "repo_id = \"ajtkulov/telegram-ru\"\n",
    "\n",
    "output_dir = \"data/posts/ajtkulov/selected\"               # куда сохранять финальный результат\n",
    "cache_dir = os.path.join(output_dir, \"cache\")             # временный кэш для ZIP\n",
    "progress_file = os.path.join(output_dir, \"progress.json\") # какие ZIP уже обработаны\n",
    "stats_file = os.path.join(output_dir, \"channel_top_posts.json\")  # топ-посты по каналам\n",
    "selected_posts_file = os.path.join(output_dir, \"selected_posts_temp.jsonl\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Пути к файлам с категориями\n",
    "channels_file = \"data/posts/ajtkulov/meta/all_channels_clean.csv\"\n",
    "categories_file = \"data/posts/ajtkulov/meta/filtered_categories.csv\"\n",
    "\n",
    "# Проверка существования файлов\n",
    "print(\"Загрузка информации о каналах и категориях...\")\n",
    "if not os.path.exists(channels_file):\n",
    "    print(f\"ОШИБКА: Файл не найден: {channels_file}\")\n",
    "    print(\"Убедитесь, что файл all_channels_clean.csv находится в папке data/posts/ajtkulov/\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(categories_file):\n",
    "    print(f\"ОШИБКА: Файл не найден: {categories_file}\")\n",
    "    print(\"Убедитесь, что файл filtered_categories.csv находится в папке data/posts/ajtkulov/\")\n",
    "    exit()\n",
    "\n",
    "# Загружаем данные о каналах и категориях\n",
    "all_channels_clean = pd.read_csv(channels_file)\n",
    "filtered_categories = pd.read_csv(categories_file)\n",
    "\n",
    "print(f\"Загружено каналов: {len(all_channels_clean)}\")\n",
    "print(f\"Загружено категорий для фильтрации: {len(filtered_categories)}\")\n",
    "\n",
    "# Создаем словарь channel -> category\n",
    "channel_to_category = {}\n",
    "if 'category' in all_channels_clean.columns and 'channel' in all_channels_clean.columns:\n",
    "    for _, row in all_channels_clean.iterrows():\n",
    "        channel_to_category[row['channel']] = row['category']\n",
    "    print(f\"Создан словарь категорий для {len(channel_to_category)} каналов\")\n",
    "else:\n",
    "    print(\"ОШИБКА: В файле all_channels_clean.csv должны быть колонки 'channel' и 'category'\")\n",
    "    print(f\"Найдены колонки: {list(all_channels_clean.columns)}\")\n",
    "    exit()\n",
    "\n",
    "# Создаем множество разрешенных категорий\n",
    "if 'category' in filtered_categories.columns:\n",
    "    allowed_categories = set(filtered_categories['category'].dropna().astype(str).tolist())\n",
    "    print(f\"Разрешено {len(allowed_categories)} категорий\")\n",
    "    print(\"Первые 10 разрешенных категорий:\")\n",
    "    for i, cat in enumerate(list(allowed_categories)[:10]):\n",
    "        print(f\"  {i+1}. {cat}\")\n",
    "else:\n",
    "    print(\"ОШИБКА: В файле filtered_categories.csv должна быть колонка 'category'\")\n",
    "    print(f\"Найдены колонки: {list(filtered_categories.columns)}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d23a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено статистики по 630 каналам\n",
      "Загружен кэш для 868 каналов\n",
      "Уже обработано ZIP-файлов: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2ba2339c434312b751eba5a02d9a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка ZIP-файлов:   0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tg.0.zip уже обработан — пропуск\n",
      "\n",
      "=== tg.1.zip ===\n",
      "Скачано: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.1.zip\n",
      "Внутри ZIP файлов: 887\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2ddc72e7404ac7af46e2ad15284de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Внутренние файлы:   0%|          | 0/887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано: 400,000 | Каналов с разрешенной категорией: 38 | Потенциальных отобранных: 3,287\n",
      "Обработано: 1,800,000 | Каналов с разрешенной категорией: 197 | Потенциальных отобранных: 4,072\n",
      "Обработано: 2,400,000 | Каналов с разрешенной категорией: 235 | Потенциальных отобранных: 4,258\n",
      "Обработано: 2,900,000 | Каналов с разрешенной категорией: 304 | Потенциальных отобранных: 4,595\n",
      "Обработано: 3,000,000 | Каналов с разрешенной категорией: 318 | Потенциальных отобранных: 4,664\n",
      "Обработано: 6,100,000 | Каналов с разрешенной категорией: 584 | Потенциальных отобранных: 5,973\n",
      "Обработано: 6,400,000 | Каналов с разрешенной категорией: 635 | Потенциальных отобранных: 6,228\n",
      "Обработано: 6,700,000 | Каналов с разрешенной категорией: 647 | Потенциальных отобранных: 6,286\n",
      "Промежуточный отбор лучших постов...\n",
      "ZIP удалён: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.1.zip\n",
      "Статистика после обработки tg.1.zip:\n",
      "  - Обработано записей: 7,067,101\n",
      "  - Отобрано постов всего: 6,426\n",
      "  - Каналов с разрешенной категорией: 675\n",
      "\n",
      "=== tg.2.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012253105a094288a0785b1428423790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.2.zip:   5%|4         | 62.9M/1.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачано: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.2.zip\n",
      "Внутри ZIP файлов: 895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe36d3fa8525499480ed76e8090b4787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Внутренние файлы:   0%|          | 0/895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано: 8,100,000 | Каналов с разрешенной категорией: 822 | Потенциальных отобранных: 7,144\n",
      "Обработано: 9,600,000 | Каналов с разрешенной категорией: 948 | Потенциальных отобранных: 7,755\n",
      "Обработано: 9,800,000 | Каналов с разрешенной категорией: 966 | Потенциальных отобранных: 7,845\n",
      "Обработано: 9,900,000 | Каналов с разрешенной категорией: 966 | Потенциальных отобранных: 7,845\n",
      "Обработано: 10,000,000 | Каналов с разрешенной категорией: 966 | Потенциальных отобранных: 7,845\n",
      "Обработано: 10,500,000 | Каналов с разрешенной категорией: 1,027 | Потенциальных отобранных: 8,138\n",
      "Обработано: 11,000,000 | Каналов с разрешенной категорией: 1,083 | Потенциальных отобранных: 8,415\n",
      "Обработано: 11,700,000 | Каналов с разрешенной категорией: 1,170 | Потенциальных отобранных: 8,832\n",
      "Обработано: 11,800,000 | Каналов с разрешенной категорией: 1,187 | Потенциальных отобранных: 8,917\n",
      "Обработано: 12,800,000 | Каналов с разрешенной категорией: 1,326 | Потенциальных отобранных: 9,595\n",
      "Промежуточный отбор лучших постов...\n",
      "ZIP удалён: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.2.zip\n",
      "Статистика после обработки tg.2.zip:\n",
      "  - Обработано записей: 12,862,824\n",
      "  - Отобрано постов всего: 16,065\n",
      "  - Каналов с разрешенной категорией: 1,336\n",
      "\n",
      "=== tg.3.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822f5e1070fb4104a576bf1c2e08b940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.3.zip:   0%|          | 0.00/1.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачано: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.3.zip\n",
      "Внутри ZIP файлов: 907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269541db0d324444945860de25da7a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Внутренние файлы:   0%|          | 0/907 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано: 13,800,000 | Каналов с разрешенной категорией: 1,435 | Потенциальных отобранных: 10,127\n",
      "Обработано: 15,200,000 | Каналов с разрешенной категорией: 1,571 | Потенциальных отобранных: 10,791\n",
      "Обработано: 16,200,000 | Каналов с разрешенной категорией: 1,692 | Потенциальных отобранных: 11,375\n",
      "Обработано: 16,600,000 | Каналов с разрешенной категорией: 1,715 | Потенциальных отобранных: 11,488\n",
      "Обработано: 17,900,000 | Каналов с разрешенной категорией: 1,852 | Потенциальных отобранных: 12,167\n",
      "Обработано: 19,000,000 | Каналов с разрешенной категорией: 1,962 | Потенциальных отобранных: 12,700\n",
      "Промежуточный отбор лучших постов...\n",
      "ZIP удалён: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.3.zip\n",
      "Статистика после обработки tg.3.zip:\n",
      "  - Обработано записей: 19,359,815\n",
      "  - Отобрано постов всего: 28,955\n",
      "  - Каналов с разрешенной категорией: 2,000\n",
      "\n",
      "=== tg.4.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbb45c31f7b4e92b52538f76142fb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.4.zip:   0%|          | 0.00/1.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m     88\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m     89\u001b[0m         filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     90\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     91\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir\n\u001b[0;32m     92\u001b[0m     )\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mСкачано: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Открываем файл для добавления отобранных постов (режим 'a' - append)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1012\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1013\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1014\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1017\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1018\u001b[0m         headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[0;32m   1019\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1020\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1021\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1023\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1024\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\file_download.py:1168\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1168\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1169\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1170\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[0;32m   1171\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1172\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1173\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1174\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1175\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1176\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1177\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[0;32m   1178\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[0;32m   1179\u001b[0m     )\n\u001b[0;32m   1180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1181\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\file_download.py:1735\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[0;32m   1728\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n\u001b[0;32m   1729\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1730\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1732\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1733\u001b[0m             )\n\u001b[1;32m-> 1735\u001b[0m         http_get(\n\u001b[0;32m   1736\u001b[0m             url_to_download,\n\u001b[0;32m   1737\u001b[0m             f,\n\u001b[0;32m   1738\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1739\u001b[0m             resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1740\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1741\u001b[0m             expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1742\u001b[0m         )\n\u001b[0;32m   1744\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1745\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\file_download.py:493\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    491\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    495\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\urllib3\\response.py:1091\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1090\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1091\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1094\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\urllib3\\response.py:980\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 980\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[0;32m    982\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\urllib3\\response.py:904\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    901\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\urllib3\\response.py:887\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda\\envs\\thesis\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Желаемое количество постов на канал\n",
    "MAX_POSTS_PER_CHANNEL = 5\n",
    "\n",
    "# Фильтр по длине текста\n",
    "MIN_TEXT_LEN = 150\n",
    "MAX_TEXT_LEN = 400\n",
    "\n",
    "# Глобальный лимит записей (не превысим)\n",
    "MAX_TOTAL_RECORDS = 700000\n",
    "\n",
    "# Список ZIP-файлов (0–112, но можно ограничить)\n",
    "file_names = [f\"tg.{i}.zip\" for i in range(230)]\n",
    "# file_names = file_names[:5]  # ← для теста\n",
    "\n",
    "# ==============================================\n",
    "# Вспомогательные структуры\n",
    "# ==============================================\n",
    "\n",
    "# channel → list of (views, text, obj) — храним только топ-5 по просмотрам\n",
    "channel_top_posts = {}\n",
    "\n",
    "# Кэш проверенных каналов (чтобы не проверять категорию каждый раз)\n",
    "verified_channels_cache = {}\n",
    "\n",
    "# Функция проверки, проходит ли канал по категории\n",
    "def is_channel_allowed(channel_name):\n",
    "    if channel_name in verified_channels_cache:\n",
    "        return verified_channels_cache[channel_name]\n",
    "    \n",
    "    # Получаем категорию канала\n",
    "    category = channel_to_category.get(channel_name)\n",
    "    \n",
    "    # Если категория не найдена или пустая — пропускаем\n",
    "    if not category or pd.isna(category):\n",
    "        verified_channels_cache[channel_name] = False\n",
    "        return False\n",
    "    \n",
    "    # Проверяем, находится ли категория в разрешенных\n",
    "    is_allowed = str(category) in allowed_categories\n",
    "    verified_channels_cache[channel_name] = is_allowed\n",
    "    \n",
    "    return is_allowed\n",
    "\n",
    "# Загрузка существующей статистики, если есть\n",
    "if os.path.exists(stats_file):\n",
    "    with open(stats_file, 'r', encoding='utf-8') as f:\n",
    "        channel_top_posts = json.load(f)\n",
    "    print(f\"Загружено статистики по {len(channel_top_posts):,} каналам\")\n",
    "\n",
    "# Загрузка кэша проверенных каналов\n",
    "cache_file = os.path.join(output_dir, \"verified_channels_cache.json\")\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "        verified_channels_cache = json.load(f)\n",
    "    print(f\"Загружен кэш для {len(verified_channels_cache)} каналов\")\n",
    "\n",
    "# Загрузка уже отобранных постов, если есть\n",
    "selected_posts_count = 0\n",
    "if os.path.exists(selected_posts_file):\n",
    "    # Просто посчитаем количество строк\n",
    "    with open(selected_posts_file, 'r', encoding='utf-8') as f:\n",
    "        selected_posts_count = sum(1 for _ in f)\n",
    "    print(f\"Уже отобрано постов из предыдущих запусков: {selected_posts_count:,}\")\n",
    "\n",
    "# Прогресс: какие ZIP уже обработаны\n",
    "processed_files = set()\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as pf:\n",
    "        processed_files = set(json.load(pf))\n",
    "    print(f\"Уже обработано ZIP-файлов: {len(processed_files)}\")\n",
    "\n",
    "# ==============================================\n",
    "# Основной цикл: чтение и отбор\n",
    "# ==============================================\n",
    "\n",
    "total_processed = 0\n",
    "allowed_channels_count = 0\n",
    "\n",
    "for filename in tqdm(file_names, desc=\"Обработка ZIP-файлов\"):\n",
    "    if filename in processed_files:\n",
    "        print(f\"{filename} уже обработан — пропуск\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n=== {filename} ===\")\n",
    "    \n",
    "    try:\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=f\"data/{filename}\",\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        print(f\"Скачано: {file_path}\")\n",
    "        \n",
    "        # Открываем файл для добавления отобранных постов (режим 'a' - append)\n",
    "        with open(selected_posts_file, 'a', encoding='utf-8') as spf:\n",
    "            with zipfile.ZipFile(file_path, 'r') as z:\n",
    "                inner_files = [f for f in z.namelist() if not f.endswith('/') and not f.endswith('.done')]\n",
    "                print(f\"Внутри ZIP файлов: {len(inner_files)}\")\n",
    "                \n",
    "                for inner_path in tqdm(inner_files, desc=\"Внутренние файлы\", leave=False):\n",
    "                    with z.open(inner_path) as f:\n",
    "                        for line in TextIOWrapper(f, encoding='utf-8', errors='ignore'):\n",
    "                            line = line.strip()\n",
    "                            if not line: continue\n",
    "                            \n",
    "                            total_processed += 1\n",
    "                            \n",
    "                            try:\n",
    "                                obj = json.loads(line)\n",
    "                                channel = obj.get('channel')\n",
    "                                text = obj.get('text', '')\n",
    "                                views = obj.get('views', 0)\n",
    "                                \n",
    "                                # Конвертируем views в число\n",
    "                                if isinstance(views, str):\n",
    "                                    if 'K' in views:\n",
    "                                        views = float(views.replace('K', '').replace(',', '.')) * 1000\n",
    "                                    elif 'M' in views:\n",
    "                                        views = float(views.replace('M', '').replace(',', '.')) * 1000000\n",
    "                                    else:\n",
    "                                        try:\n",
    "                                            views = float(views.replace(',', ''))\n",
    "                                        except:\n",
    "                                            views = 0\n",
    "                                \n",
    "                                if not isinstance(views, (int, float)):\n",
    "                                    views = 0\n",
    "                                \n",
    "                                if not channel or not text:\n",
    "                                    continue\n",
    "                                \n",
    "                                # Фильтр по длине\n",
    "                                text_len = len(text)\n",
    "                                if not (MIN_TEXT_LEN <= text_len <= MAX_TEXT_LEN):\n",
    "                                    continue\n",
    "                                \n",
    "                                # Фильтр по категории канала\n",
    "                                if not is_channel_allowed(channel):\n",
    "                                    continue\n",
    "                                \n",
    "                                # Сохраняем пост для канала\n",
    "                                if channel not in channel_top_posts:\n",
    "                                    channel_top_posts[channel] = []\n",
    "                                    allowed_channels_count += 1\n",
    "                                \n",
    "                                channel_top_posts[channel].append((views, text, obj))\n",
    "                                \n",
    "                                # Если у канала слишком много постов, оставляем только топ-N\n",
    "                                if len(channel_top_posts[channel]) > MAX_POSTS_PER_CHANNEL * 2:\n",
    "                                    channel_top_posts[channel].sort(key=lambda x: x[0], reverse=True)\n",
    "                                    channel_top_posts[channel] = channel_top_posts[channel][:MAX_POSTS_PER_CHANNEL * 2]\n",
    "                                \n",
    "                                # Периодический вывод статистики\n",
    "                                if total_processed % 100000 == 0:\n",
    "                                    # Подсчитываем текущее количество отобранных постов\n",
    "                                    current_selected = sum(min(MAX_POSTS_PER_CHANNEL, len(posts)) for posts in channel_top_posts.values())\n",
    "                                    print(f\"Обработано: {total_processed:,} | Каналов с разрешенной категорией: {allowed_channels_count:,} | Потенциальных отобранных: {current_selected:,}\")\n",
    "                                \n",
    "                                if len(channel_top_posts) > 0:\n",
    "                                    # Подсчитываем примерное количество отобранных постов\n",
    "                                    estimated_selected = sum(min(MAX_POSTS_PER_CHANNEL, len(posts)) for posts in channel_top_posts.values())\n",
    "                                    if estimated_selected >= MAX_TOTAL_RECORDS:\n",
    "                                        print(f\"Достигнут глобальный лимит ({estimated_selected:,} записей) — прерываем\")\n",
    "                                        raise StopIteration\n",
    "                                        \n",
    "                            except json.JSONDecodeError:\n",
    "                                continue\n",
    "                            except Exception as e:\n",
    "                                continue\n",
    "        \n",
    "        # После обработки каждого ZIP — проводим промежуточный отбор\n",
    "        print(\"Промежуточный отбор лучших постов...\")\n",
    "        \n",
    "        # Сортируем каналы по суммарным просмотрам\n",
    "        channel_stats = []\n",
    "        for channel, post_list in channel_top_posts.items():\n",
    "            if post_list:\n",
    "                total_views = sum(views for views, _, _ in post_list[:MAX_POSTS_PER_CHANNEL])\n",
    "                channel_stats.append((channel, total_views, len(post_list)))\n",
    "        \n",
    "        # Сортируем каналы по популярности\n",
    "        channel_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Отбираем посты и сохраняем их\n",
    "        intermediate_selected = []\n",
    "        for channel, total_views, post_count in channel_stats:\n",
    "            if channel not in channel_top_posts:\n",
    "                continue\n",
    "            \n",
    "            post_list = channel_top_posts[channel]\n",
    "            post_list.sort(key=lambda x: x[0], reverse=True)\n",
    "            \n",
    "            take = min(MAX_POSTS_PER_CHANNEL, len(post_list))\n",
    "            for _, _, obj in post_list[:take]:\n",
    "                intermediate_selected.append(obj)\n",
    "                \n",
    "                if len(intermediate_selected) >= MAX_TOTAL_RECORDS:\n",
    "                    break\n",
    "            \n",
    "            if len(intermediate_selected) >= MAX_TOTAL_RECORDS:\n",
    "                break\n",
    "        \n",
    "        # Сохраняем отобранные посты в файл\n",
    "        with open(selected_posts_file, 'a', encoding='utf-8') as spf:\n",
    "            for post in intermediate_selected:\n",
    "                spf.write(json.dumps(post, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # Обновляем счетчик отобранных постов\n",
    "        selected_posts_count += len(intermediate_selected)\n",
    "        \n",
    "        # Сохраняем прогресс\n",
    "        processed_files.add(filename)\n",
    "        with open(progress_file, 'w', encoding='utf-8') as pf:\n",
    "            json.dump(list(processed_files), pf, indent=2)\n",
    "        \n",
    "        # Сохраняем промежуточную статистику топ-постов\n",
    "        with open(stats_file, 'w', encoding='utf-8') as sf:\n",
    "            json.dump(channel_top_posts, sf, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Сохраняем кэш проверенных каналов\n",
    "        with open(cache_file, 'w', encoding='utf-8') as cf:\n",
    "            json.dump(verified_channels_cache, cf, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Удаляем ZIP, чтобы не занимать место\n",
    "        os.remove(file_path)\n",
    "        print(f\"ZIP удалён: {file_path}\")\n",
    "        \n",
    "        print(f\"Статистика после обработки {filename}:\")\n",
    "        print(f\"  - Обработано записей: {total_processed:,}\")\n",
    "        print(f\"  - Отобрано постов всего: {selected_posts_count:,}\")\n",
    "        print(f\"  - Каналов с разрешенной категорией: {allowed_channels_count:,}\")\n",
    "        \n",
    "        # Проверяем лимит\n",
    "        if selected_posts_count >= MAX_TOTAL_RECORDS:\n",
    "            print(f\"Достигнут глобальный лимит в {MAX_TOTAL_RECORDS:,} записей — завершаем обработку\")\n",
    "            break\n",
    "        \n",
    "    except StopIteration:\n",
    "        print(\"Обработка прервана по достижению лимита\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка {filename}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ==============================================\n",
    "# Финальный отбор и сохранение результатов\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\nФинальный отбор постов...\")\n",
    "print(f\"Всего каналов с разрешенной категорией: {len(channel_top_posts):,}\")\n",
    "print(f\"Уже отобрано постов: {selected_posts_count:,}\")\n",
    "\n",
    "# Загружаем все отобранные посты из временного файла\n",
    "selected_posts = []\n",
    "if os.path.exists(selected_posts_file):\n",
    "    with open(selected_posts_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    selected_posts.append(json.loads(line))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "# Если нужно ограничить по лимиту (на случай если превысили)\n",
    "if len(selected_posts) > MAX_TOTAL_RECORDS:\n",
    "    print(f\"Ограничиваем количество постов с {len(selected_posts):,} до {MAX_TOTAL_RECORDS:,}\")\n",
    "    selected_posts = selected_posts[:MAX_TOTAL_RECORDS]\n",
    "\n",
    "print(f\"\\nФинальное количество постов: {len(selected_posts):,}\")\n",
    "print(f\"Всего обработано записей: {total_processed:,}\")\n",
    "\n",
    "# ==============================================\n",
    "# Сохранение финального результата\n",
    "# ==============================================\n",
    "\n",
    "if selected_posts:\n",
    "    df_final = pd.DataFrame(selected_posts)\n",
    "    \n",
    "    # Сохраняем в CSV\n",
    "    csv_path = os.path.join(output_dir, \"selected_posts.csv\")\n",
    "    df_final.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Финальный CSV: {csv_path} ({len(df_final):,} строк)\")\n",
    "    \n",
    "    # Сохраняем в JSONL (одна строка = один пост)\n",
    "    jsonl_path = os.path.join(output_dir, \"selected_posts.jsonl\")\n",
    "    with open(jsonl_path, 'w', encoding='utf-8') as jf:\n",
    "        for post in selected_posts:\n",
    "            jf.write(json.dumps(post, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Финальный JSONL: {jsonl_path} ({len(selected_posts):,} записей)\")\n",
    "    \n",
    "    # Сохраняем в ДВА JSON формата\n",
    "    \n",
    "    # 1. Красивый JSON с отступами (для чтения)\n",
    "    json_path_pretty = os.path.join(output_dir, \"selected_posts_pretty.json\")\n",
    "    with open(json_path_pretty, 'w', encoding='utf-8') as jf:\n",
    "        json.dump(selected_posts, jf, ensure_ascii=False, indent=2)\n",
    "    print(f\"Форматированный JSON: {json_path_pretty}\")\n",
    "    \n",
    "    # 2. Обычный JSON без отступов (одна строка, компактный)\n",
    "    json_path_compact = os.path.join(output_dir, \"selected_posts.json\")\n",
    "    with open(json_path_compact, 'w', encoding='utf-8') as jf:\n",
    "        json.dump(selected_posts, jf, ensure_ascii=False)\n",
    "    print(f\"Компактный JSON: {json_path_compact} ({len(selected_posts):,} записей)\")\n",
    "    \n",
    "    # Сохраняем статистику обработки\n",
    "    stats_summary = {\n",
    "        \"total_selected_posts\": len(selected_posts),\n",
    "        \"total_processed_records\": total_processed,\n",
    "        \"total_allowed_channels\": len(channel_top_posts),\n",
    "        \"max_posts_per_channel\": MAX_POSTS_PER_CHANNEL,\n",
    "        \"min_text_length\": MIN_TEXT_LEN,\n",
    "        \"max_text_length\": MAX_TEXT_LEN,\n",
    "        \"max_total_records_limit\": MAX_TOTAL_RECORDS,\n",
    "        \"allowed_categories_count\": len(allowed_categories),\n",
    "        \"processed_zip_files\": len(processed_files),\n",
    "        \"estimated_memory_saved_MB\": (total_processed - len(selected_posts)) * 0.5 / 1024  # примерная оценка\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(output_dir, \"processing_summary.json\")\n",
    "    with open(summary_path, 'w', encoding='utf-8') as sf:\n",
    "        json.dump(stats_summary, sf, ensure_ascii=False, indent=2)\n",
    "    print(f\"Статистика обработки: {summary_path}\")\n",
    "    \n",
    "    # Удаляем временный файл\n",
    "    if os.path.exists(selected_posts_file):\n",
    "        os.remove(selected_posts_file)\n",
    "        print(f\"Временный файл удалён: {selected_posts_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Не удалось отобрать ни одного поста\")\n",
    "\n",
    "print(\"\\nГотово!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
