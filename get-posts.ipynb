{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddb458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm.auto import tqdm\n",
    "from io import TextIOWrapper\n",
    "import pandas as pd\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def36e1",
   "metadata": {},
   "source": [
    "### Загрузка и подготовка данных о Telegram-каналах\n",
    "\n",
    "Эта ячейка выполняет следующие задачи:\n",
    "\n",
    "1. **Скачивает** архив `all.channels.csv.zip` из датасета на Hugging Face  \n",
    "   (`ajtkulov/telegram-ru`)\n",
    "\n",
    "2. **Распаковывает** его в локальную папку `data/posts/ajtkulov/meta`\n",
    "\n",
    "3. **Находит** внутри распакованный CSV-файл (независимо от того, как он называется)\n",
    "\n",
    "4. **Читает** CSV с максимально устойчивой обработкой проблем:\n",
    "   - пробует несколько популярных кодировок (utf-8-sig → cp1251 → utf-8 и др.)\n",
    "   - пропускает битые строки\n",
    "   - заменяет нераспознанные символы\n",
    "   - использует разделитель табуляцию (`\\t`)\n",
    "\n",
    "5. **Переименовывает** столбцы в понятные имена, если их количество совпадает с ожидаемым:\n",
    "   - `link`, `name`, `description`, `category`, `message_id`\n",
    "\n",
    "6. **Выводит** базовую статистику:\n",
    "   - общее количество каналов\n",
    "   - список столбцов\n",
    "   - первые 10 строк\n",
    "   - распределение по категориям (топ-150)\n",
    "   - список всех уникальных категорий\n",
    "\n",
    "7. **Сохраняет** таблицу с уникальными категориями в файл `unique_categories.csv`  \n",
    "   (удобно для дальнейшего анализа и маппинга)\n",
    "\n",
    "**Результат работы ячейки** — подготовленный pandas-датафрейм `df_channels` с мета-информацией о тысячах Telegram-каналов, готовый к дальнейшей фильтрации, выборке или использованию в пайплайне сбора постов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a142b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP-файл скачан: C:\\Users\\Admin\\.cache\\huggingface\\hub\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\meta\\all.channels.csv.zip\n",
      "Распаковка в папку: data/posts/ajtkulov/meta\n",
      "Файлы внутри ZIP: ['all.channels.csv']\n",
      "Распаковка завершена\n",
      "\n",
      "Чтение CSV из файла: data/posts/ajtkulov/meta\\all.channels.csv\n",
      "\n",
      "УСПЕХ! Файл прочитан с кодировкой: utf-8-sig\n",
      "\n",
      "Колонки успешно переименованы в: link, name, description, category, message_id\n",
      "Количество каналов: 380,467\n",
      "Количество столбцов: 5\n",
      "Столбцы: ['link', 'name', 'description', 'category', 'message_id']\n",
      "\n",
      "Первые 10 каналов:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>message_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://tgstat.ru/channel/@premium</td>\n",
       "      <td>Telegram Premium</td>\n",
       "      <td>Telegram Premium – a subscription that unlocks...</td>\n",
       "      <td>Telegram</td>\n",
       "      <td>7980100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://tgstat.ru/channel/sOj9iDAtUkMyYWQy</td>\n",
       "      <td>Топор Live</td>\n",
       "      <td>Нейтрально, без пропаганды. Топор Live с быстр...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>4388359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://tgstat.ru/channel/@wewantyoutodothejob</td>\n",
       "      <td>WeWantYou</td>\n",
       "      <td>Канал для поиска исполнителей для разных задач...</td>\n",
       "      <td>Другое</td>\n",
       "      <td>4158678.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://tgstat.ru/channel/@leoday</td>\n",
       "      <td>Леонардо Дайвинчик</td>\n",
       "      <td>Бот знакомств @leomatchbot</td>\n",
       "      <td>Юмор и развлечение</td>\n",
       "      <td>4057819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://tgstat.ru/channel/@novosti_efir</td>\n",
       "      <td>Прямой Эфир • Новости</td>\n",
       "      <td>️Все самое важное в одном канале. Новости Росс...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3886208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://tgstat.ru/channel/@novosti_voinaa</td>\n",
       "      <td>СМИ Россия не Москва</td>\n",
       "      <td>Эруктации информпространства России и ее Окраин.</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3368394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://tgstat.ru/channel/@rian_ru</td>\n",
       "      <td>РИА Новости</td>\n",
       "      <td>Главные Новости РИА t.me/rian_ru</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3220976.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://tgstat.ru/channel/@invest_zonaa</td>\n",
       "      <td>INVEST ZONE</td>\n",
       "      <td>Привет! Я Руслан, с 2017 года торгую рынок кри...</td>\n",
       "      <td>Криптовалюты</td>\n",
       "      <td>3057506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://tgstat.ru/channel/@mash</td>\n",
       "      <td>Mash</td>\n",
       "      <td>Прислать новость, фото, видео, аудио, бересту:...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>2827677.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://tgstat.ru/channel/@crypto_drop_stukach</td>\n",
       "      <td>Дропы от Стукача</td>\n",
       "      <td>Все о крипто раздачах, прибыльных темах и абуз...</td>\n",
       "      <td>Шок-конент</td>\n",
       "      <td>2636192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             link                   name  \\\n",
       "0              https://tgstat.ru/channel/@premium       Telegram Premium   \n",
       "1      https://tgstat.ru/channel/sOj9iDAtUkMyYWQy             Топор Live   \n",
       "2  https://tgstat.ru/channel/@wewantyoutodothejob              WeWantYou   \n",
       "3               https://tgstat.ru/channel/@leoday     Леонардо Дайвинчик   \n",
       "4         https://tgstat.ru/channel/@novosti_efir  Прямой Эфир • Новости   \n",
       "5       https://tgstat.ru/channel/@novosti_voinaa   СМИ Россия не Москва   \n",
       "6              https://tgstat.ru/channel/@rian_ru            РИА Новости   \n",
       "7         https://tgstat.ru/channel/@invest_zonaa            INVEST ZONE   \n",
       "8                 https://tgstat.ru/channel/@mash                   Mash   \n",
       "9  https://tgstat.ru/channel/@crypto_drop_stukach       Дропы от Стукача   \n",
       "\n",
       "                                         description            category  \\\n",
       "0  Telegram Premium – a subscription that unlocks...            Telegram   \n",
       "1  Нейтрально, без пропаганды. Топор Live с быстр...             Новости   \n",
       "2  Канал для поиска исполнителей для разных задач...              Другое   \n",
       "3                         Бот знакомств @leomatchbot  Юмор и развлечение   \n",
       "4  ️Все самое важное в одном канале. Новости Росс...             Новости   \n",
       "5   Эруктации информпространства России и ее Окраин.             Новости   \n",
       "6                   Главные Новости РИА t.me/rian_ru             Новости   \n",
       "7  Привет! Я Руслан, с 2017 года торгую рынок кри...        Криптовалюты   \n",
       "8  Прислать новость, фото, видео, аудио, бересту:...             Новости   \n",
       "9  Все о крипто раздачах, прибыльных темах и абуз...          Шок-конент   \n",
       "\n",
       "   message_id  \n",
       "0   7980100.0  \n",
       "1   4388359.0  \n",
       "2   4158678.0  \n",
       "3   4057819.0  \n",
       "4   3886208.0  \n",
       "5   3368394.0  \n",
       "6   3220976.0  \n",
       "7   3057506.0  \n",
       "8   2827677.0  \n",
       "9   2636192.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Распределение по category (топ-15):\n",
      "category\n",
      "Новости                                                               25909\n",
      "Блоги                                                                 25599\n",
      "Другое                                                                20693\n",
      "Мода и красота                                                        17673\n",
      "Психология                                                            13740\n",
      "                                                                      ...  \n",
      "Политика|||Регион|||Свердловская область                                 19\n",
      "Политика|||Регион|||Самарская область                                    19\n",
      "Путешествия|||Регион|||Приморский край                                   18\n",
      "Новости|||Регион|||Кабардино-Балкарская Республика|||Новости и СМИ       18\n",
      "Политика|||Регион|||Пермский край                                        18\n",
      "Name: count, Length: 150, dtype: int64\n",
      "\n",
      "Уникальные категории сохранены в файл: data/posts/ajtkulov/meta\\unique_categories.csv\n",
      "Количество уникальных категорий: 1748\n",
      "Первые 20 уникальных категорий:\n",
      "                                     category\n",
      "0                                    Telegram\n",
      "1                                     Новости\n",
      "2                                      Другое\n",
      "3                          Юмор и развлечение\n",
      "4                                Криптовалюты\n",
      "5                                  Шок-конент\n",
      "6   Новости|||Регион|||Москва|||Новости и СМИ\n",
      "7                                    Политика\n",
      "8                                   Экономика\n",
      "9                                  Технологии\n",
      "10                                      Блоги\n",
      "11                                    Продажи\n",
      "12                             Видео и фильмы\n",
      "13                                       Игры\n",
      "14                          Софт и приложения\n",
      "15                                    Карьера\n",
      "16                               Букмекерство\n",
      "17                                     Бизнес\n",
      "18                                Образование\n",
      "19                               Для взрослых\n",
      "CSV лежит здесь: data/posts/ajtkulov/meta\\all.channels.csv\n"
     ]
    }
   ],
   "source": [
    "# Параметры\n",
    "repo_id = \"ajtkulov/telegram-ru\"\n",
    "meta_zip_filename = \"meta/all.channels.csv.zip\"          # путь в репозитории\n",
    "output_meta_dir = \"data/posts/ajtkulov/meta\"             # куда сохраняем\n",
    "\n",
    "os.makedirs(output_meta_dir, exist_ok=True)\n",
    "\n",
    "# Шаг 1: Скачивание ZIP-файла\n",
    "try:\n",
    "    zip_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=meta_zip_filename,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    print(f\"ZIP-файл скачан: {zip_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка скачивания: {e}\")\n",
    "    raise\n",
    "\n",
    "# Шаг 2: Распаковка в нужную папку\n",
    "print(\"Распаковка в папку:\", output_meta_dir)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    inner_files = zip_ref.namelist()\n",
    "    print(f\"Файлы внутри ZIP: {inner_files}\")\n",
    "    \n",
    "    if not inner_files:\n",
    "        raise ValueError(\"ZIP-файл пустой\")\n",
    "    \n",
    "    zip_ref.extractall(output_meta_dir)\n",
    "    print(\"Распаковка завершена\")\n",
    "\n",
    "# Шаг 3: Поиск распакованного CSV-файла\n",
    "csv_path = None\n",
    "for file in os.listdir(output_meta_dir):\n",
    "    if file.lower().endswith('.csv'):\n",
    "        csv_path = os.path.join(output_meta_dir, file)\n",
    "        break\n",
    "\n",
    "if not csv_path:\n",
    "    raise FileNotFoundError(\"CSV-файл не найден после распаковки. Проверьте папку.\")\n",
    "\n",
    "print(f\"\\nЧтение CSV из файла: {csv_path}\")\n",
    "\n",
    "# Шаг 4: Чтение с обработкой всех типичных проблем\n",
    "encodings = ['utf-8-sig', 'cp1251', 'utf-8', 'latin1', 'iso-8859-1']\n",
    "df_channels = None\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        df_channels = pd.read_csv(\n",
    "            csv_path,\n",
    "            encoding=encoding,\n",
    "            sep='\\t',                  # табуляция (как у вас)\n",
    "            on_bad_lines='skip',       # пропуск битых строк\n",
    "            low_memory=False,\n",
    "            encoding_errors='replace'  # замена кракозябр\n",
    "        )\n",
    "        print(f\"\\nУСПЕХ! Файл прочитан с кодировкой: {encoding}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Попытка с {encoding} провалилась: {str(e)}\")\n",
    "\n",
    "if df_channels is None:\n",
    "    raise ValueError(\"Не удалось прочитать CSV ни одной кодировкой. Возможно файл повреждён.\")\n",
    "\n",
    "# ← Добавляем названия колонок\n",
    "column_names = [\"link\", \"name\", \"description\", \"category\", \"message_id\"]\n",
    "if len(df_channels.columns) == len(column_names):\n",
    "    df_channels.columns = column_names\n",
    "    print(\"\\nКолонки успешно переименованы в: link, name, description, category, message_id\")\n",
    "else:\n",
    "    print(f\"\\nВнимание! Количество столбцов ({len(df_channels.columns)}) не равно ожидаемому ({len(column_names)}).\")\n",
    "    print(\"Колонки НЕ переименованы. Текущие:\", df_channels.columns.tolist())\n",
    "\n",
    "\n",
    "# Общая статистика\n",
    "print(f\"Количество каналов: {len(df_channels):,}\")\n",
    "print(f\"Количество столбцов: {len(df_channels.columns)}\")\n",
    "print(\"Столбцы:\", df_channels.columns.tolist())\n",
    "\n",
    "# Первые 10 строк\n",
    "print(\"\\nПервые 10 каналов:\")\n",
    "display(df_channels.head(10))\n",
    "\n",
    "# Распределение категорий\n",
    "if 'category' in df_channels.columns:\n",
    "    print(\"\\nРаспределение по category (топ-15):\")\n",
    "    print(df_channels['category'].value_counts().head(150))\n",
    "else:\n",
    "    print(\"\\nКолонка 'category' не найдена\")\n",
    "\n",
    "if 'category' in df_channels.columns:\n",
    "    unique_categories = df_channels['category'].dropna().unique()\n",
    "    unique_df = pd.DataFrame(unique_categories, columns=['category'])\n",
    "    \n",
    "    unique_csv_path = os.path.join(output_meta_dir, \"unique_categories.csv\")\n",
    "    unique_df.to_csv(unique_csv_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\nУникальные категории сохранены в файл: {unique_csv_path}\")\n",
    "    print(f\"Количество уникальных категорий: {len(unique_categories)}\")\n",
    "    print(\"Первые 20 уникальных категорий:\")\n",
    "    print(unique_df.head(20))\n",
    "else:\n",
    "    print(\"\\nНе удалось сохранить категории — колонка 'category' отсутствует\")\n",
    "\n",
    "print(\"CSV лежит здесь:\", csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc46d4",
   "metadata": {},
   "source": [
    "### Фильтрация категорий Telegram-каналов (удаление нежелательных)\n",
    "\n",
    "Эта ячейка очищает список уникальных категорий, полученный на предыдущем шаге, от нежелательных, слишком общих или проблемных тематик.\n",
    "\n",
    "**Что делает ячейка:**\n",
    "\n",
    "1. Читает файл `unique_categories.csv` (созданный ранее)\n",
    "2. Применяет жёсткую фильтрацию по двум уровням:\n",
    "   - **Точное совпадение** с запрещённым списком (`FORBIDDEN_PATTERNS`):\n",
    "     - Новости, Политика, Шок-контент, Darknet, Для взрослых, Эротика, Право, Религия, Инстаграм, Другое, Telegram и др.\n",
    "   - **Содержит подстроку** из списка `FORBIDDEN_SUBSTRINGS` (даже частичное вхождение):\n",
    "     - Новости, Политика, Шок, Darknet\n",
    "3. Удаляет все строки, попавшие под любой из этих критериев\n",
    "4. Убирает возможные дубликаты и пустые значения\n",
    "5. Сохраняет очищенный список в новый файл:  \n",
    "   `data/posts/ajtkulov/meta/filtered_categories.csv`\n",
    "\n",
    "**Цель фильтрации**  \n",
    "Оставить только потенциально полезные, нишевые категории, которые могут быть связаны с товарами, услугами, хобби, техникой, красотой, спортом и т.п.  \n",
    "Исключаются:  \n",
    "- новостные / политические / шок-контент каналы  \n",
    "- слишком общие категории  \n",
    "- взрослый контент и сомнительные тематики\n",
    "\n",
    "**Результат**  \n",
    "Чистый, компактный список категорий, который удобно использовать для:\n",
    "- выборки каналов по тематике\n",
    "- последующего сбора постов\n",
    "- маппинга на категории товаров\n",
    "\n",
    "После выполнения ячейки рекомендуется посмотреть на первые 20–30 строк результата и при необходимости дополнить список запрещённых паттернов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7631edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Чтение файла: data/posts/ajtkulov/meta/unique_categories.csv\n",
      "Всего уникальных категорий в исходном файле: 1,748\n",
      "Первые 10 категорий:\n",
      "                                    category\n",
      "0                                   Telegram\n",
      "1                                    Новости\n",
      "2                                     Другое\n",
      "3                         Юмор и развлечение\n",
      "4                               Криптовалюты\n",
      "5                                 Шок-конент\n",
      "6  Новости|||Регион|||Москва|||Новости и СМИ\n",
      "7                                   Политика\n",
      "8                                  Экономика\n",
      "9                                 Технологии\n",
      "\n",
      "Запрещённые категории / подстроки (удаляем их):\n",
      " - Новости\n",
      " - Политика\n",
      " - Шок-контент\n",
      " - Шок-конент\n",
      " - Darknet\n",
      " - Для взрослых\n",
      " - Эротика\n",
      " - Право\n",
      " - Религия\n",
      " - Инстаграм\n",
      " - Другое\n",
      " - Telegram\n",
      " - Новости\n",
      " - Политика\n",
      " - Шок\n",
      " - Darknet\n",
      "\n",
      "После фильтрации осталось категорий: 1,417\n",
      "\n",
      "Первые 20 оставшихся категорий:\n",
      "              category\n",
      "3   Юмор и развлечение\n",
      "4         Криптовалюты\n",
      "8            Экономика\n",
      "9           Технологии\n",
      "10               Блоги\n",
      "11             Продажи\n",
      "12      Видео и фильмы\n",
      "13                Игры\n",
      "14   Софт и приложения\n",
      "15             Карьера\n",
      "16        Букмекерство\n",
      "17              Бизнес\n",
      "18         Образование\n",
      "20         Путешествия\n",
      "22     Картинки и фото\n",
      "23      Мода и красота\n",
      "24      Позновательное\n",
      "25              Музыка\n",
      "26        Семья и дети\n",
      "28     Еда и кулинария\n",
      "\n",
      "Отфильтрованный файл сохранён: data/posts/ajtkulov/meta/filtered_categories.csv\n",
      "Теперь в нём только нишевые / полезные категории\n"
     ]
    }
   ],
   "source": [
    "# Ячейка: Фильтрация категорий (убираем ненужные, включая подстроки)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Путь к файлу с уникальными категориями\n",
    "input_csv = 'data/posts/ajtkulov/meta/unique_categories.csv'\n",
    "\n",
    "# Путь для сохранения отфильтрованного файла\n",
    "output_csv = 'data/posts/ajtkulov/meta/filtered_categories.csv'\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\"Чтение файла: {input_csv}\")\n",
    "\n",
    "# Читаем файл (учитывая BOM и кодировку)\n",
    "df = pd.read_csv(input_csv, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Всего уникальных категорий в исходном файле: {len(df):,}\")\n",
    "print(\"Первые 10 категорий:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Список запрещённых категорий и подстрок (расширенный)\n",
    "FORBIDDEN_PATTERNS = [\n",
    "    'Новости', 'Политика', 'Шок-контент', 'Шок-конент', 'Darknet',\n",
    "    'Для взрослых', 'Эротика', 'Право', 'Религия', 'Инстаграм',\n",
    "    'Другое', 'Telegram',  # слишком общие\n",
    "]\n",
    "\n",
    "# Дополнительно: любые категории, содержащие эти слова (даже с |||)\n",
    "FORBIDDEN_SUBSTRINGS = ['Новости', 'Политика', 'Шок', 'Darknet']\n",
    "\n",
    "print(\"\\nЗапрещённые категории / подстроки (удаляем их):\")\n",
    "for p in FORBIDDEN_PATTERNS + FORBIDDEN_SUBSTRINGS:\n",
    "    print(f\" - {p}\")\n",
    "\n",
    "# Фильтрация:\n",
    "# 1. Точное совпадение с запрещённым списком\n",
    "# 2. Содержит любую запрещённую подстроку\n",
    "mask = (\n",
    "    df['category'].isin(FORBIDDEN_PATTERNS) |\n",
    "    df['category'].str.contains('|'.join(FORBIDDEN_SUBSTRINGS), na=False, regex=True)\n",
    ")\n",
    "\n",
    "filtered_df = df[~mask].copy()\n",
    "\n",
    "# Убираем дубликаты и NaN\n",
    "filtered_df = filtered_df.dropna(subset=['category']).drop_duplicates(subset=['category'])\n",
    "\n",
    "print(f\"\\nПосле фильтрации осталось категорий: {len(filtered_df):,}\")\n",
    "\n",
    "print(\"\\nПервые 20 оставшихся категорий:\")\n",
    "print(filtered_df.head(20))\n",
    "\n",
    "# Сохранение отфильтрованного файла\n",
    "filtered_df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nОтфильтрованный файл сохранён: {output_csv}\")\n",
    "print(\"Теперь в нём только нишевые / полезные категории\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f33bf1",
   "metadata": {},
   "source": [
    "### Очистка первого столбца мета-данных (нормализация имён каналов)\n",
    "\n",
    "Эта ячейка приводит первый столбец исходного файла `all.channels.csv` к единому, чистому и удобному формату — остаётся **только username канала** (например, `channelname` вместо `https://tgstat.ru/channel/@channelname` или `@channelname`).\n",
    "\n",
    "**Зачем это нужно:**\n",
    "\n",
    "В основном датасете каналы представлены в разных форматах:  \n",
    "- полные ссылки `https://tgstat.ru/channel/@username`  \n",
    "- ссылки с @ в начале `@username`  \n",
    "- иногда просто username без префиксов  \n",
    "\n",
    "Такой разнобой мешает:  \n",
    "- корректно искать каналы по имени  \n",
    "- фильтровать по категориям  \n",
    "- сопоставлять с другими источниками (например, с постами, собранными через Telethon)  \n",
    "- формировать единообразные ссылки `t.me/username`  \n",
    "\n",
    "Поэтому проводится **нормализация** — из всех вариантов остаётся только чистое имя канала без префиксов, @ и лишних символов. Это критически важно для дальнейшей работы: поиска по категориям, выборки каналов, сбора постов и построения связей «категория → канал → пост → товар».\n",
    "\n",
    "**Что делает ячейка шаг за шагом:**\n",
    "\n",
    "1. Читает файл `all.channels.csv`  \n",
    "   - Сначала пробует `utf-8-sig` + разделитель `\\t`  \n",
    "   - При ошибке — fallback на `cp1251`\n",
    "\n",
    "2. Если первый столбец без имени и содержит `https://tgstat.ru` → переименовывает его в `link`\n",
    "\n",
    "3. Применяет функцию очистки:\n",
    "   - Удаляет префикс `https://tgstat.ru/channel/`\n",
    "   - Убирает ведущий `@`\n",
    "   - Удаляет лишние пробелы\n",
    "   - Обрабатывает пропущенные значения\n",
    "\n",
    "4. Показывает первые 10 строк для визуального контроля\n",
    "\n",
    "5. Сохраняет результат в новый файл:  \n",
    "   `data/posts/ajtkulov/meta/all_channels_clean.csv`\n",
    "\n",
    "**Результат**  \n",
    "Файл с нормализованными именами каналов, готовый к использованию в следующих шагах:  \n",
    "- фильтрация по категориям  \n",
    "- выбор целевых каналов  \n",
    "- сбор постов через Telegram API  \n",
    "- дальнейшее сопоставление с товарами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b713148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Чтение файла: data/posts/ajtkulov/meta/all.channels.csv\n",
      "Файл успешно прочитан. Строк: 380,467\n",
      "Первый столбец не имеет имени — переименовываем в 'link'\n",
      "\n",
      "Очистка первого столбца...\n",
      "\n",
      "Первые 10 строк после очистки:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>Топор 18+</th>\n",
       "      <th>Самый популярный русскоязычный Telegram канал.</th>\n",
       "      <th>Шок-конент</th>\n",
       "      <th>8179938</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>premium</td>\n",
       "      <td>Telegram Premium</td>\n",
       "      <td>Telegram Premium – a subscription that unlocks...</td>\n",
       "      <td>Telegram</td>\n",
       "      <td>7980100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sOj9iDAtUkMyYWQy</td>\n",
       "      <td>Топор Live</td>\n",
       "      <td>Нейтрально, без пропаганды. Топор Live с быстр...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>4388359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wewantyoutodothejob</td>\n",
       "      <td>WeWantYou</td>\n",
       "      <td>Канал для поиска исполнителей для разных задач...</td>\n",
       "      <td>Другое</td>\n",
       "      <td>4158678.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leoday</td>\n",
       "      <td>Леонардо Дайвинчик</td>\n",
       "      <td>Бот знакомств @leomatchbot</td>\n",
       "      <td>Юмор и развлечение</td>\n",
       "      <td>4057819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>novosti_efir</td>\n",
       "      <td>Прямой Эфир • Новости</td>\n",
       "      <td>️Все самое важное в одном канале. Новости Росс...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3886208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>novosti_voinaa</td>\n",
       "      <td>СМИ Россия не Москва</td>\n",
       "      <td>Эруктации информпространства России и ее Окраин.</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3368394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rian_ru</td>\n",
       "      <td>РИА Новости</td>\n",
       "      <td>Главные Новости РИА t.me/rian_ru</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3220976.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>invest_zonaa</td>\n",
       "      <td>INVEST ZONE</td>\n",
       "      <td>Привет! Я Руслан, с 2017 года торгую рынок кри...</td>\n",
       "      <td>Криптовалюты</td>\n",
       "      <td>3057506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mash</td>\n",
       "      <td>Mash</td>\n",
       "      <td>Прислать новость, фото, видео, аудио, бересту:...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>2827677.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>crypto_drop_stukach</td>\n",
       "      <td>Дропы от Стукача</td>\n",
       "      <td>Все о крипто раздачах, прибыльных темах и абуз...</td>\n",
       "      <td>Шок-конент</td>\n",
       "      <td>2636192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  link              Топор 18+  \\\n",
       "0              premium       Telegram Premium   \n",
       "1     sOj9iDAtUkMyYWQy             Топор Live   \n",
       "2  wewantyoutodothejob              WeWantYou   \n",
       "3               leoday     Леонардо Дайвинчик   \n",
       "4         novosti_efir  Прямой Эфир • Новости   \n",
       "5       novosti_voinaa   СМИ Россия не Москва   \n",
       "6              rian_ru            РИА Новости   \n",
       "7         invest_zonaa            INVEST ZONE   \n",
       "8                 mash                   Mash   \n",
       "9  crypto_drop_stukach       Дропы от Стукача   \n",
       "\n",
       "      Самый популярный русскоязычный Telegram канал.          Шок-конент  \\\n",
       "0  Telegram Premium – a subscription that unlocks...            Telegram   \n",
       "1  Нейтрально, без пропаганды. Топор Live с быстр...             Новости   \n",
       "2  Канал для поиска исполнителей для разных задач...              Другое   \n",
       "3                         Бот знакомств @leomatchbot  Юмор и развлечение   \n",
       "4  ️Все самое важное в одном канале. Новости Росс...             Новости   \n",
       "5   Эруктации информпространства России и ее Окраин.             Новости   \n",
       "6                   Главные Новости РИА t.me/rian_ru             Новости   \n",
       "7  Привет! Я Руслан, с 2017 года торгую рынок кри...        Криптовалюты   \n",
       "8  Прислать новость, фото, видео, аудио, бересту:...             Новости   \n",
       "9  Все о крипто раздачах, прибыльных темах и абуз...          Шок-конент   \n",
       "\n",
       "     8179938  \n",
       "0  7980100.0  \n",
       "1  4388359.0  \n",
       "2  4158678.0  \n",
       "3  4057819.0  \n",
       "4  3886208.0  \n",
       "5  3368394.0  \n",
       "6  3220976.0  \n",
       "7  3057506.0  \n",
       "8  2827677.0  \n",
       "9  2636192.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Очищенный файл сохранён: data/posts/ajtkulov/meta/all_channels_clean.csv\n",
      "Первый столбец теперь содержит только имена каналов (без https и @)\n"
     ]
    }
   ],
   "source": [
    "# Ячейка: Очистка первого столбца (оставляем только имя канала)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Путь к исходному файлу\n",
    "input_file = 'data/posts/ajtkulov/meta/all.channels.csv'\n",
    "\n",
    "# Путь для сохранения очищенного файла\n",
    "output_file = 'data/posts/ajtkulov/meta/all_channels_clean.csv'\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\"Чтение файла: {input_file}\")\n",
    "\n",
    "# Читаем CSV (учитываем возможные проблемы с кодировкой и разделителями)\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        input_file,\n",
    "        encoding='utf-8-sig',\n",
    "        sep='\\t',                  # если табуляция\n",
    "        on_bad_lines='skip',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(f\"Файл успешно прочитан. Строк: {len(df):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка чтения: {e}\")\n",
    "    # Попробуем другую кодировку\n",
    "    df = pd.read_csv(\n",
    "        input_file,\n",
    "        encoding='cp1251',\n",
    "        sep='\\t',\n",
    "        on_bad_lines='skip',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(\"Успешно прочитано с cp1251\")\n",
    "\n",
    "# Проверяем, есть ли первый столбец (по умолчанию он без имени — берём по индексу)\n",
    "if df.columns[0].startswith('https://tgstat.ru'):\n",
    "    print(\"Первый столбец не имеет имени — переименовываем в 'link'\")\n",
    "    df = df.rename(columns={df.columns[0]: 'link'})\n",
    "\n",
    "# Очистка первого столбца\n",
    "def clean_channel_link(link):\n",
    "    if pd.isna(link):\n",
    "        return link\n",
    "    link = str(link).strip()\n",
    "    # Убираем префикс https://tgstat.ru/channel/\n",
    "    if link.startswith('https://tgstat.ru/channel/'):\n",
    "        link = link.replace('https://tgstat.ru/channel/', '')\n",
    "    # Убираем @ в начале, если остался\n",
    "    if link.startswith('@'):\n",
    "        link = link[1:]\n",
    "    return link\n",
    "\n",
    "print(\"\\nОчистка первого столбца...\")\n",
    "df['link'] = df['link'].apply(clean_channel_link)\n",
    "\n",
    "# Показываем результат\n",
    "print(\"\\nПервые 10 строк после очистки:\")\n",
    "display(df.head(10))\n",
    "\n",
    "# Сохранение нового файла\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nОчищенный файл сохранён: {output_file}\")\n",
    "print(f\"Первый столбец теперь содержит только имена каналов (без https и @)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb88ee3",
   "metadata": {},
   "source": [
    "### Подготовка данных и настроек для выборки постов по целевым категориям\n",
    "\n",
    "Эта ячейка — **стартовый блок** большого пайплайна по отбору и скачиванию постов из Telegram-каналов.  \n",
    "Она выполняет всю предварительную настройку и проверку, чтобы дальше можно было безопасно и осмысленно работать с огромным датасетом `ajtkulov/telegram-ru`.\n",
    "\n",
    "**Что именно делает ячейка:**\n",
    "\n",
    "1. **Определяет ключевые пути и настройки** (всё в одном месте для удобства изменения):\n",
    "   - репозиторий на Hugging Face\n",
    "   - папки для сохранения результатов и кэша\n",
    "   - файлы прогресса, статистики и временных результатов\n",
    "   - пути к уже подготовленным мета-файлам (`all_channels_clean.csv` и `filtered_categories.csv`)\n",
    "\n",
    "2. **Проверяет наличие критически важных файлов**  \n",
    "   Если хотя бы один из них отсутствует — сразу завершает работу с понятным сообщением об ошибке.\n",
    "\n",
    "3. **Загружает очищенный список каналов** (`all_channels_clean.csv`)  \n",
    "   → содержит чистые usernames каналов (уже нормализованные на предыдущем шаге)\n",
    "\n",
    "4. **Загружает отфильтрованный список разрешённых категорий** (`filtered_categories.csv`)  \n",
    "   → только те категории, которые мы решили оставить после очистки от новостей, политики, шок-контента и т.п.\n",
    "\n",
    "5. **Создаёт словарь сопоставления** `channel → category`  \n",
    "   → позволяет быстро узнать, к какой категории относится любой канал  \n",
    "   → используется дальше для фильтрации постов именно по нужным тематикам\n",
    "\n",
    "6. **Формирует множество разрешённых категорий** (`allowed_categories`)  \n",
    "   → будет использоваться как фильтр при обработке ZIP-архивов с постами\n",
    "\n",
    "7. **Выводит диагностическую информацию** для контроля:\n",
    "   - сколько всего каналов и категорий загружено\n",
    "   - первые 10 разрешённых категорий (чтобы сразу увидеть, что фильтр сработал корректно)\n",
    "   - предупреждения, если структура файлов не соответствует ожидаемой\n",
    "\n",
    "**Зачем это важно**\n",
    "\n",
    "Без этой ячейки дальнейшая работа невозможна или приведёт к ошибкам:  \n",
    "- мы будем обрабатывать ненужные категории (новости, политика, эротика и т.д.)  \n",
    "- не сможем связать посты с категориями товаров  \n",
    "- потеряем контроль над тем, какие каналы и посты мы вообще скачиваем  \n",
    "\n",
    "**Результат выполнения**  \n",
    "Готовые к использованию структуры данных:  \n",
    "- `all_channels_clean` — DataFrame со всеми каналами  \n",
    "- `channel_to_category` — словарь channel → категория  \n",
    "- `allowed_categories` — множество только нужных категорий  \n",
    "\n",
    "Дальше код будет скачивать ZIP-архивы из датасета, фильтровать посты именно по этим категориям и сохранять отобранные посты для последующей генерации/разметки пар «пост — товар»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510b5ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка информации о каналах и категориях...\n",
      "Загружено каналов: 380467\n",
      "Загружено категорий для фильтрации: 1417\n",
      "ОШИБКА: В файле all_channels_clean.csv должны быть колонки 'channel' и 'category'\n",
      "Найдены колонки: ['link', 'Топор 18+', 'Самый популярный русскоязычный Telegram канал.', 'Шок-конент', '8179938']\n",
      "Разрешено 1417 категорий\n",
      "Первые 10 разрешенных категорий:\n",
      "  1. Природа|||Регион|||Республика Башкортостан\n",
      "  2. Регион|||Сахалинская область|||Спорт\n",
      "  3. Еда и кулинария|||Регион|||Нижегородская область\n",
      "  4. Продажи|||Регион|||Чеченская Республика\n",
      "  5. Регион|||Краснодарский край|||Путешествия\n",
      "  6. Интерьер и строительство|||Регион|||Рязанская область\n",
      "  7. Другое|||Регион|||Ханты-Мансийский автономный округ - Югра\n",
      "  8. Право|||Регион|||Республика Марий Эл\n",
      "  9. Продажи|||Регион|||Тамбовская область\n",
      "  10. Бизнес|||Регион|||Самарская область|||Бизнес и стартапы\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm.auto import tqdm\n",
    "from io import TextIOWrapper\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# ==============================================\n",
    "# НАСТРОЙКИ (изменяйте здесь)\n",
    "# ==============================================\n",
    "\n",
    "repo_id = \"ajtkulov/telegram-ru\"\n",
    "\n",
    "output_dir = \"data/posts/ajtkulov/selected\"               # куда сохранять финальный результат\n",
    "cache_dir = os.path.join(output_dir, \"cache\")             # временный кэш для ZIP\n",
    "progress_file = os.path.join(output_dir, \"progress.json\") # какие ZIP уже обработаны\n",
    "stats_file = os.path.join(output_dir, \"channel_top_posts.json\")  # топ-посты по каналам\n",
    "selected_posts_file = os.path.join(output_dir, \"selected_posts_temp.jsonl\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Пути к файлам с категориями\n",
    "channels_file = \"data/posts/ajtkulov/meta/all_channels_clean.csv\"\n",
    "categories_file = \"data/posts/ajtkulov/meta/filtered_categories.csv\"\n",
    "\n",
    "# Проверка существования файлов\n",
    "print(\"Загрузка информации о каналах и категориях...\")\n",
    "if not os.path.exists(channels_file):\n",
    "    print(f\"ОШИБКА: Файл не найден: {channels_file}\")\n",
    "    print(\"Убедитесь, что файл all_channels_clean.csv находится в папке data/posts/ajtkulov/\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(categories_file):\n",
    "    print(f\"ОШИБКА: Файл не найден: {categories_file}\")\n",
    "    print(\"Убедитесь, что файл filtered_categories.csv находится в папке data/posts/ajtkulov/\")\n",
    "    exit()\n",
    "\n",
    "# Загружаем данные о каналах и категориях\n",
    "all_channels_clean = pd.read_csv(channels_file)\n",
    "filtered_categories = pd.read_csv(categories_file)\n",
    "\n",
    "print(f\"Загружено каналов: {len(all_channels_clean)}\")\n",
    "print(f\"Загружено категорий для фильтрации: {len(filtered_categories)}\")\n",
    "\n",
    "# Создаем словарь channel -> category\n",
    "channel_to_category = {}\n",
    "if 'category' in all_channels_clean.columns and 'channel' in all_channels_clean.columns:\n",
    "    for _, row in all_channels_clean.iterrows():\n",
    "        channel_to_category[row['channel']] = row['category']\n",
    "    print(f\"Создан словарь категорий для {len(channel_to_category)} каналов\")\n",
    "else:\n",
    "    print(\"ОШИБКА: В файле all_channels_clean.csv должны быть колонки 'channel' и 'category'\")\n",
    "    print(f\"Найдены колонки: {list(all_channels_clean.columns)}\")\n",
    "    exit()\n",
    "\n",
    "# Создаем множество разрешенных категорий\n",
    "if 'category' in filtered_categories.columns:\n",
    "    allowed_categories = set(filtered_categories['category'].dropna().astype(str).tolist())\n",
    "    print(f\"Разрешено {len(allowed_categories)} категорий\")\n",
    "    print(\"Первые 10 разрешенных категорий:\")\n",
    "    for i, cat in enumerate(list(allowed_categories)[:10]):\n",
    "        print(f\"  {i+1}. {cat}\")\n",
    "else:\n",
    "    print(\"ОШИБКА: В файле filtered_categories.csv должна быть колонка 'category'\")\n",
    "    print(f\"Найдены колонки: {list(filtered_categories.columns)}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb895c",
   "metadata": {},
   "source": [
    "### Основной пайплайн: скачивание, фильтрация и отбор качественных постов из Telegram-каналов\n",
    "\n",
    "Это **главная рабочая ячейка** всего проекта по подготовке датасета постов.  \n",
    "Она скачивает огромный датасет `ajtkulov/telegram-ru` (230 ZIP-архивов), фильтрует посты по категориям каналов, длине текста и стратегии отбора, а затем сохраняет только лучшие / средние по популярности посты в удобных форматах.\n",
    "\n",
    "**Ключевые параметры (настраиваются в начале):**\n",
    "\n",
    "- `MAX_POSTS_PER_CHANNEL = 5` — максимум постов с одного канала  \n",
    "- `MIN_TEXT_LEN = 500` / `MAX_TEXT_LEN = 1200` — диапазон длины текста поста  \n",
    "- `MAX_TOTAL_RECORDS = 700000` — жёсткий глобальный лимит (не превысим)  \n",
    "- `file_names = [tg.0.zip … tg.229.zip]` — список всех ZIP-файлов (можно ограничить для теста)\n",
    "\n",
    "**Основная логика работы:**\n",
    "\n",
    "1. **Загрузка предыдущего прогресса** (если запуск не первый):\n",
    "   - `channel_top_posts` — топ-посты по каналам\n",
    "   - `verified_channels_cache` — кэш проверенных категорий\n",
    "   - `selected_posts_temp.jsonl` — уже отобранные посты\n",
    "   - `progress.json` — список обработанных ZIP\n",
    "\n",
    "2. **Функция фильтрации каналов** `is_channel_allowed()`  \n",
    "   Проверяет, относится ли канал к одной из **разрешённых категорий** (из `filtered_categories.csv`).  \n",
    "   Использует быстрый кэш, чтобы не проверять один и тот же канал тысячи раз.\n",
    "\n",
    "3. **Умный отбор постов из середины** `select_mid_range_posts()`  \n",
    "   Вместо топовых постов (которые часто — реклама / кликбейт) или самых непопулярных берём **середину** по просмотрам.  \n",
    "   Это даёт более естественный, типичный контент канала.\n",
    "\n",
    "4. **Основной цикл по ZIP-файлам**:\n",
    "   - Скачивает каждый ZIP с Hugging Face (с кэшированием)\n",
    "   - Распаковывает и читает построчно (JSONL внутри ZIP)\n",
    "   - Для каждого поста:\n",
    "     - Проверяет длину текста (500–1200 символов)\n",
    "     - Проверяет категорию канала\n",
    "     - Конвертирует просмотры (K → ×1000, M → ×1M)\n",
    "     - Сохраняет в временный буфер `channel_top_posts[channel]`\n",
    "   - Если буфер канала переполняется → оставляет только топ-20 (буфер ×4)\n",
    "   - Периодически выводит статистику\n",
    "\n",
    "5. **Промежуточный отбор после каждого ZIP**:\n",
    "   - Из каждого канала берёт 5 постов из середины рейтинга\n",
    "   - Дописывает их в `selected_posts_temp.jsonl` (режим append)\n",
    "   - Сохраняет прогресс, статистику и кэш\n",
    "   - Удаляет скачанный ZIP, чтобы не занимать место на диске\n",
    "\n",
    "6. **Финальный этап** (после всех ZIP или достижения лимита):\n",
    "   - Собирает все отобранные посты из временного файла\n",
    "   - Ограничивает до `MAX_TOTAL_RECORDS`, если превысили\n",
    "   - Сохраняет результат в **несколько форматов**:\n",
    "     - `selected_posts.csv` — табличный вид (удобно для анализа)\n",
    "     - `selected_posts.jsonl` — построчно (для больших данных)\n",
    "     - `selected_posts_pretty.json` — красиво отформатированный JSON\n",
    "     - `selected_posts.json` — компактный JSON\n",
    "   - Создаёт сводную статистику `processing_summary.json`\n",
    "   - Удаляет временный файл\n",
    "\n",
    "**Результат выполнения**  \n",
    "Готовый качественный датасет постов из нишевых Telegram-каналов:  \n",
    "- только разрешённые категории  \n",
    "- текст нормальной длины (500–1200 символов)  \n",
    "- не топовые / не мусорные посты, а «средние по больнице»  \n",
    "- до 700 000 записей (или меньше, если постов подходящего качества меньше)  \n",
    "- несколько форматов на выбор + полная статистика обработки\n",
    "\n",
    "Этот датасет идеально подходит для следующих шагов:  \n",
    "- генерации описаний товаров по постам  \n",
    "- разметки пар «пост — товар»  \n",
    "- обучения моделей релевантности / ранжирования\n",
    "\n",
    "**Важно:** ячейка устойчива к перезапускам — продолжает с того места, где остановилась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d23a5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selected\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Загрузка существующей статистики, если есть\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(stats_file):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(stats_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     87\u001b[0m         channel_top_posts \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Желаемое количество постов на канал\n",
    "MAX_POSTS_PER_CHANNEL = 5\n",
    "\n",
    "# Фильтр по длине текста\n",
    "MIN_TEXT_LEN = 500\n",
    "MAX_TEXT_LEN = 1200\n",
    "\n",
    "# Глобальный лимит записей (не превысим)\n",
    "MAX_TOTAL_RECORDS = 700000\n",
    "\n",
    "# Список ZIP-файлов (0–112, но можно ограничить)\n",
    "file_names = [f\"tg.{i}.zip\" for i in range(230)]\n",
    "# file_names = file_names[:5]  # ← для теста\n",
    "\n",
    "# ==============================================\n",
    "# Вспомогательные структуры\n",
    "# ==============================================\n",
    "\n",
    "# channel → list of (views, text, obj) — храним только топ-5 по просмотрам\n",
    "channel_top_posts = {}\n",
    "\n",
    "# Кэш проверенных каналов (чтобы не проверять категорию каждый раз)\n",
    "verified_channels_cache = {}\n",
    "\n",
    "# Функция проверки, проходит ли канал по категории\n",
    "def is_channel_allowed(channel_name):\n",
    "    if channel_name in verified_channels_cache:\n",
    "        return verified_channels_cache[channel_name]\n",
    "    \n",
    "    # Получаем категорию канала\n",
    "    category = channel_to_category.get(channel_name)\n",
    "    \n",
    "    # Если категория не найдена или пустая — пропускаем\n",
    "    if not category or pd.isna(category):\n",
    "        verified_channels_cache[channel_name] = False\n",
    "        return False\n",
    "    \n",
    "    # Проверяем, находится ли категория в разрешенных\n",
    "    is_allowed = str(category) in allowed_categories\n",
    "    verified_channels_cache[channel_name] = is_allowed\n",
    "    \n",
    "    return is_allowed\n",
    "\n",
    "# Функция для выбора постов \"из серединки\"\n",
    "def select_mid_range_posts(post_list, num_to_select=5):\n",
    "    \"\"\"\n",
    "    Выбирает посты из середины отсортированного списка.\n",
    "    Не берет ни самые популярные, ни самые непопулярные.\n",
    "    \"\"\"\n",
    "    if not post_list or num_to_select <= 0:\n",
    "        return []\n",
    "    \n",
    "    # Сортируем по просмотрам\n",
    "    post_list.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Если постов меньше или равно нужному количеству - берем все\n",
    "    if len(post_list) <= num_to_select:\n",
    "        return [obj for _, _, obj in post_list]\n",
    "    \n",
    "    # Вычисляем позиции для выбора из середины\n",
    "    total_posts = len(post_list)\n",
    "    \n",
    "    # Стратегия 1: берем равномерно из середины (пропускаем топ и низ)\n",
    "    # Например, для 5 постов из 20: берем позиции 5, 8, 11, 14, 17\n",
    "    if total_posts >= num_to_select * 4:\n",
    "        # Много постов - берем равномерно из середины\n",
    "        step = total_posts // (num_to_select + 1)\n",
    "        start_idx = step\n",
    "        indices = [start_idx + i*step for i in range(num_to_select)]\n",
    "    else:\n",
    "        # Мало постов - берем просто середину\n",
    "        start_idx = max(1, (total_posts - num_to_select) // 2)\n",
    "        indices = list(range(start_idx, start_idx + num_to_select))\n",
    "    \n",
    "    # Берем посты по вычисленным индексам\n",
    "    selected = []\n",
    "    for idx in indices:\n",
    "        if idx < len(post_list):\n",
    "            _, _, obj = post_list[idx]\n",
    "            selected.append(obj)\n",
    "    \n",
    "    return selected\n",
    "\n",
    "# Загрузка существующей статистики, если есть\n",
    "if os.path.exists(stats_file):\n",
    "    with open(stats_file, 'r', encoding='utf-8') as f:\n",
    "        channel_top_posts = json.load(f)\n",
    "    print(f\"Загружено статистики по {len(channel_top_posts):,} каналам\")\n",
    "\n",
    "# Загрузка кэша проверенных каналов\n",
    "cache_file = os.path.join(output_dir, \"verified_channels_cache.json\")\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "        verified_channels_cache = json.load(f)\n",
    "    print(f\"Загружен кэш для {len(verified_channels_cache)} каналов\")\n",
    "\n",
    "# Загрузка уже отобранных постов, если есть\n",
    "selected_posts_count = 0\n",
    "if os.path.exists(selected_posts_file):\n",
    "    # Просто посчитаем количество строк\n",
    "    with open(selected_posts_file, 'r', encoding='utf-8') as f:\n",
    "        selected_posts_count = sum(1 for _ in f)\n",
    "    print(f\"Уже отобрано постов из предыдущих запусков: {selected_posts_count:,}\")\n",
    "\n",
    "# Прогресс: какие ZIP уже обработаны\n",
    "processed_files = set()\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as pf:\n",
    "        processed_files = set(json.load(pf))\n",
    "    print(f\"Уже обработано ZIP-файлов: {len(processed_files)}\")\n",
    "\n",
    "# ==============================================\n",
    "# Основной цикл: чтение и отбор\n",
    "# ==============================================\n",
    "\n",
    "total_processed = 0\n",
    "allowed_channels_count = 0\n",
    "\n",
    "for filename in tqdm(file_names, desc=\"Обработка ZIP-файлов\"):\n",
    "    if filename in processed_files:\n",
    "        print(f\"{filename} уже обработан — пропуск\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n=== {filename} ===\")\n",
    "    \n",
    "    try:\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=f\"data/{filename}\",\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        print(f\"Скачано: {file_path}\")\n",
    "        \n",
    "        # Открываем файл для добавления отобранных постов (режим 'a' - append)\n",
    "        with open(selected_posts_file, 'a', encoding='utf-8') as spf:\n",
    "            with zipfile.ZipFile(file_path, 'r') as z:\n",
    "                inner_files = [f for f in z.namelist() if not f.endswith('/') and not f.endswith('.done')]\n",
    "                print(f\"Внутри ZIP файлов: {len(inner_files)}\")\n",
    "                \n",
    "                for inner_path in tqdm(inner_files, desc=\"Внутренние файлы\", leave=False):\n",
    "                    with z.open(inner_path) as f:\n",
    "                        for line in TextIOWrapper(f, encoding='utf-8', errors='ignore'):\n",
    "                            line = line.strip()\n",
    "                            if not line: continue\n",
    "                            \n",
    "                            total_processed += 1\n",
    "                            \n",
    "                            try:\n",
    "                                obj = json.loads(line)\n",
    "                                channel = obj.get('channel')\n",
    "                                text = obj.get('text', '')\n",
    "                                views = obj.get('views', 0)\n",
    "                                \n",
    "                                # Конвертируем views в число\n",
    "                                if isinstance(views, str):\n",
    "                                    if 'K' in views:\n",
    "                                        views = float(views.replace('K', '').replace(',', '.')) * 1000\n",
    "                                    elif 'M' in views:\n",
    "                                        views = float(views.replace('M', '').replace(',', '.')) * 1000000\n",
    "                                    else:\n",
    "                                        try:\n",
    "                                            views = float(views.replace(',', ''))\n",
    "                                        except:\n",
    "                                            views = 0\n",
    "                                \n",
    "                                if not isinstance(views, (int, float)):\n",
    "                                    views = 0\n",
    "                                \n",
    "                                if not channel or not text:\n",
    "                                    continue\n",
    "                                \n",
    "                                # Фильтр по длине\n",
    "                                text_len = len(text)\n",
    "                                if not (MIN_TEXT_LEN <= text_len <= MAX_TEXT_LEN):\n",
    "                                    continue\n",
    "                                \n",
    "                                # Фильтр по категории канала\n",
    "                                if not is_channel_allowed(channel):\n",
    "                                    continue\n",
    "                                \n",
    "                                # Сохраняем пост для канала\n",
    "                                if channel not in channel_top_posts:\n",
    "                                    channel_top_posts[channel] = []\n",
    "                                    allowed_channels_count += 1\n",
    "                                \n",
    "                                channel_top_posts[channel].append((views, text, obj))\n",
    "                                \n",
    "                                # Если у канала слишком много постов, оставляем только топ-N*2\n",
    "                                if len(channel_top_posts[channel]) > MAX_POSTS_PER_CHANNEL * 4:  # увеличен буфер\n",
    "                                    channel_top_posts[channel].sort(key=lambda x: x[0], reverse=True)\n",
    "                                    channel_top_posts[channel] = channel_top_posts[channel][:MAX_POSTS_PER_CHANNEL * 4]\n",
    "                                \n",
    "                                # Периодический вывод статистики\n",
    "                                if total_processed % 100000 == 0:\n",
    "                                    # Подсчитываем текущее количество отобранных постов\n",
    "                                    current_selected = sum(min(MAX_POSTS_PER_CHANNEL, len(posts)) for posts in channel_top_posts.values())\n",
    "                                    print(f\"Обработано: {total_processed:,} | Каналов с разрешенной категорией: {allowed_channels_count:,} | Потенциальных отобранных: {current_selected:,}\")\n",
    "                                \n",
    "                                if len(channel_top_posts) > 0:\n",
    "                                    # Подсчитываем примерное количество отобранных постов\n",
    "                                    estimated_selected = sum(min(MAX_POSTS_PER_CHANNEL, len(posts)) for posts in channel_top_posts.values())\n",
    "                                    if estimated_selected >= MAX_TOTAL_RECORDS:\n",
    "                                        print(f\"Достигнут глобальный лимит ({estimated_selected:,} записей) — прерываем\")\n",
    "                                        raise StopIteration\n",
    "                                        \n",
    "                            except json.JSONDecodeError:\n",
    "                                continue\n",
    "                            except Exception as e:\n",
    "                                continue\n",
    "        \n",
    "        # После обработки каждого ZIP — проводим промежуточный отбор\n",
    "        print(\"Промежуточный отбор постов из середины рейтинга...\")\n",
    "        \n",
    "        # Отбираем посты из середины для каждого канала\n",
    "        intermediate_selected = []\n",
    "        for channel, post_list in channel_top_posts.items():\n",
    "            if not post_list:\n",
    "                continue\n",
    "            \n",
    "            # Используем новую функцию для отбора из середины\n",
    "            selected_for_channel = select_mid_range_posts(post_list, MAX_POSTS_PER_CHANNEL)\n",
    "            intermediate_selected.extend(selected_for_channel)\n",
    "            \n",
    "            # Останавливаемся при достижении лимита\n",
    "            if len(intermediate_selected) >= MAX_TOTAL_RECORDS:\n",
    "                intermediate_selected = intermediate_selected[:MAX_TOTAL_RECORDS]\n",
    "                break\n",
    "        \n",
    "        # Сохраняем отобранные посты в файл\n",
    "        with open(selected_posts_file, 'a', encoding='utf-8') as spf:\n",
    "            for post in intermediate_selected:\n",
    "                spf.write(json.dumps(post, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # Обновляем счетчик отобранных постов\n",
    "        selected_posts_count += len(intermediate_selected)\n",
    "        \n",
    "        # Сохраняем прогресс\n",
    "        processed_files.add(filename)\n",
    "        with open(progress_file, 'w', encoding='utf-8') as pf:\n",
    "            json.dump(list(processed_files), pf, indent=2)\n",
    "        \n",
    "        # Сохраняем промежуточную статистику топ-постов\n",
    "        with open(stats_file, 'w', encoding='utf-8') as sf:\n",
    "            json.dump(channel_top_posts, sf, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Сохраняем кэш проверенных каналов\n",
    "        with open(cache_file, 'w', encoding='utf-8') as cf:\n",
    "            json.dump(verified_channels_cache, cf, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Удаляем ZIP, чтобы не занимать место\n",
    "        os.remove(file_path)\n",
    "        print(f\"ZIP удалён: {file_path}\")\n",
    "        \n",
    "        print(f\"Статистика после обработки {filename}:\")\n",
    "        print(f\"  - Обработано записей: {total_processed:,}\")\n",
    "        print(f\"  - Отобрано постов всего: {selected_posts_count:,}\")\n",
    "        print(f\"  - Каналов с разрешенной категорией: {allowed_channels_count:,}\")\n",
    "        \n",
    "        # Проверяем лимит\n",
    "        if selected_posts_count >= MAX_TOTAL_RECORDS:\n",
    "            print(f\"Достигнут глобальный лимит в {MAX_TOTAL_RECORDS:,} записей — завершаем обработку\")\n",
    "            break\n",
    "        \n",
    "    except StopIteration:\n",
    "        print(\"Обработка прервана по достижению лимита\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка {filename}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ==============================================\n",
    "# Финальный отбор и сохранение результатов\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\nФинальный отбор постов...\")\n",
    "print(f\"Всего каналов с разрешенной категорией: {len(channel_top_posts):,}\")\n",
    "print(f\"Уже отобрано постов: {selected_posts_count:,}\")\n",
    "\n",
    "# Загружаем все отобранные посты из временного файла\n",
    "selected_posts = []\n",
    "if os.path.exists(selected_posts_file):\n",
    "    with open(selected_posts_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    selected_posts.append(json.loads(line))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "# Если нужно ограничить по лимиту (на случай если превысили)\n",
    "if len(selected_posts) > MAX_TOTAL_RECORDS:\n",
    "    print(f\"Ограничиваем количество постов с {len(selected_posts):,} до {MAX_TOTAL_RECORDS:,}\")\n",
    "    selected_posts = selected_posts[:MAX_TOTAL_RECORDS]\n",
    "\n",
    "print(f\"\\nФинальное количество постов: {len(selected_posts):,}\")\n",
    "print(f\"Всего обработано записей: {total_processed:,}\")\n",
    "\n",
    "# ==============================================\n",
    "# Сохранение финального результата\n",
    "# ==============================================\n",
    "\n",
    "if selected_posts:\n",
    "    df_final = pd.DataFrame(selected_posts)\n",
    "    \n",
    "    # Сохраняем в CSV\n",
    "    csv_path = os.path.join(output_dir, \"selected_posts.csv\")\n",
    "    df_final.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Финальный CSV: {csv_path} ({len(df_final):,} строк)\")\n",
    "    \n",
    "    # Сохраняем в JSONL (одна строка = один пост)\n",
    "    jsonl_path = os.path.join(output_dir, \"selected_posts.jsonl\")\n",
    "    with open(jsonl_path, 'w', encoding='utf-8') as jf:\n",
    "        for post in selected_posts:\n",
    "            jf.write(json.dumps(post, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Финальный JSONL: {jsonl_path} ({len(selected_posts):,} записей)\")\n",
    "    \n",
    "    # Сохраняем в ДВА JSON формата\n",
    "    \n",
    "    # 1. Красивый JSON с отступами (для чтения)\n",
    "    # json_path_pretty = os.path.join(output_dir, \"selected_posts_pretty.json\")\n",
    "    # with open(json_path_pretty, 'w', encoding='utf-8') as jf:\n",
    "    #     json.dump(selected_posts, jf, ensure_ascii=False, indent=2)\n",
    "    # print(f\"Форматированный JSON: {json_path_pretty}\")\n",
    "    \n",
    "    # 2. Обычный JSON без отступов (одна строка, компактный)\n",
    "    # json_path_compact = os.path.join(output_dir, \"selected_posts.json\")\n",
    "    # with open(json_path_compact, 'w', encoding='utf-8') as jf:\n",
    "    #     json.dump(selected_posts, jf, ensure_ascii=False)\n",
    "    # print(f\"Компактный JSON: {json_path_compact} ({len(selected_posts):,} записей)\")\n",
    "    \n",
    "    # Сохраняем статистику обработки\n",
    "    stats_summary = {\n",
    "        \"total_selected_posts\": len(selected_posts),\n",
    "        \"total_processed_records\": total_processed,\n",
    "        \"total_allowed_channels\": len(channel_top_posts),\n",
    "        \"max_posts_per_channel\": MAX_POSTS_PER_CHANNEL,\n",
    "        \"min_text_length\": MIN_TEXT_LEN,\n",
    "        \"max_text_length\": MAX_TEXT_LEN,\n",
    "        \"max_total_records_limit\": MAX_TOTAL_RECORDS,\n",
    "        \"allowed_categories_count\": len(allowed_categories),\n",
    "        \"processed_zip_files\": len(processed_files),\n",
    "        \"selection_strategy\": \"mid_range (not top, not bottom)\",\n",
    "        \"estimated_memory_saved_MB\": (total_processed - len(selected_posts)) * 0.5 / 1024  # примерная оценка\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(output_dir, \"processing_summary.json\")\n",
    "    with open(summary_path, 'w', encoding='utf-8') as sf:\n",
    "        json.dump(stats_summary, sf, ensure_ascii=False, indent=2)\n",
    "    print(f\"Статистика обработки: {summary_path}\")\n",
    "    \n",
    "    # Удаляем временный файл\n",
    "    if os.path.exists(selected_posts_file):\n",
    "        os.remove(selected_posts_file)\n",
    "        print(f\"Временный файл удалён: {selected_posts_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Не удалось отобрать ни одного поста\")\n",
    "\n",
    "print(\"\\nГотово!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c4e78",
   "metadata": {},
   "source": [
    "добавляет поле категории к датасету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf24f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем словарь категорий...\n",
      "Загружено 380,468 каналов с категориями\n",
      "Всего строк в исходном файле: 700,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка строк: 100%|██████████| 700000/700000 [03:19<00:00, 3517.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обработка завершена!\n",
      "Новый файл: data/posts/ajtkulov/selected/selected_posts_with_category.jsonl\n",
      "Прогресс сохранён в: data/posts/ajtkulov/selected/last_processed_line.txt\n",
      "Чтобы начать заново — удалите файл прогресса.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==================== НАСТРОЙКИ ====================\n",
    "input_jsonl   = \"data/posts/ajtkulov/selected/selected_posts.jsonl\"           # ваш исходный файл\n",
    "channels_csv  = \"data/posts/ajtkulov/meta/all_channels_clean_with_cols.csv\"             # файл с категориями\n",
    "output_jsonl  = \"data/posts/ajtkulov/selected/selected_posts_with_category.jsonl\"  # куда писать результат\n",
    "progress_file = \"data/posts/ajtkulov/selected/last_processed_line.txt\"        # файл с номером последней строки\n",
    "\n",
    "# Поля, которые должны быть в channels_csv\n",
    "CHANNEL_COL = \"link\"       # столбец с username канала (без @)\n",
    "CATEGORY_COL = \"category\"  # столбец с категорией\n",
    "\n",
    "# ==================== ПОДГОТОВКА ====================\n",
    "# Загружаем словарь канал → категория (один раз)\n",
    "print(\"Загружаем словарь категорий...\")\n",
    "channels_df = pd.read_csv(channels_csv, usecols=[CHANNEL_COL, CATEGORY_COL])\n",
    "channel_to_category = dict(zip(channels_df[CHANNEL_COL], channels_df[CATEGORY_COL]))\n",
    "print(f\"Загружено {len(channel_to_category):,} каналов с категориями\")\n",
    "\n",
    "# Определяем, с какой строки продолжать\n",
    "start_line = 0\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, \"r\") as f:\n",
    "        try:\n",
    "            start_line = int(f.read().strip())\n",
    "            print(f\"Возобновление с строки {start_line + 1}\")\n",
    "        except:\n",
    "            print(\"Не удалось прочитать прогресс → начинаем с начала\")\n",
    "            start_line = 0\n",
    "\n",
    "# Подсчитываем общее количество строк (для tqdm)\n",
    "total_lines = sum(1 for _ in open(input_jsonl, encoding=\"utf-8\"))\n",
    "print(f\"Всего строк в исходном файле: {total_lines:,}\")\n",
    "\n",
    "# Открываем выходной файл в режиме append\n",
    "with open(output_jsonl, \"a\", encoding=\"utf-8\") as out_f:\n",
    "    # Пропускаем уже обработанные строки\n",
    "    with open(input_jsonl, \"r\", encoding=\"utf-8\") as in_f:\n",
    "        for i, line in enumerate(tqdm(in_f, total=total_lines, desc=\"Обработка строк\", initial=start_line)):\n",
    "            if i < start_line:\n",
    "                continue  # пропускаем уже сделанное\n",
    "\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                post = json.loads(line)\n",
    "                channel = post.get(\"channel\", \"\")\n",
    "\n",
    "                # Ищем категорию\n",
    "                category = channel_to_category.get(channel, \"UNKNOWN\")\n",
    "                post[\"category\"] = category\n",
    "\n",
    "                # Записываем сразу в выходной файл\n",
    "                out_f.write(json.dumps(post, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                # Сохраняем прогресс после каждой строки\n",
    "                with open(progress_file, \"w\") as pf:\n",
    "                    pf.write(str(i))\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Ошибка JSON в строке {i+1} → пропуск\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Неизвестная ошибка в строке {i+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "print(\"\\nОбработка завершена!\")\n",
    "print(f\"Новый файл: {output_jsonl}\")\n",
    "print(f\"Прогресс сохранён в: {progress_file}\")\n",
    "print(\"Чтобы начать заново — удалите файл прогресса.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e391a",
   "metadata": {},
   "source": [
    "отбирает равномерно 10к постов из разных категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17fd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем файл постов...\n",
      "Всего постов в исходном файле: 700,000\n",
      "Уникальных категорий: 124\n",
      "\n",
      "Топ-10 категорий по количеству постов:\n",
      "category\n",
      "Блоги                 125923\n",
      "Криптовалюты           47220\n",
      "Игры                   46386\n",
      "Мода и красота         43054\n",
      "Юмор и развлечение     28034\n",
      "Экономика              26413\n",
      "Продажи                26245\n",
      "Технологии             25817\n",
      "Психология             24307\n",
      "Еда и кулинария        20208\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Отбираем посты равномерно по категориям...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Отбор по категориям: 124it [00:07, 17.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Отобрано постов: 10,029\n",
      "Уникальных категорий в выборке: 124\n",
      "\n",
      "Распределение в новой выборке (топ-10):\n",
      "category\n",
      "Блоги                 1799\n",
      "Криптовалюты           675\n",
      "Игры                   663\n",
      "Мода и красота         615\n",
      "Юмор и развлечение     400\n",
      "Экономика              377\n",
      "Продажи                375\n",
      "Технологии             369\n",
      "Психология             347\n",
      "Еда и кулинария        289\n",
      "Name: count, dtype: int64\n",
      "Конвертируем столбец 'date' в строку для JSON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Сохранение: 100%|██████████| 10029/10029 [00:00<00:00, 18245.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Готово! Новый файл сохранён: data/posts/ajtkulov/selected/selected10k.jsonl\n",
      "Размер: 10,029 постов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==================== НАСТРОЙКИ ====================\n",
    "input_file = \"data/posts/ajtkulov/selected/selected_posts_with_category.jsonl\"\n",
    "output_file = \"data/posts/ajtkulov/selected/selected10k.jsonl\"\n",
    "N_TOTAL = 10000          # ← сколько постов всего хотим в итоге\n",
    "# Если хотите ровно по N постов на категорию — укажите здесь, иначе равномерно распределяет\n",
    "# N_PER_CATEGORY = 200   # ← раскомментируйте, если хотите фиксированное число на категорию\n",
    "CATEGORY_COL = \"category\"\n",
    "\n",
    "# ==================== ЗАГРУЗКА ====================\n",
    "print(\"Загружаем файл постов...\")\n",
    "df = pd.read_json(input_file, lines=True)\n",
    "print(f\"Всего постов в исходном файле: {len(df):,}\")\n",
    "\n",
    "if CATEGORY_COL not in df.columns:\n",
    "    raise ValueError(f\"Столбец '{CATEGORY_COL}' не найден. Доступные столбцы: {df.columns.tolist()}\")\n",
    "\n",
    "# Очистка и подсчёт\n",
    "df[CATEGORY_COL] = df[CATEGORY_COL].fillna(\"Без категории\").astype(str)\n",
    "category_counts = df[CATEGORY_COL].value_counts()\n",
    "print(f\"Уникальных категорий: {len(category_counts)}\")\n",
    "print(\"\\nТоп-10 категорий по количеству постов:\")\n",
    "print(category_counts.head(10))\n",
    "\n",
    "# ==================== СТРАТИФИЦИРОВАННЫЙ ОТБОР ====================\n",
    "print(\"\\nОтбираем посты равномерно по категориям...\")\n",
    "selected = []\n",
    "\n",
    "# Вариант 1: равномерное распределение (по пропорции)\n",
    "if 'N_PER_CATEGORY' not in globals() or N_PER_CATEGORY is None:\n",
    "    # Делим N_TOTAL пропорционально количеству постов в категории\n",
    "    total_posts = len(df)\n",
    "    for cat, count in tqdm(category_counts.items(), desc=\"Отбор по категориям\"):\n",
    "        if count == 0:\n",
    "            continue\n",
    "        # Сколько взять от этой категории\n",
    "        n_take = max(1, round(count / total_posts * N_TOTAL))\n",
    "        # Берём случайные посты\n",
    "        cat_posts = df[df[CATEGORY_COL] == cat].sample(n=n_take, random_state=42)\n",
    "        selected.append(cat_posts)\n",
    "else:\n",
    "    # Вариант 2: фиксированное число постов на категорию (если раскомментировали N_PER_CATEGORY)\n",
    "    n_take = N_PER_CATEGORY\n",
    "    for cat in tqdm(category_counts.index, desc=\"Отбор по категориям\"):\n",
    "        cat_posts = df[df[CATEGORY_COL] == cat]\n",
    "        if len(cat_posts) == 0:\n",
    "            continue\n",
    "        sampled = cat_posts.sample(n=min(n_take, len(cat_posts)), random_state=42)\n",
    "        selected.append(sampled)\n",
    "\n",
    "# Объединяем\n",
    "if selected:\n",
    "    df_selected = pd.concat(selected, ignore_index=True)\n",
    "    # Перемешиваем, чтобы не было блоков по категориям\n",
    "    df_selected = df_selected.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nОтобрано постов: {len(df_selected):,}\")\n",
    "    print(f\"Уникальных категорий в выборке: {df_selected[CATEGORY_COL].nunique()}\")\n",
    "    \n",
    "    # Статистика по категориям в новой выборке\n",
    "    new_counts = df_selected[CATEGORY_COL].value_counts()\n",
    "    print(\"\\nРаспределение в новой выборке (топ-10):\")\n",
    "    print(new_counts.head(10))\n",
    "    \n",
    "    # ────────────────────────────────────────────────\n",
    "    # ФИКС ОШИБКИ: преобразуем Timestamp в строку\n",
    "    if 'date' in df_selected.columns and pd.api.types.is_datetime64_any_dtype(df_selected['date']):\n",
    "        print(\"Конвертируем столбец 'date' в строку для JSON...\")\n",
    "        df_selected['date'] = df_selected['date'].astype(str)\n",
    "    # ────────────────────────────────────────────────\n",
    "    \n",
    "    # Сохраняем\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for _, row in tqdm(df_selected.iterrows(), total=len(df_selected), desc=\"Сохранение\"):\n",
    "            f.write(json.dumps(row.to_dict(), ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"\\nГотово! Новый файл сохранён: {output_file}\")\n",
    "    print(f\"Размер: {len(df_selected):,} постов\")\n",
    "else:\n",
    "    print(\"Не удалось отобрать ни одного поста — проверьте данные.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046a8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружает отобранные 10к постов\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def load_jsonl_to_dicts(input_file: str) -> List[Dict[str, Any]]:\n",
    "    data = []\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if line:  # пропускаем пустые строки\n",
    "                    try:\n",
    "                        item = json.loads(line)\n",
    "                        data.append(item)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Ошибка парсинга JSON в строке {line_num}: {e}\")\n",
    "                        print(f\"Проблемная строка: {line[:100]}...\")\n",
    "                        continue\n",
    "        print(f\"✓ Загружено {len(data):,} записей из {input_file}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ Файл не найден: {input_file}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Ошибка при чтении файла {input_file}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5060d",
   "metadata": {},
   "source": [
    "очищает и нормализует текст постов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f68e3662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Загружено 10,029 записей из data/posts/ajtkulov/selected/selected10k.jsonl\n",
      "posts: {'channel': 'blueunicorn_ru', 'text': '🔥Ссылки на мини-капсулу для читательницы: 1. Черные брюки2. Синий джемпер3. Юбка из кожи и еще одна4. Джинсы5. Белая рубашка6. Пуховик и еще один 7. Шерстяное пальто и еще одно 8. Водолазки синяя и черная9. Лонгслив10. Пиджак11. Кардиган и еще один 12. Шапки синяя и серая13. Шарф и перчатки 14. Ботильоны и ботинки 15. Сумки: синяя (еще синяя), белая, черная 16. Серьги', 'link': 'https://t.me/blueunicorn_ru/3466', 'id': 3466, 'date': '2023-12-23 09:00:58+00:00', 'views': '18.3K', 'category': 'Мода и красота'}\n",
      "✓ Обработано 10029 постов\n",
      "✓ Изменено текстов: 7578\n",
      "[{'channel': 'blueunicorn_ru', 'text': 'Ссылки на мини-капсулу для читательницы: 1. Черные брюки2. Синий джемпер3. Юбка из кожи и еще одна4. Джинсы5. Белая рубашка6. Пуховик и еще один 7. Шерстяное пальто и еще одно 8. Водолазки синяя и черная9. Лонгслив10. Пиджак11. Кардиган и еще один 12. Шапки синяя и серая13. Шарф и перчатки 14. Ботильоны и ботинки 15. Сумки: синяя (еще синяя), белая, черная 16. Серьги', 'link': 'https://t.me/blueunicorn_ru/3466', 'id': 3466, 'date': '2023-12-23 09:00:58+00:00', 'views': '18.3K', 'category': 'Мода и красота', 'original_length': 370, 'cleaned_length': 369}, {'channel': 'glvrdru', 'text': 'В «Практикуме» вышел бесплатный курс по визуальным презентациям. Я там выступил автором, ребята из «Практикума» отвечали за интерактив и методику. Бесплатно. Про визуальные презентации. На интерактивных тренажерах. Просто радость:', 'link': 'https://t.me/glvrdru/3270', 'id': 3270, 'date': '2021-06-24 14:14:45+00:00', 'views': '40.4K', 'category': 'Маркетинг, PR, реклама', 'original_length': 279, 'cleaned_length': 230}, {'channel': 'karocinema', 'text': '#КАРОконкурс - ИТОГИНаборы шпионских носков получают: Поздравляем! В ближайшее время свяжемся с вами в личных сообщениях!', 'link': 'https://t.me/karocinema/3890', 'id': 3890, 'date': '2023-02-03 12:00:23+00:00', 'views': '25.8K', 'category': 'Видео и фильмы', 'original_length': 165, 'cleaned_length': 121}, {'channel': 'beremenaya_sovet', 'text': 'Ангелы рядом с нами живут.По вечерам колыбельную ждут.Спать не дают иногда по ночам.Очень нужны эти ангелы нам.Солнце рисуют на новых обоях.Могут корабль, из стульев, построить.Любят конфеты и лимонад.На карусели кружиться хотят.Ангелы эти бывают задиры.Часто кричат, хоть беги из квартиры.Вымажут кашей все, что кругомИ улыбаются мило при том.Если болеют сходим с ума.Думаем: «Лучше б болела сама!»Дарим игрушки и сами же рады.Ангел смеется — это награда!После любых испытаний и бедСпящие ангелы дарят нам свет.Ангелы эти в детской живут.Ангелов этих «ДЕТИ» зовут! Мамино счастье', 'link': 'https://t.me/beremenaya_sovet/290', 'id': 290, 'date': '2023-05-08 05:52:11+00:00', 'views': '19.1K', 'category': 'Медицина', 'original_length': 580, 'cleaned_length': 580}, {'channel': 'animgifk', 'text': 'Вам нравится читать контент на этом канале?Возможно, вы задумывались о том, чтобы купить на нем интеграцию?Следуйте 3 простым шагам, чтобы сделать это:1) Регистрируйтесь по ссылке: Пополняйтесь удобным способом3) Размещайте публикациюЕсли тематика вашего поста подойдет нашему каналу, мы с удовольствием опубликуем его.', 'link': 'https://t.me/animgifk/7496', 'id': 7496, 'date': '2023-12-22 11:10:51+00:00', 'views': '8.6K', 'category': 'Картинки и фото', 'original_length': 350, 'cleaned_length': 319}, {'channel': 'anchabaranova', 'text': 'Для жителя США делать различные хирургические операции в Мексике - это национальный спорт. Потому что намного, намного дешевле. Особенно если страховки нет, или не покрывает. Но иногда что-то идет не так. В двух клиниках в провинции Матаморос при проведении спинальной анестезии целой куче пациентов из США в мозговые оболочки был занесен грибок. Получился менингит. 9 подозрительных случаев, 9 «возможных» случаев (это не одно и тоже), двое умерли и 206 случаев сейчас расследуется. Какой именно грибок - не говорят, хотя он подтвержден ПЦР. А всех кто недавно навещал провинцию со зловещим названием Матаморас предупредили - на развитие симптомов уходит до 6 недель, и лучше сдавайтесь сразу, провериться.', 'link': 'https://t.me/anchabaranova/1897', 'id': 1897, 'date': '2023-05-25 22:33:18+00:00', 'views': '18.9K', 'category': 'Медицина', 'original_length': 707, 'cleaned_length': 707}, {'channel': 'luxury_filmi', 'text': 'Фильм №301 — Солдаты неудачи (США, 2008)#боевик, #комедия, #военныйКамера. Мотор. Начали! В реальных джунглях проходят съемки суперблокбастера с участием больших звезд, среди которых: крутой герой боевиков, мастер туалетного юмора, чернокожий рэпер Альпа Чино и обладатель всех кинопремий, специально для фильма перекрасивший кожу в черный цвет. Остальные номера фильмов тут', 'link': 'https://t.me/luxury_filmi/568', 'id': 568, 'date': '2022-09-10 02:00:00+00:00', 'views': '35.8K', 'category': 'Видео и фильмы', 'original_length': 378, 'cleaned_length': 374}, {'channel': 'recyclemagru', 'text': 'Очередное напоминание, к чему приводит выжигание травы на сельхозземлях. За последние сутки огонь уничтожил десятки домов сразу в нескольких селах Кузбасса. Причина пожаров — масштабный пал травы. Особенно пострадала деревня Постниково, где сгорели 22 строения. Общая площадь возгорания превысила 1 тыс. кв. м. А в самом Кемерово тем временем уже несколько дней действует режим «черного неба».', 'link': 'https://t.me/recyclemagru/1379', 'id': 1379, 'date': '2020-04-24 13:17:52+00:00', 'views': '25.2K', 'category': 'Природа', 'original_length': 393, 'cleaned_length': 393}, {'channel': 'g_telec', 'text': 'Что заставляет Тельца грустить. Вы испытываете грусть, когда стараетесь изо всех сил, но всё идёт не по плану. Вы надеетесь, что ваша безумная отдача всегда приведёт к нужному результату, но так бывает далеко не всегда. Проходя через подобные моменты грусти, помните, что вы одни из самых упрямых знаков. А это значит, что сложности не должны вас останавливать от достижения целей. Вы часто не даёте самому себе второй шанс попробовать то, что не получилось однажды. Это нормально, когда что-то не получается сразу.', 'link': 'https://t.me/g_telec/1121', 'id': 1121, 'date': '2024-02-15 08:23:46+00:00', 'views': '23.5K', 'category': 'Эзотерика', 'original_length': 517, 'cleaned_length': 515}, {'channel': 'hockeyshtab', 'text': 'Ткачёв оформляет дубль - 3:3Интересно, что перед этим судьи дважды могли удалить форварда \"Авангарда\" за \"разговоры\" (сразу после удаления, а потом после выхода со скамейки штрафников), но обошлись устным внушением (даже попросили Кравеца успокоить подопечного). Момент, на который наверняка обратит внимание СКА', 'link': 'https://t.me/hockeyshtab/1948', 'id': 1948, 'date': '2023-02-11 12:32:07+00:00', 'views': '35.3K', 'category': 'Спорт', 'original_length': 313, 'cleaned_length': 312}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Сохранение очищенных постов: 100%|██████████| 10029/10029 [00:00<00:00, 59920.93it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_post_text(text: str, remove_emojis: bool = True, \n",
    "                    remove_urls: bool = True, \n",
    "                    remove_mentions: bool = True,\n",
    "                    remove_hashtags: bool = False,\n",
    "                    remove_extra_spaces: bool = True,\n",
    "                    remove_special_chars: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Очищает текст поста от нежелательных элементов.\n",
    "    \n",
    "    Args:\n",
    "        text: исходный текст\n",
    "        remove_emojis: удалять эмодзи/смайлики\n",
    "        remove_urls: удалять URL ссылки\n",
    "        remove_mentions: удалять упоминания (@username)\n",
    "        remove_hashtags: удалять хэштеги (#тег)\n",
    "        remove_extra_spaces: удалять лишние пробелы\n",
    "        remove_special_chars: удалять специальные символы (оставить только буквы/цифры/пробелы)\n",
    "        \n",
    "    Returns:\n",
    "        str: очищенный текст\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    cleaned = text\n",
    "    \n",
    "    # 1. Удаление эмодзи и смайликов\n",
    "    if remove_emojis:\n",
    "        # Используем библиотеку emoji для удаления эмодзи\n",
    "        cleaned = emoji.replace_emoji(cleaned, '')\n",
    "        \n",
    "        # Дополнительно удаляем常见ные символы эмодзи (на всякий случай)\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # эмоции\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # символы и пиктограммы\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # транспорт и символы\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # флаги\n",
    "            u\"\\U00002702-\\U000027B0\"  # различные символы\n",
    "            u\"\\U000024C2-\\U0001F251\"  # дополнительные\n",
    "            u\"\\U0001F900-\\U0001F9FF\"  # дополнительные эмодзи\n",
    "            u\"\\U0001FA70-\\U0001FAFF\"  # дополнительные эмодзи\n",
    "            u\"\\U00002600-\\U000026FF\"  # различные символы\n",
    "            u\"\\U00002700-\\U000027BF\"  # символы Dingbats\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "        cleaned = emoji_pattern.sub('', cleaned)\n",
    "    \n",
    "    # 2. Удаление URL\n",
    "    if remove_urls:\n",
    "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        cleaned = url_pattern.sub('', cleaned)\n",
    "    \n",
    "    # 3. Удаление упоминаний (@username)\n",
    "    if remove_mentions:\n",
    "        mention_pattern = re.compile(r'@\\w+')\n",
    "        cleaned = mention_pattern.sub('', cleaned)\n",
    "    \n",
    "    # 4. Удаление хэштегов\n",
    "    if remove_hashtags:\n",
    "        hashtag_pattern = re.compile(r'#\\w+')\n",
    "        cleaned = hashtag_pattern.sub('', cleaned)\n",
    "    \n",
    "    # 5. Удаление специальных символов (оставляем только буквы, цифры и пробелы)\n",
    "    if remove_special_chars:\n",
    "        # Оставляем кириллицу, латиницу, цифры и пробелы\n",
    "        special_chars_pattern = re.compile(r'[^а-яА-Яa-zA-Z0-9\\s]')\n",
    "        cleaned = special_chars_pattern.sub('', cleaned)\n",
    "    \n",
    "    # 6. Удаление лишних пробелов\n",
    "    if remove_extra_spaces:\n",
    "        # Заменяем множественные пробелы на один\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        # Удаляем пробелы в начале и конце\n",
    "        cleaned = cleaned.strip()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def process_posts(posts: List[Dict[str, Any]], \n",
    "                 text_field: str = \"text\",\n",
    "                 inplace: bool = False,\n",
    "                 **clean_kwargs) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Обрабатывает список постов, очищая текстовые поля.\n",
    "    \n",
    "    Args:\n",
    "        posts: список словарей с постами\n",
    "        text_field: название поля с текстом для очистки\n",
    "        inplace: изменять исходный список или создать новый\n",
    "        **clean_kwargs: параметры для clean_post_text()\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: обработанный список постов\n",
    "    \"\"\"\n",
    "    if inplace:\n",
    "        result = posts\n",
    "    else:\n",
    "        result = [post.copy() for post in posts]\n",
    "    \n",
    "    cleaned_count = 0\n",
    "    for post in result:\n",
    "        if text_field in post and isinstance(post[text_field], str):\n",
    "            original_text = post[text_field]\n",
    "            cleaned_text = clean_post_text(original_text, **clean_kwargs)\n",
    "            post[text_field] = cleaned_text\n",
    "            \n",
    "            if original_text != cleaned_text:\n",
    "                cleaned_count += 1\n",
    "                \n",
    "            # Добавляем информацию об очистке (опционально)\n",
    "            if 'original_length' not in post:\n",
    "                post['original_length'] = len(original_text)\n",
    "            post['cleaned_length'] = len(cleaned_text)\n",
    "    \n",
    "    print(f\"✓ Обработано {len(result)} постов\")\n",
    "    print(f\"✓ Изменено текстов: {cleaned_count}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "posts = load_jsonl_to_dicts(\"data/posts/ajtkulov/selected/selected10k.jsonl\")\n",
    "print('posts:', posts[0])\n",
    "cleaned_posts = process_posts(posts)\n",
    "print(cleaned_posts[:10])\n",
    "\n",
    "output_file = \"data/posts/ajtkulov/selected/selected10k_cleaned.jsonl\"\n",
    "\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for post in tqdm(cleaned_posts, total=len(cleaned_posts), desc=\"Сохранение очищенных постов\"):\n",
    "        f.write(json.dumps(post, ensure_ascii=False) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
