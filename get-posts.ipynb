{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddb458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm.auto import tqdm\n",
    "from io import TextIOWrapper\n",
    "import pandas as pd\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def36e1",
   "metadata": {},
   "source": [
    "### Загрузка и подготовка данных о Telegram-каналах\n",
    "\n",
    "Эта ячейка выполняет следующие задачи:\n",
    "\n",
    "1. **Скачивает** архив `all.channels.csv.zip` из датасета на Hugging Face  \n",
    "   (`ajtkulov/telegram-ru`)\n",
    "\n",
    "2. **Распаковывает** его в локальную папку `data/posts/ajtkulov/meta`\n",
    "\n",
    "3. **Находит** внутри распакованный CSV-файл (независимо от того, как он называется)\n",
    "\n",
    "4. **Читает** CSV с максимально устойчивой обработкой проблем:\n",
    "   - пробует несколько популярных кодировок (utf-8-sig → cp1251 → utf-8 и др.)\n",
    "   - пропускает битые строки\n",
    "   - заменяет нераспознанные символы\n",
    "   - использует разделитель табуляцию (`\\t`)\n",
    "\n",
    "5. **Переименовывает** столбцы в понятные имена, если их количество совпадает с ожидаемым:\n",
    "   - `link`, `name`, `description`, `category`, `message_id`\n",
    "\n",
    "6. **Выводит** базовую статистику:\n",
    "   - общее количество каналов\n",
    "   - список столбцов\n",
    "   - первые 10 строк\n",
    "   - распределение по категориям (топ-150)\n",
    "   - список всех уникальных категорий\n",
    "\n",
    "7. **Сохраняет** таблицу с уникальными категориями в файл `unique_categories.csv`  \n",
    "   (удобно для дальнейшего анализа и маппинга)\n",
    "\n",
    "**Результат работы ячейки** — подготовленный pandas-датафрейм `df_channels` с мета-информацией о тысячах Telegram-каналов, готовый к дальнейшей фильтрации, выборке или использованию в пайплайне сбора постов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a142b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP-файл скачан: C:\\Users\\Admin\\.cache\\huggingface\\hub\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\meta\\all.channels.csv.zip\n",
      "Распаковка в папку: data/posts/ajtkulov/meta\n",
      "Файлы внутри ZIP: ['all.channels.csv']\n",
      "Распаковка завершена\n",
      "\n",
      "Чтение CSV из файла: data/posts/ajtkulov/meta\\all.channels.csv\n",
      "\n",
      "УСПЕХ! Файл прочитан с кодировкой: utf-8-sig\n",
      "\n",
      "Колонки успешно переименованы в: link, name, description, category, message_id\n",
      "Количество каналов: 380,467\n",
      "Количество столбцов: 5\n",
      "Столбцы: ['link', 'name', 'description', 'category', 'message_id']\n",
      "\n",
      "Первые 10 каналов:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>message_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://tgstat.ru/channel/@premium</td>\n",
       "      <td>Telegram Premium</td>\n",
       "      <td>Telegram Premium – a subscription that unlocks...</td>\n",
       "      <td>Telegram</td>\n",
       "      <td>7980100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://tgstat.ru/channel/sOj9iDAtUkMyYWQy</td>\n",
       "      <td>Топор Live</td>\n",
       "      <td>Нейтрально, без пропаганды. Топор Live с быстр...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>4388359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://tgstat.ru/channel/@wewantyoutodothejob</td>\n",
       "      <td>WeWantYou</td>\n",
       "      <td>Канал для поиска исполнителей для разных задач...</td>\n",
       "      <td>Другое</td>\n",
       "      <td>4158678.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://tgstat.ru/channel/@leoday</td>\n",
       "      <td>Леонардо Дайвинчик</td>\n",
       "      <td>Бот знакомств @leomatchbot</td>\n",
       "      <td>Юмор и развлечение</td>\n",
       "      <td>4057819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://tgstat.ru/channel/@novosti_efir</td>\n",
       "      <td>Прямой Эфир • Новости</td>\n",
       "      <td>️Все самое важное в одном канале. Новости Росс...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3886208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://tgstat.ru/channel/@novosti_voinaa</td>\n",
       "      <td>СМИ Россия не Москва</td>\n",
       "      <td>Эруктации информпространства России и ее Окраин.</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3368394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://tgstat.ru/channel/@rian_ru</td>\n",
       "      <td>РИА Новости</td>\n",
       "      <td>Главные Новости РИА t.me/rian_ru</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3220976.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://tgstat.ru/channel/@invest_zonaa</td>\n",
       "      <td>INVEST ZONE</td>\n",
       "      <td>Привет! Я Руслан, с 2017 года торгую рынок кри...</td>\n",
       "      <td>Криптовалюты</td>\n",
       "      <td>3057506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://tgstat.ru/channel/@mash</td>\n",
       "      <td>Mash</td>\n",
       "      <td>Прислать новость, фото, видео, аудио, бересту:...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>2827677.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://tgstat.ru/channel/@crypto_drop_stukach</td>\n",
       "      <td>Дропы от Стукача</td>\n",
       "      <td>Все о крипто раздачах, прибыльных темах и абуз...</td>\n",
       "      <td>Шок-конент</td>\n",
       "      <td>2636192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             link                   name  \\\n",
       "0              https://tgstat.ru/channel/@premium       Telegram Premium   \n",
       "1      https://tgstat.ru/channel/sOj9iDAtUkMyYWQy             Топор Live   \n",
       "2  https://tgstat.ru/channel/@wewantyoutodothejob              WeWantYou   \n",
       "3               https://tgstat.ru/channel/@leoday     Леонардо Дайвинчик   \n",
       "4         https://tgstat.ru/channel/@novosti_efir  Прямой Эфир • Новости   \n",
       "5       https://tgstat.ru/channel/@novosti_voinaa   СМИ Россия не Москва   \n",
       "6              https://tgstat.ru/channel/@rian_ru            РИА Новости   \n",
       "7         https://tgstat.ru/channel/@invest_zonaa            INVEST ZONE   \n",
       "8                 https://tgstat.ru/channel/@mash                   Mash   \n",
       "9  https://tgstat.ru/channel/@crypto_drop_stukach       Дропы от Стукача   \n",
       "\n",
       "                                         description            category  \\\n",
       "0  Telegram Premium – a subscription that unlocks...            Telegram   \n",
       "1  Нейтрально, без пропаганды. Топор Live с быстр...             Новости   \n",
       "2  Канал для поиска исполнителей для разных задач...              Другое   \n",
       "3                         Бот знакомств @leomatchbot  Юмор и развлечение   \n",
       "4  ️Все самое важное в одном канале. Новости Росс...             Новости   \n",
       "5   Эруктации информпространства России и ее Окраин.             Новости   \n",
       "6                   Главные Новости РИА t.me/rian_ru             Новости   \n",
       "7  Привет! Я Руслан, с 2017 года торгую рынок кри...        Криптовалюты   \n",
       "8  Прислать новость, фото, видео, аудио, бересту:...             Новости   \n",
       "9  Все о крипто раздачах, прибыльных темах и абуз...          Шок-конент   \n",
       "\n",
       "   message_id  \n",
       "0   7980100.0  \n",
       "1   4388359.0  \n",
       "2   4158678.0  \n",
       "3   4057819.0  \n",
       "4   3886208.0  \n",
       "5   3368394.0  \n",
       "6   3220976.0  \n",
       "7   3057506.0  \n",
       "8   2827677.0  \n",
       "9   2636192.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Распределение по category (топ-15):\n",
      "category\n",
      "Новости                                                               25909\n",
      "Блоги                                                                 25599\n",
      "Другое                                                                20693\n",
      "Мода и красота                                                        17673\n",
      "Психология                                                            13740\n",
      "                                                                      ...  \n",
      "Политика|||Регион|||Свердловская область                                 19\n",
      "Политика|||Регион|||Самарская область                                    19\n",
      "Путешествия|||Регион|||Приморский край                                   18\n",
      "Новости|||Регион|||Кабардино-Балкарская Республика|||Новости и СМИ       18\n",
      "Политика|||Регион|||Пермский край                                        18\n",
      "Name: count, Length: 150, dtype: int64\n",
      "\n",
      "Уникальные категории сохранены в файл: data/posts/ajtkulov/meta\\unique_categories.csv\n",
      "Количество уникальных категорий: 1748\n",
      "Первые 20 уникальных категорий:\n",
      "                                     category\n",
      "0                                    Telegram\n",
      "1                                     Новости\n",
      "2                                      Другое\n",
      "3                          Юмор и развлечение\n",
      "4                                Криптовалюты\n",
      "5                                  Шок-конент\n",
      "6   Новости|||Регион|||Москва|||Новости и СМИ\n",
      "7                                    Политика\n",
      "8                                   Экономика\n",
      "9                                  Технологии\n",
      "10                                      Блоги\n",
      "11                                    Продажи\n",
      "12                             Видео и фильмы\n",
      "13                                       Игры\n",
      "14                          Софт и приложения\n",
      "15                                    Карьера\n",
      "16                               Букмекерство\n",
      "17                                     Бизнес\n",
      "18                                Образование\n",
      "19                               Для взрослых\n",
      "CSV лежит здесь: data/posts/ajtkulov/meta\\all.channels.csv\n"
     ]
    }
   ],
   "source": [
    "# Параметры\n",
    "repo_id = \"ajtkulov/telegram-ru\"\n",
    "meta_zip_filename = \"meta/all.channels.csv.zip\"          # путь в репозитории\n",
    "output_meta_dir = \"data/posts/ajtkulov/meta\"             # куда сохраняем\n",
    "\n",
    "os.makedirs(output_meta_dir, exist_ok=True)\n",
    "\n",
    "# Шаг 1: Скачивание ZIP-файла\n",
    "try:\n",
    "    zip_path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=meta_zip_filename,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    print(f\"ZIP-файл скачан: {zip_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка скачивания: {e}\")\n",
    "    raise\n",
    "\n",
    "# Шаг 2: Распаковка в нужную папку\n",
    "print(\"Распаковка в папку:\", output_meta_dir)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    inner_files = zip_ref.namelist()\n",
    "    print(f\"Файлы внутри ZIP: {inner_files}\")\n",
    "    \n",
    "    if not inner_files:\n",
    "        raise ValueError(\"ZIP-файл пустой\")\n",
    "    \n",
    "    zip_ref.extractall(output_meta_dir)\n",
    "    print(\"Распаковка завершена\")\n",
    "\n",
    "# Шаг 3: Поиск распакованного CSV-файла\n",
    "csv_path = None\n",
    "for file in os.listdir(output_meta_dir):\n",
    "    if file.lower().endswith('.csv'):\n",
    "        csv_path = os.path.join(output_meta_dir, file)\n",
    "        break\n",
    "\n",
    "if not csv_path:\n",
    "    raise FileNotFoundError(\"CSV-файл не найден после распаковки. Проверьте папку.\")\n",
    "\n",
    "print(f\"\\nЧтение CSV из файла: {csv_path}\")\n",
    "\n",
    "# Шаг 4: Чтение с обработкой всех типичных проблем\n",
    "encodings = ['utf-8-sig', 'cp1251', 'utf-8', 'latin1', 'iso-8859-1']\n",
    "df_channels = None\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        df_channels = pd.read_csv(\n",
    "            csv_path,\n",
    "            encoding=encoding,\n",
    "            sep='\\t',                  # табуляция (как у вас)\n",
    "            on_bad_lines='skip',       # пропуск битых строк\n",
    "            low_memory=False,\n",
    "            encoding_errors='replace'  # замена кракозябр\n",
    "        )\n",
    "        print(f\"\\nУСПЕХ! Файл прочитан с кодировкой: {encoding}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Попытка с {encoding} провалилась: {str(e)}\")\n",
    "\n",
    "if df_channels is None:\n",
    "    raise ValueError(\"Не удалось прочитать CSV ни одной кодировкой. Возможно файл повреждён.\")\n",
    "\n",
    "# ← Добавляем названия колонок\n",
    "column_names = [\"link\", \"name\", \"description\", \"category\", \"message_id\"]\n",
    "if len(df_channels.columns) == len(column_names):\n",
    "    df_channels.columns = column_names\n",
    "    print(\"\\nКолонки успешно переименованы в: link, name, description, category, message_id\")\n",
    "else:\n",
    "    print(f\"\\nВнимание! Количество столбцов ({len(df_channels.columns)}) не равно ожидаемому ({len(column_names)}).\")\n",
    "    print(\"Колонки НЕ переименованы. Текущие:\", df_channels.columns.tolist())\n",
    "\n",
    "\n",
    "# Общая статистика\n",
    "print(f\"Количество каналов: {len(df_channels):,}\")\n",
    "print(f\"Количество столбцов: {len(df_channels.columns)}\")\n",
    "print(\"Столбцы:\", df_channels.columns.tolist())\n",
    "\n",
    "# Первые 10 строк\n",
    "print(\"\\nПервые 10 каналов:\")\n",
    "display(df_channels.head(10))\n",
    "\n",
    "# Распределение категорий\n",
    "if 'category' in df_channels.columns:\n",
    "    print(\"\\nРаспределение по category (топ-15):\")\n",
    "    print(df_channels['category'].value_counts().head(150))\n",
    "else:\n",
    "    print(\"\\nКолонка 'category' не найдена\")\n",
    "\n",
    "if 'category' in df_channels.columns:\n",
    "    unique_categories = df_channels['category'].dropna().unique()\n",
    "    unique_df = pd.DataFrame(unique_categories, columns=['category'])\n",
    "    \n",
    "    unique_csv_path = os.path.join(output_meta_dir, \"unique_categories.csv\")\n",
    "    unique_df.to_csv(unique_csv_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\nУникальные категории сохранены в файл: {unique_csv_path}\")\n",
    "    print(f\"Количество уникальных категорий: {len(unique_categories)}\")\n",
    "    print(\"Первые 20 уникальных категорий:\")\n",
    "    print(unique_df.head(20))\n",
    "else:\n",
    "    print(\"\\nНе удалось сохранить категории — колонка 'category' отсутствует\")\n",
    "\n",
    "print(\"CSV лежит здесь:\", csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc46d4",
   "metadata": {},
   "source": [
    "### Фильтрация категорий Telegram-каналов (удаление нежелательных)\n",
    "\n",
    "Эта ячейка очищает список уникальных категорий, полученный на предыдущем шаге, от нежелательных, слишком общих или проблемных тематик.\n",
    "\n",
    "**Что делает ячейка:**\n",
    "\n",
    "1. Читает файл `unique_categories.csv` (созданный ранее)\n",
    "2. Применяет жёсткую фильтрацию по двум уровням:\n",
    "   - **Точное совпадение** с запрещённым списком (`FORBIDDEN_PATTERNS`):\n",
    "     - Новости, Политика, Шок-контент, Darknet, Для взрослых, Эротика, Право, Религия, Инстаграм, Другое, Telegram и др.\n",
    "   - **Содержит подстроку** из списка `FORBIDDEN_SUBSTRINGS` (даже частичное вхождение):\n",
    "     - Новости, Политика, Шок, Darknet\n",
    "3. Удаляет все строки, попавшие под любой из этих критериев\n",
    "4. Убирает возможные дубликаты и пустые значения\n",
    "5. Сохраняет очищенный список в новый файл:  \n",
    "   `data/posts/ajtkulov/meta/filtered_categories.csv`\n",
    "\n",
    "**Цель фильтрации**  \n",
    "Оставить только потенциально полезные, нишевые категории, которые могут быть связаны с товарами, услугами, хобби, техникой, красотой, спортом и т.п.  \n",
    "Исключаются:  \n",
    "- новостные / политические / шок-контент каналы  \n",
    "- слишком общие категории  \n",
    "- взрослый контент и сомнительные тематики\n",
    "\n",
    "**Результат**  \n",
    "Чистый, компактный список категорий, который удобно использовать для:\n",
    "- выборки каналов по тематике\n",
    "- последующего сбора постов\n",
    "- маппинга на категории товаров\n",
    "\n",
    "После выполнения ячейки рекомендуется посмотреть на первые 20–30 строк результата и при необходимости дополнить список запрещённых паттернов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7631edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Чтение файла: data/posts/ajtkulov/meta/unique_categories.csv\n",
      "Всего уникальных категорий в исходном файле: 1,748\n",
      "Первые 10 категорий:\n",
      "                                    category\n",
      "0                                   Telegram\n",
      "1                                    Новости\n",
      "2                                     Другое\n",
      "3                         Юмор и развлечение\n",
      "4                               Криптовалюты\n",
      "5                                 Шок-конент\n",
      "6  Новости|||Регион|||Москва|||Новости и СМИ\n",
      "7                                   Политика\n",
      "8                                  Экономика\n",
      "9                                 Технологии\n",
      "\n",
      "Запрещённые категории / подстроки (удаляем их):\n",
      " - Новости\n",
      " - Политика\n",
      " - Шок-контент\n",
      " - Шок-конент\n",
      " - Darknet\n",
      " - Для взрослых\n",
      " - Эротика\n",
      " - Право\n",
      " - Религия\n",
      " - Инстаграм\n",
      " - Другое\n",
      " - Telegram\n",
      " - Новости\n",
      " - Политика\n",
      " - Шок\n",
      " - Darknet\n",
      "\n",
      "После фильтрации осталось категорий: 1,417\n",
      "\n",
      "Первые 20 оставшихся категорий:\n",
      "              category\n",
      "3   Юмор и развлечение\n",
      "4         Криптовалюты\n",
      "8            Экономика\n",
      "9           Технологии\n",
      "10               Блоги\n",
      "11             Продажи\n",
      "12      Видео и фильмы\n",
      "13                Игры\n",
      "14   Софт и приложения\n",
      "15             Карьера\n",
      "16        Букмекерство\n",
      "17              Бизнес\n",
      "18         Образование\n",
      "20         Путешествия\n",
      "22     Картинки и фото\n",
      "23      Мода и красота\n",
      "24      Позновательное\n",
      "25              Музыка\n",
      "26        Семья и дети\n",
      "28     Еда и кулинария\n",
      "\n",
      "Отфильтрованный файл сохранён: data/posts/ajtkulov/meta/filtered_categories.csv\n",
      "Теперь в нём только нишевые / полезные категории\n"
     ]
    }
   ],
   "source": [
    "# Ячейка: Фильтрация категорий (убираем ненужные, включая подстроки)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Путь к файлу с уникальными категориями\n",
    "input_csv = 'data/posts/ajtkulov/meta/unique_categories.csv'\n",
    "\n",
    "# Путь для сохранения отфильтрованного файла\n",
    "output_csv = 'data/posts/ajtkulov/meta/filtered_categories.csv'\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\"Чтение файла: {input_csv}\")\n",
    "\n",
    "# Читаем файл (учитывая BOM и кодировку)\n",
    "df = pd.read_csv(input_csv, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Всего уникальных категорий в исходном файле: {len(df):,}\")\n",
    "print(\"Первые 10 категорий:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Список запрещённых категорий и подстрок (расширенный)\n",
    "FORBIDDEN_PATTERNS = [\n",
    "    'Новости', 'Политика', 'Шок-контент', 'Шок-конент', 'Darknet',\n",
    "    'Для взрослых', 'Эротика', 'Право', 'Религия', 'Инстаграм',\n",
    "    'Другое', 'Telegram',  # слишком общие\n",
    "]\n",
    "\n",
    "# Дополнительно: любые категории, содержащие эти слова (даже с |||)\n",
    "FORBIDDEN_SUBSTRINGS = ['Новости', 'Политика', 'Шок', 'Darknet']\n",
    "\n",
    "print(\"\\nЗапрещённые категории / подстроки (удаляем их):\")\n",
    "for p in FORBIDDEN_PATTERNS + FORBIDDEN_SUBSTRINGS:\n",
    "    print(f\" - {p}\")\n",
    "\n",
    "# Фильтрация:\n",
    "# 1. Точное совпадение с запрещённым списком\n",
    "# 2. Содержит любую запрещённую подстроку\n",
    "mask = (\n",
    "    df['category'].isin(FORBIDDEN_PATTERNS) |\n",
    "    df['category'].str.contains('|'.join(FORBIDDEN_SUBSTRINGS), na=False, regex=True)\n",
    ")\n",
    "\n",
    "filtered_df = df[~mask].copy()\n",
    "\n",
    "# Убираем дубликаты и NaN\n",
    "filtered_df = filtered_df.dropna(subset=['category']).drop_duplicates(subset=['category'])\n",
    "\n",
    "print(f\"\\nПосле фильтрации осталось категорий: {len(filtered_df):,}\")\n",
    "\n",
    "print(\"\\nПервые 20 оставшихся категорий:\")\n",
    "print(filtered_df.head(20))\n",
    "\n",
    "# Сохранение отфильтрованного файла\n",
    "filtered_df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nОтфильтрованный файл сохранён: {output_csv}\")\n",
    "print(\"Теперь в нём только нишевые / полезные категории\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f33bf1",
   "metadata": {},
   "source": [
    "### Очистка первого столбца мета-данных (нормализация имён каналов)\n",
    "\n",
    "Эта ячейка приводит первый столбец исходного файла `all.channels.csv` к единому, чистому и удобному формату — остаётся **только username канала** (например, `channelname` вместо `https://tgstat.ru/channel/@channelname` или `@channelname`).\n",
    "\n",
    "**Зачем это нужно:**\n",
    "\n",
    "В основном датасете каналы представлены в разных форматах:  \n",
    "- полные ссылки `https://tgstat.ru/channel/@username`  \n",
    "- ссылки с @ в начале `@username`  \n",
    "- иногда просто username без префиксов  \n",
    "\n",
    "Такой разнобой мешает:  \n",
    "- корректно искать каналы по имени  \n",
    "- фильтровать по категориям  \n",
    "- сопоставлять с другими источниками (например, с постами, собранными через Telethon)  \n",
    "- формировать единообразные ссылки `t.me/username`  \n",
    "\n",
    "Поэтому проводится **нормализация** — из всех вариантов остаётся только чистое имя канала без префиксов, @ и лишних символов. Это критически важно для дальнейшей работы: поиска по категориям, выборки каналов, сбора постов и построения связей «категория → канал → пост → товар».\n",
    "\n",
    "**Что делает ячейка шаг за шагом:**\n",
    "\n",
    "1. Читает файл `all.channels.csv`  \n",
    "   - Сначала пробует `utf-8-sig` + разделитель `\\t`  \n",
    "   - При ошибке — fallback на `cp1251`\n",
    "\n",
    "2. Если первый столбец без имени и содержит `https://tgstat.ru` → переименовывает его в `link`\n",
    "\n",
    "3. Применяет функцию очистки:\n",
    "   - Удаляет префикс `https://tgstat.ru/channel/`\n",
    "   - Убирает ведущий `@`\n",
    "   - Удаляет лишние пробелы\n",
    "   - Обрабатывает пропущенные значения\n",
    "\n",
    "4. Показывает первые 10 строк для визуального контроля\n",
    "\n",
    "5. Сохраняет результат в новый файл:  \n",
    "   `data/posts/ajtkulov/meta/all_channels_clean.csv`\n",
    "\n",
    "**Результат**  \n",
    "Файл с нормализованными именами каналов, готовый к использованию в следующих шагах:  \n",
    "- фильтрация по категориям  \n",
    "- выбор целевых каналов  \n",
    "- сбор постов через Telegram API  \n",
    "- дальнейшее сопоставление с товарами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b713148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Чтение файла: data/posts/ajtkulov/meta/all.channels.csv\n",
      "Файл успешно прочитан. Строк: 380,467\n",
      "Первый столбец не имеет имени — переименовываем в 'link'\n",
      "\n",
      "Очистка первого столбца...\n",
      "\n",
      "Первые 10 строк после очистки:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>Топор 18+</th>\n",
       "      <th>Самый популярный русскоязычный Telegram канал.</th>\n",
       "      <th>Шок-конент</th>\n",
       "      <th>8179938</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>premium</td>\n",
       "      <td>Telegram Premium</td>\n",
       "      <td>Telegram Premium – a subscription that unlocks...</td>\n",
       "      <td>Telegram</td>\n",
       "      <td>7980100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sOj9iDAtUkMyYWQy</td>\n",
       "      <td>Топор Live</td>\n",
       "      <td>Нейтрально, без пропаганды. Топор Live с быстр...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>4388359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wewantyoutodothejob</td>\n",
       "      <td>WeWantYou</td>\n",
       "      <td>Канал для поиска исполнителей для разных задач...</td>\n",
       "      <td>Другое</td>\n",
       "      <td>4158678.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leoday</td>\n",
       "      <td>Леонардо Дайвинчик</td>\n",
       "      <td>Бот знакомств @leomatchbot</td>\n",
       "      <td>Юмор и развлечение</td>\n",
       "      <td>4057819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>novosti_efir</td>\n",
       "      <td>Прямой Эфир • Новости</td>\n",
       "      <td>️Все самое важное в одном канале. Новости Росс...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3886208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>novosti_voinaa</td>\n",
       "      <td>СМИ Россия не Москва</td>\n",
       "      <td>Эруктации информпространства России и ее Окраин.</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3368394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rian_ru</td>\n",
       "      <td>РИА Новости</td>\n",
       "      <td>Главные Новости РИА t.me/rian_ru</td>\n",
       "      <td>Новости</td>\n",
       "      <td>3220976.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>invest_zonaa</td>\n",
       "      <td>INVEST ZONE</td>\n",
       "      <td>Привет! Я Руслан, с 2017 года торгую рынок кри...</td>\n",
       "      <td>Криптовалюты</td>\n",
       "      <td>3057506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mash</td>\n",
       "      <td>Mash</td>\n",
       "      <td>Прислать новость, фото, видео, аудио, бересту:...</td>\n",
       "      <td>Новости</td>\n",
       "      <td>2827677.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>crypto_drop_stukach</td>\n",
       "      <td>Дропы от Стукача</td>\n",
       "      <td>Все о крипто раздачах, прибыльных темах и абуз...</td>\n",
       "      <td>Шок-конент</td>\n",
       "      <td>2636192.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  link              Топор 18+  \\\n",
       "0              premium       Telegram Premium   \n",
       "1     sOj9iDAtUkMyYWQy             Топор Live   \n",
       "2  wewantyoutodothejob              WeWantYou   \n",
       "3               leoday     Леонардо Дайвинчик   \n",
       "4         novosti_efir  Прямой Эфир • Новости   \n",
       "5       novosti_voinaa   СМИ Россия не Москва   \n",
       "6              rian_ru            РИА Новости   \n",
       "7         invest_zonaa            INVEST ZONE   \n",
       "8                 mash                   Mash   \n",
       "9  crypto_drop_stukach       Дропы от Стукача   \n",
       "\n",
       "      Самый популярный русскоязычный Telegram канал.          Шок-конент  \\\n",
       "0  Telegram Premium – a subscription that unlocks...            Telegram   \n",
       "1  Нейтрально, без пропаганды. Топор Live с быстр...             Новости   \n",
       "2  Канал для поиска исполнителей для разных задач...              Другое   \n",
       "3                         Бот знакомств @leomatchbot  Юмор и развлечение   \n",
       "4  ️Все самое важное в одном канале. Новости Росс...             Новости   \n",
       "5   Эруктации информпространства России и ее Окраин.             Новости   \n",
       "6                   Главные Новости РИА t.me/rian_ru             Новости   \n",
       "7  Привет! Я Руслан, с 2017 года торгую рынок кри...        Криптовалюты   \n",
       "8  Прислать новость, фото, видео, аудио, бересту:...             Новости   \n",
       "9  Все о крипто раздачах, прибыльных темах и абуз...          Шок-конент   \n",
       "\n",
       "     8179938  \n",
       "0  7980100.0  \n",
       "1  4388359.0  \n",
       "2  4158678.0  \n",
       "3  4057819.0  \n",
       "4  3886208.0  \n",
       "5  3368394.0  \n",
       "6  3220976.0  \n",
       "7  3057506.0  \n",
       "8  2827677.0  \n",
       "9  2636192.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Очищенный файл сохранён: data/posts/ajtkulov/meta/all_channels_clean.csv\n",
      "Первый столбец теперь содержит только имена каналов (без https и @)\n"
     ]
    }
   ],
   "source": [
    "# Ячейка: Очистка первого столбца (оставляем только имя канала)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Путь к исходному файлу\n",
    "input_file = 'data/posts/ajtkulov/meta/all.channels.csv'\n",
    "\n",
    "# Путь для сохранения очищенного файла\n",
    "output_file = 'data/posts/ajtkulov/meta/all_channels_clean.csv'\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(f\"Чтение файла: {input_file}\")\n",
    "\n",
    "# Читаем CSV (учитываем возможные проблемы с кодировкой и разделителями)\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        input_file,\n",
    "        encoding='utf-8-sig',\n",
    "        sep='\\t',                  # если табуляция\n",
    "        on_bad_lines='skip',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(f\"Файл успешно прочитан. Строк: {len(df):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка чтения: {e}\")\n",
    "    # Попробуем другую кодировку\n",
    "    df = pd.read_csv(\n",
    "        input_file,\n",
    "        encoding='cp1251',\n",
    "        sep='\\t',\n",
    "        on_bad_lines='skip',\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(\"Успешно прочитано с cp1251\")\n",
    "\n",
    "# Проверяем, есть ли первый столбец (по умолчанию он без имени — берём по индексу)\n",
    "if df.columns[0].startswith('https://tgstat.ru'):\n",
    "    print(\"Первый столбец не имеет имени — переименовываем в 'link'\")\n",
    "    df = df.rename(columns={df.columns[0]: 'link'})\n",
    "\n",
    "# Очистка первого столбца\n",
    "def clean_channel_link(link):\n",
    "    if pd.isna(link):\n",
    "        return link\n",
    "    link = str(link).strip()\n",
    "    # Убираем префикс https://tgstat.ru/channel/\n",
    "    if link.startswith('https://tgstat.ru/channel/'):\n",
    "        link = link.replace('https://tgstat.ru/channel/', '')\n",
    "    # Убираем @ в начале, если остался\n",
    "    if link.startswith('@'):\n",
    "        link = link[1:]\n",
    "    return link\n",
    "\n",
    "print(\"\\nОчистка первого столбца...\")\n",
    "df['link'] = df['link'].apply(clean_channel_link)\n",
    "\n",
    "# Показываем результат\n",
    "print(\"\\nПервые 10 строк после очистки:\")\n",
    "display(df.head(10))\n",
    "\n",
    "# Сохранение нового файла\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nОчищенный файл сохранён: {output_file}\")\n",
    "print(f\"Первый столбец теперь содержит только имена каналов (без https и @)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb88ee3",
   "metadata": {},
   "source": [
    "### Подготовка данных и настроек для выборки постов по целевым категориям\n",
    "\n",
    "Эта ячейка — **стартовый блок** большого пайплайна по отбору и скачиванию постов из Telegram-каналов.  \n",
    "Она выполняет всю предварительную настройку и проверку, чтобы дальше можно было безопасно и осмысленно работать с огромным датасетом `ajtkulov/telegram-ru`.\n",
    "\n",
    "**Что именно делает ячейка:**\n",
    "\n",
    "1. **Определяет ключевые пути и настройки** (всё в одном месте для удобства изменения):\n",
    "   - репозиторий на Hugging Face\n",
    "   - папки для сохранения результатов и кэша\n",
    "   - файлы прогресса, статистики и временных результатов\n",
    "   - пути к уже подготовленным мета-файлам (`all_channels_clean.csv` и `filtered_categories.csv`)\n",
    "\n",
    "2. **Проверяет наличие критически важных файлов**  \n",
    "   Если хотя бы один из них отсутствует — сразу завершает работу с понятным сообщением об ошибке.\n",
    "\n",
    "3. **Загружает очищенный список каналов** (`all_channels_clean.csv`)  \n",
    "   → содержит чистые usernames каналов (уже нормализованные на предыдущем шаге)\n",
    "\n",
    "4. **Загружает отфильтрованный список разрешённых категорий** (`filtered_categories.csv`)  \n",
    "   → только те категории, которые мы решили оставить после очистки от новостей, политики, шок-контента и т.п.\n",
    "\n",
    "5. **Создаёт словарь сопоставления** `channel → category`  \n",
    "   → позволяет быстро узнать, к какой категории относится любой канал  \n",
    "   → используется дальше для фильтрации постов именно по нужным тематикам\n",
    "\n",
    "6. **Формирует множество разрешённых категорий** (`allowed_categories`)  \n",
    "   → будет использоваться как фильтр при обработке ZIP-архивов с постами\n",
    "\n",
    "7. **Выводит диагностическую информацию** для контроля:\n",
    "   - сколько всего каналов и категорий загружено\n",
    "   - первые 10 разрешённых категорий (чтобы сразу увидеть, что фильтр сработал корректно)\n",
    "   - предупреждения, если структура файлов не соответствует ожидаемой\n",
    "\n",
    "**Зачем это важно**\n",
    "\n",
    "Без этой ячейки дальнейшая работа невозможна или приведёт к ошибкам:  \n",
    "- мы будем обрабатывать ненужные категории (новости, политика, эротика и т.д.)  \n",
    "- не сможем связать посты с категориями товаров  \n",
    "- потеряем контроль над тем, какие каналы и посты мы вообще скачиваем  \n",
    "\n",
    "**Результат выполнения**  \n",
    "Готовые к использованию структуры данных:  \n",
    "- `all_channels_clean` — DataFrame со всеми каналами  \n",
    "- `channel_to_category` — словарь channel → категория  \n",
    "- `allowed_categories` — множество только нужных категорий  \n",
    "\n",
    "Дальше код будет скачивать ZIP-архивы из датасета, фильтровать посты именно по этим категориям и сохранять отобранные посты для последующей генерации/разметки пар «пост — товар»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510b5ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка информации о каналах и категориях...\n",
      "Загружено каналов: 380467\n",
      "Загружено категорий для фильтрации: 1417\n",
      "ОШИБКА: В файле all_channels_clean.csv должны быть колонки 'channel' и 'category'\n",
      "Найдены колонки: ['link', 'Топор 18+', 'Самый популярный русскоязычный Telegram канал.', 'Шок-конент', '8179938']\n",
      "Разрешено 1417 категорий\n",
      "Первые 10 разрешенных категорий:\n",
      "  1. Бизнес|||Регион|||Ульяновская область|||Бизнес и стартапы\n",
      "  2. Позновательное|||Регион|||Тульская область|||Познавательное\n",
      "  3. Регион|||Саратовская область|||Юмор и развлечения|||Юмор и развлечение\n",
      "  4. Регион|||Московская область|||Блоги\n",
      "  5. Образование|||Регион|||Республика Саха (Якутия)\n",
      "  6. Медицина|||Регион|||Ивановская область\n",
      "  7. Путешествия|||Регион|||Карачаево-Черкесская республика\n",
      "  8. Регион|||Свердловская область|||Познавательное\n",
      "  9. Позновательное|||Регион|||Московская область|||Познавательное\n",
      "  10. Регион|||Республика Крым|||Блоги\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm.auto import tqdm\n",
    "from io import TextIOWrapper\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# ==============================================\n",
    "# НАСТРОЙКИ (изменяйте здесь)\n",
    "# ==============================================\n",
    "\n",
    "repo_id = \"ajtkulov/telegram-ru\"\n",
    "\n",
    "output_dir = \"data/posts/ajtkulov/selected\"               # куда сохранять финальный результат\n",
    "cache_dir = os.path.join(output_dir, \"cache\")             # временный кэш для ZIP\n",
    "progress_file = os.path.join(output_dir, \"progress.json\") # какие ZIP уже обработаны\n",
    "stats_file = os.path.join(output_dir, \"channel_top_posts.json\")  # топ-посты по каналам\n",
    "selected_posts_file = os.path.join(output_dir, \"selected_posts_temp.jsonl\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Пути к файлам с категориями\n",
    "channels_file = \"data/posts/ajtkulov/meta/all_channels_clean.csv\"\n",
    "categories_file = \"data/posts/ajtkulov/meta/filtered_categories.csv\"\n",
    "\n",
    "# Проверка существования файлов\n",
    "print(\"Загрузка информации о каналах и категориях...\")\n",
    "if not os.path.exists(channels_file):\n",
    "    print(f\"ОШИБКА: Файл не найден: {channels_file}\")\n",
    "    print(\"Убедитесь, что файл all_channels_clean.csv находится в папке data/posts/ajtkulov/\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(categories_file):\n",
    "    print(f\"ОШИБКА: Файл не найден: {categories_file}\")\n",
    "    print(\"Убедитесь, что файл filtered_categories.csv находится в папке data/posts/ajtkulov/\")\n",
    "    exit()\n",
    "\n",
    "# Загружаем данные о каналах и категориях\n",
    "all_channels_clean = pd.read_csv(channels_file)\n",
    "filtered_categories = pd.read_csv(categories_file)\n",
    "\n",
    "print(f\"Загружено каналов: {len(all_channels_clean)}\")\n",
    "print(f\"Загружено категорий для фильтрации: {len(filtered_categories)}\")\n",
    "\n",
    "# Создаем словарь channel -> category\n",
    "channel_to_category = {}\n",
    "if 'category' in all_channels_clean.columns and 'channel' in all_channels_clean.columns:\n",
    "    for _, row in all_channels_clean.iterrows():\n",
    "        channel_to_category[row['channel']] = row['category']\n",
    "    print(f\"Создан словарь категорий для {len(channel_to_category)} каналов\")\n",
    "else:\n",
    "    print(\"ОШИБКА: В файле all_channels_clean.csv должны быть колонки 'channel' и 'category'\")\n",
    "    print(f\"Найдены колонки: {list(all_channels_clean.columns)}\")\n",
    "    exit()\n",
    "\n",
    "# Создаем множество разрешенных категорий\n",
    "if 'category' in filtered_categories.columns:\n",
    "    allowed_categories = set(filtered_categories['category'].dropna().astype(str).tolist())\n",
    "    print(f\"Разрешено {len(allowed_categories)} категорий\")\n",
    "    print(\"Первые 10 разрешенных категорий:\")\n",
    "    for i, cat in enumerate(list(allowed_categories)[:10]):\n",
    "        print(f\"  {i+1}. {cat}\")\n",
    "else:\n",
    "    print(\"ОШИБКА: В файле filtered_categories.csv должна быть колонка 'category'\")\n",
    "    print(f\"Найдены колонки: {list(filtered_categories.columns)}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb895c",
   "metadata": {},
   "source": [
    "### Основной пайплайн: скачивание, фильтрация и отбор качественных постов из Telegram-каналов\n",
    "\n",
    "Это **главная рабочая ячейка** всего проекта по подготовке датасета постов.  \n",
    "Она скачивает огромный датасет `ajtkulov/telegram-ru` (230 ZIP-архивов), фильтрует посты по категориям каналов, длине текста и стратегии отбора, а затем сохраняет только лучшие / средние по популярности посты в удобных форматах.\n",
    "\n",
    "**Ключевые параметры (настраиваются в начале):**\n",
    "\n",
    "- `MAX_POSTS_PER_CHANNEL = 5` — максимум постов с одного канала  \n",
    "- `MIN_TEXT_LEN = 500` / `MAX_TEXT_LEN = 1200` — диапазон длины текста поста  \n",
    "- `MAX_TOTAL_RECORDS = 700000` — жёсткий глобальный лимит (не превысим)  \n",
    "- `file_names = [tg.0.zip … tg.229.zip]` — список всех ZIP-файлов (можно ограничить для теста)\n",
    "\n",
    "**Основная логика работы:**\n",
    "\n",
    "1. **Загрузка предыдущего прогресса** (если запуск не первый):\n",
    "   - `channel_top_posts` — топ-посты по каналам\n",
    "   - `verified_channels_cache` — кэш проверенных категорий\n",
    "   - `selected_posts_temp.jsonl` — уже отобранные посты\n",
    "   - `progress.json` — список обработанных ZIP\n",
    "\n",
    "2. **Функция фильтрации каналов** `is_channel_allowed()`  \n",
    "   Проверяет, относится ли канал к одной из **разрешённых категорий** (из `filtered_categories.csv`).  \n",
    "   Использует быстрый кэш, чтобы не проверять один и тот же канал тысячи раз.\n",
    "\n",
    "3. **Умный отбор постов из середины** `select_mid_range_posts()`  \n",
    "   Вместо топовых постов (которые часто — реклама / кликбейт) или самых непопулярных берём **середину** по просмотрам.  \n",
    "   Это даёт более естественный, типичный контент канала.\n",
    "\n",
    "4. **Основной цикл по ZIP-файлам**:\n",
    "   - Скачивает каждый ZIP с Hugging Face (с кэшированием)\n",
    "   - Распаковывает и читает построчно (JSONL внутри ZIP)\n",
    "   - Для каждого поста:\n",
    "     - Проверяет длину текста (500–1200 символов)\n",
    "     - Проверяет категорию канала\n",
    "     - Конвертирует просмотры (K → ×1000, M → ×1M)\n",
    "     - Сохраняет в временный буфер `channel_top_posts[channel]`\n",
    "   - Если буфер канала переполняется → оставляет только топ-20 (буфер ×4)\n",
    "   - Периодически выводит статистику\n",
    "\n",
    "5. **Промежуточный отбор после каждого ZIP**:\n",
    "   - Из каждого канала берёт 5 постов из середины рейтинга\n",
    "   - Дописывает их в `selected_posts_temp.jsonl` (режим append)\n",
    "   - Сохраняет прогресс, статистику и кэш\n",
    "   - Удаляет скачанный ZIP, чтобы не занимать место на диске\n",
    "\n",
    "6. **Финальный этап** (после всех ZIP или достижения лимита):\n",
    "   - Собирает все отобранные посты из временного файла\n",
    "   - Ограничивает до `MAX_TOTAL_RECORDS`, если превысили\n",
    "   - Сохраняет результат в **несколько форматов**:\n",
    "     - `selected_posts.csv` — табличный вид (удобно для анализа)\n",
    "     - `selected_posts.jsonl` — построчно (для больших данных)\n",
    "     - `selected_posts_pretty.json` — красиво отформатированный JSON\n",
    "     - `selected_posts.json` — компактный JSON\n",
    "   - Создаёт сводную статистику `processing_summary.json`\n",
    "   - Удаляет временный файл\n",
    "\n",
    "**Результат выполнения**  \n",
    "Готовый качественный датасет постов из нишевых Telegram-каналов:  \n",
    "- только разрешённые категории  \n",
    "- текст нормальной длины (500–1200 символов)  \n",
    "- не топовые / не мусорные посты, а «средние по больнице»  \n",
    "- до 700 000 записей (или меньше, если постов подходящего качества меньше)  \n",
    "- несколько форматов на выбор + полная статистика обработки\n",
    "\n",
    "Этот датасет идеально подходит для следующих шагов:  \n",
    "- генерации описаний товаров по постам  \n",
    "- разметки пар «пост — товар»  \n",
    "- обучения моделей релевантности / ранжирования\n",
    "\n",
    "**Важно:** ячейка устойчива к перезапускам — продолжает с того места, где остановилась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d23a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено статистики по 9,836 каналам\n",
      "Загружен кэш для 13245 каналов\n",
      "Уже отобрано постов из предыдущих запусков: 382,176\n",
      "Уже обработано ZIP-файлов: 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea2c58d1b23424db172d78630566e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка ZIP-файлов:   0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tg.0.zip уже обработан — пропуск\n",
      "tg.1.zip уже обработан — пропуск\n",
      "tg.2.zip уже обработан — пропуск\n",
      "tg.3.zip уже обработан — пропуск\n",
      "tg.4.zip уже обработан — пропуск\n",
      "tg.5.zip уже обработан — пропуск\n",
      "tg.6.zip уже обработан — пропуск\n",
      "tg.7.zip уже обработан — пропуск\n",
      "tg.8.zip уже обработан — пропуск\n",
      "tg.9.zip уже обработан — пропуск\n",
      "tg.10.zip уже обработан — пропуск\n",
      "tg.11.zip уже обработан — пропуск\n",
      "tg.12.zip уже обработан — пропуск\n",
      "tg.13.zip уже обработан — пропуск\n",
      "tg.14.zip уже обработан — пропуск\n",
      "\n",
      "=== tg.15.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dfa9a75e954cbc9c2b145a867eeabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.15.zip:   3%|2         | 21.0M/837M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачано: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.15.zip\n",
      "Внутри ZIP файлов: 899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\thesis\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Projects\\thesis\\data\\posts\\ajtkulov\\selected\\cache\\datasets--ajtkulov--telegram-ru. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b859598f88a14bd2a75c7c70ffb713a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Внутренние файлы:   0%|          | 0/899 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано: 1,100,000 | Каналов с разрешенной категорией: 152 | Потенциальных отобранных: 48,933\n",
      "Обработано: 2,400,000 | Каналов с разрешенной категорией: 380 | Потенциальных отобранных: 50,017\n",
      "Обработано: 2,900,000 | Каналов с разрешенной категорией: 462 | Потенциальных отобранных: 50,406\n",
      "Обработано: 3,200,000 | Каналов с разрешенной категорией: 501 | Потенциальных отобранных: 50,589\n",
      "Промежуточный отбор постов из середины рейтинга...\n",
      "ZIP удалён: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.15.zip\n",
      "Статистика после обработки tg.15.zip:\n",
      "  - Обработано записей: 4,263,300\n",
      "  - Отобрано постов всего: 433,515\n",
      "  - Каналов с разрешенной категорией: 658\n",
      "\n",
      "=== tg.16.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2492b345fa504660bcf33f114b80d34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.16.zip:   0%|          | 0.00/959M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачано: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.16.zip\n",
      "Внутри ZIP файлов: 898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be312d1ef942479e8e181059a25a2997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Внутренние файлы:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано: 6,200,000 | Каналов с разрешенной категорией: 916 | Потенциальных отобранных: 52,558\n",
      "Обработано: 8,900,000 | Каналов с разрешенной категорией: 1,276 | Потенциальных отобранных: 54,293\n",
      "Промежуточный отбор постов из середины рейтинга...\n",
      "ZIP удалён: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.16.zip\n",
      "Статистика после обработки tg.16.zip:\n",
      "  - Обработано записей: 9,094,177\n",
      "  - Отобрано постов всего: 487,951\n",
      "  - Каналов с разрешенной категорией: 1,306\n",
      "\n",
      "=== tg.17.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63719a736b8d4bc3af639122435760d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.17.zip:   0%|          | 0.00/852M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачано: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.17.zip\n",
      "Внутри ZIP файлов: 885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f763a0df7354c89950ee9968ff033cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Внутренние файлы:   0%|          | 0/885 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано: 10,600,000 | Каналов с разрешенной категорией: 1,593 | Потенциальных отобранных: 55,782\n",
      "Обработано: 11,200,000 | Каналов с разрешенной категорией: 1,663 | Потенциальных отобранных: 56,126\n",
      "Обработано: 11,300,000 | Каналов с разрешенной категорией: 1,680 | Потенциальных отобранных: 56,209\n",
      "Обработано: 11,400,000 | Каналов с разрешенной категорией: 1,694 | Потенциальных отобранных: 56,275\n",
      "Обработано: 11,800,000 | Каналов с разрешенной категорией: 1,745 | Потенциальных отобранных: 56,518\n",
      "Обработано: 12,400,000 | Каналов с разрешенной категорией: 1,841 | Потенциальных отобранных: 56,969\n",
      "Промежуточный отбор постов из середины рейтинга...\n",
      "ZIP удалён: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.17.zip\n",
      "Статистика после обработки tg.17.zip:\n",
      "  - Обработано записей: 13,052,392\n",
      "  - Отобрано постов всего: 545,446\n",
      "  - Каналов с разрешенной категорией: 1,951\n",
      "\n",
      "=== tg.18.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d776c7b531584869a4491b3b3c261202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.18.zip:   0%|          | 0.00/733M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачано: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.18.zip\n",
      "Внутри ZIP файлов: 895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28161abd14c412eb34347951e415ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Внутренние файлы:   0%|          | 0/895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано: 13,800,000 | Каналов с разрешенной категорией: 2,087 | Потенциальных отобранных: 58,156\n",
      "Обработано: 14,200,000 | Каналов с разрешенной категорией: 2,149 | Потенциальных отобранных: 58,453\n",
      "Обработано: 14,500,000 | Каналов с разрешенной категорией: 2,200 | Потенциальных отобранных: 58,695\n",
      "Обработано: 17,200,000 | Каналов с разрешенной категорией: 2,523 | Потенциальных отобранных: 60,265\n",
      "Промежуточный отбор постов из середины рейтинга...\n",
      "ZIP удалён: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.18.zip\n",
      "Статистика после обработки tg.18.zip:\n",
      "  - Обработано записей: 17,556,368\n",
      "  - Отобрано постов всего: 606,116\n",
      "  - Каналов с разрешенной категорией: 2,609\n",
      "\n",
      "=== tg.19.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71dd5d051ad466c98c63837f6f55d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.19.zip:   0%|          | 0.00/673M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачано: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.19.zip\n",
      "Внутри ZIP файлов: 910\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08599fc2d7749a18aee94c549eb3b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Внутренние файлы:   0%|          | 0/910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано: 18,100,000 | Каналов с разрешенной категорией: 2,725 | Потенциальных отобранных: 61,215\n",
      "Обработано: 19,100,000 | Каналов с разрешенной категорией: 2,937 | Потенциальных отобранных: 62,232\n",
      "Промежуточный отбор постов из середины рейтинга...\n",
      "ZIP удалён: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.19.zip\n",
      "Статистика после обработки tg.19.zip:\n",
      "  - Обработано записей: 21,313,121\n",
      "  - Отобрано постов всего: 670,000\n",
      "  - Каналов с разрешенной категорией: 3,282\n",
      "\n",
      "=== tg.20.zip ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2985fd6d7e19430aafc8b0d500a131ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tg.20.zip:   0%|          | 0.00/777M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачано: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.20.zip\n",
      "Внутри ZIP файлов: 893\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a414af325d44069ba67b921cf0fca8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Внутренние файлы:   0%|          | 0/893 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано: 23,200,000 | Каналов с разрешенной категорией: 3,603 | Потенциальных отобранных: 65,417\n",
      "Обработано: 24,000,000 | Каналов с разрешенной категорией: 3,722 | Потенциальных отобранных: 65,984\n",
      "Обработано: 24,100,000 | Каналов с разрешенной категорией: 3,741 | Потенциальных отобранных: 66,073\n",
      "Промежуточный отбор постов из середины рейтинга...\n",
      "ZIP удалён: data/posts/ajtkulov/selected\\cache\\datasets--ajtkulov--telegram-ru\\snapshots\\e7c2668f8ffe8d7b9725d4639d3f9e96a25a58b4\\data\\tg.20.zip\n",
      "Статистика после обработки tg.20.zip:\n",
      "  - Обработано записей: 25,348,581\n",
      "  - Отобрано постов всего: 737,025\n",
      "  - Каналов с разрешенной категорией: 3,937\n",
      "Достигнут глобальный лимит в 700,000 записей — завершаем обработку\n",
      "\n",
      "Финальный отбор постов...\n",
      "Всего каналов с разрешенной категорией: 13,773\n",
      "Уже отобрано постов: 737,025\n",
      "Ограничиваем количество постов с 737,025 до 700,000\n",
      "\n",
      "Финальное количество постов: 700,000\n",
      "Всего обработано записей: 25,348,581\n",
      "Финальный CSV: data/posts/ajtkulov/selected\\selected_posts.csv (700,000 строк)\n",
      "Финальный JSONL: data/posts/ajtkulov/selected\\selected_posts.jsonl (700,000 записей)\n",
      "Форматированный JSON: data/posts/ajtkulov/selected\\selected_posts_pretty.json\n",
      "Компактный JSON: data/posts/ajtkulov/selected\\selected_posts.json (700,000 записей)\n",
      "Статистика обработки: data/posts/ajtkulov/selected\\processing_summary.json\n",
      "Временный файл удалён: data/posts/ajtkulov/selected\\selected_posts_temp.jsonl\n",
      "\n",
      "Готово!\n"
     ]
    }
   ],
   "source": [
    "# Желаемое количество постов на канал\n",
    "MAX_POSTS_PER_CHANNEL = 5\n",
    "\n",
    "# Фильтр по длине текста\n",
    "MIN_TEXT_LEN = 500\n",
    "MAX_TEXT_LEN = 1200\n",
    "\n",
    "# Глобальный лимит записей (не превысим)\n",
    "MAX_TOTAL_RECORDS = 700000\n",
    "\n",
    "# Список ZIP-файлов (0–112, но можно ограничить)\n",
    "file_names = [f\"tg.{i}.zip\" for i in range(230)]\n",
    "# file_names = file_names[:5]  # ← для теста\n",
    "\n",
    "# ==============================================\n",
    "# Вспомогательные структуры\n",
    "# ==============================================\n",
    "\n",
    "# channel → list of (views, text, obj) — храним только топ-5 по просмотрам\n",
    "channel_top_posts = {}\n",
    "\n",
    "# Кэш проверенных каналов (чтобы не проверять категорию каждый раз)\n",
    "verified_channels_cache = {}\n",
    "\n",
    "# Функция проверки, проходит ли канал по категории\n",
    "def is_channel_allowed(channel_name):\n",
    "    if channel_name in verified_channels_cache:\n",
    "        return verified_channels_cache[channel_name]\n",
    "    \n",
    "    # Получаем категорию канала\n",
    "    category = channel_to_category.get(channel_name)\n",
    "    \n",
    "    # Если категория не найдена или пустая — пропускаем\n",
    "    if not category or pd.isna(category):\n",
    "        verified_channels_cache[channel_name] = False\n",
    "        return False\n",
    "    \n",
    "    # Проверяем, находится ли категория в разрешенных\n",
    "    is_allowed = str(category) in allowed_categories\n",
    "    verified_channels_cache[channel_name] = is_allowed\n",
    "    \n",
    "    return is_allowed\n",
    "\n",
    "# Функция для выбора постов \"из серединки\"\n",
    "def select_mid_range_posts(post_list, num_to_select=5):\n",
    "    \"\"\"\n",
    "    Выбирает посты из середины отсортированного списка.\n",
    "    Не берет ни самые популярные, ни самые непопулярные.\n",
    "    \"\"\"\n",
    "    if not post_list or num_to_select <= 0:\n",
    "        return []\n",
    "    \n",
    "    # Сортируем по просмотрам\n",
    "    post_list.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Если постов меньше или равно нужному количеству - берем все\n",
    "    if len(post_list) <= num_to_select:\n",
    "        return [obj for _, _, obj in post_list]\n",
    "    \n",
    "    # Вычисляем позиции для выбора из середины\n",
    "    total_posts = len(post_list)\n",
    "    \n",
    "    # Стратегия 1: берем равномерно из середины (пропускаем топ и низ)\n",
    "    # Например, для 5 постов из 20: берем позиции 5, 8, 11, 14, 17\n",
    "    if total_posts >= num_to_select * 4:\n",
    "        # Много постов - берем равномерно из середины\n",
    "        step = total_posts // (num_to_select + 1)\n",
    "        start_idx = step\n",
    "        indices = [start_idx + i*step for i in range(num_to_select)]\n",
    "    else:\n",
    "        # Мало постов - берем просто середину\n",
    "        start_idx = max(1, (total_posts - num_to_select) // 2)\n",
    "        indices = list(range(start_idx, start_idx + num_to_select))\n",
    "    \n",
    "    # Берем посты по вычисленным индексам\n",
    "    selected = []\n",
    "    for idx in indices:\n",
    "        if idx < len(post_list):\n",
    "            _, _, obj = post_list[idx]\n",
    "            selected.append(obj)\n",
    "    \n",
    "    return selected\n",
    "\n",
    "# Загрузка существующей статистики, если есть\n",
    "if os.path.exists(stats_file):\n",
    "    with open(stats_file, 'r', encoding='utf-8') as f:\n",
    "        channel_top_posts = json.load(f)\n",
    "    print(f\"Загружено статистики по {len(channel_top_posts):,} каналам\")\n",
    "\n",
    "# Загрузка кэша проверенных каналов\n",
    "cache_file = os.path.join(output_dir, \"verified_channels_cache.json\")\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "        verified_channels_cache = json.load(f)\n",
    "    print(f\"Загружен кэш для {len(verified_channels_cache)} каналов\")\n",
    "\n",
    "# Загрузка уже отобранных постов, если есть\n",
    "selected_posts_count = 0\n",
    "if os.path.exists(selected_posts_file):\n",
    "    # Просто посчитаем количество строк\n",
    "    with open(selected_posts_file, 'r', encoding='utf-8') as f:\n",
    "        selected_posts_count = sum(1 for _ in f)\n",
    "    print(f\"Уже отобрано постов из предыдущих запусков: {selected_posts_count:,}\")\n",
    "\n",
    "# Прогресс: какие ZIP уже обработаны\n",
    "processed_files = set()\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as pf:\n",
    "        processed_files = set(json.load(pf))\n",
    "    print(f\"Уже обработано ZIP-файлов: {len(processed_files)}\")\n",
    "\n",
    "# ==============================================\n",
    "# Основной цикл: чтение и отбор\n",
    "# ==============================================\n",
    "\n",
    "total_processed = 0\n",
    "allowed_channels_count = 0\n",
    "\n",
    "for filename in tqdm(file_names, desc=\"Обработка ZIP-файлов\"):\n",
    "    if filename in processed_files:\n",
    "        print(f\"{filename} уже обработан — пропуск\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n=== {filename} ===\")\n",
    "    \n",
    "    try:\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=f\"data/{filename}\",\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        print(f\"Скачано: {file_path}\")\n",
    "        \n",
    "        # Открываем файл для добавления отобранных постов (режим 'a' - append)\n",
    "        with open(selected_posts_file, 'a', encoding='utf-8') as spf:\n",
    "            with zipfile.ZipFile(file_path, 'r') as z:\n",
    "                inner_files = [f for f in z.namelist() if not f.endswith('/') and not f.endswith('.done')]\n",
    "                print(f\"Внутри ZIP файлов: {len(inner_files)}\")\n",
    "                \n",
    "                for inner_path in tqdm(inner_files, desc=\"Внутренние файлы\", leave=False):\n",
    "                    with z.open(inner_path) as f:\n",
    "                        for line in TextIOWrapper(f, encoding='utf-8', errors='ignore'):\n",
    "                            line = line.strip()\n",
    "                            if not line: continue\n",
    "                            \n",
    "                            total_processed += 1\n",
    "                            \n",
    "                            try:\n",
    "                                obj = json.loads(line)\n",
    "                                channel = obj.get('channel')\n",
    "                                text = obj.get('text', '')\n",
    "                                views = obj.get('views', 0)\n",
    "                                \n",
    "                                # Конвертируем views в число\n",
    "                                if isinstance(views, str):\n",
    "                                    if 'K' in views:\n",
    "                                        views = float(views.replace('K', '').replace(',', '.')) * 1000\n",
    "                                    elif 'M' in views:\n",
    "                                        views = float(views.replace('M', '').replace(',', '.')) * 1000000\n",
    "                                    else:\n",
    "                                        try:\n",
    "                                            views = float(views.replace(',', ''))\n",
    "                                        except:\n",
    "                                            views = 0\n",
    "                                \n",
    "                                if not isinstance(views, (int, float)):\n",
    "                                    views = 0\n",
    "                                \n",
    "                                if not channel or not text:\n",
    "                                    continue\n",
    "                                \n",
    "                                # Фильтр по длине\n",
    "                                text_len = len(text)\n",
    "                                if not (MIN_TEXT_LEN <= text_len <= MAX_TEXT_LEN):\n",
    "                                    continue\n",
    "                                \n",
    "                                # Фильтр по категории канала\n",
    "                                if not is_channel_allowed(channel):\n",
    "                                    continue\n",
    "                                \n",
    "                                # Сохраняем пост для канала\n",
    "                                if channel not in channel_top_posts:\n",
    "                                    channel_top_posts[channel] = []\n",
    "                                    allowed_channels_count += 1\n",
    "                                \n",
    "                                channel_top_posts[channel].append((views, text, obj))\n",
    "                                \n",
    "                                # Если у канала слишком много постов, оставляем только топ-N*2\n",
    "                                if len(channel_top_posts[channel]) > MAX_POSTS_PER_CHANNEL * 4:  # увеличен буфер\n",
    "                                    channel_top_posts[channel].sort(key=lambda x: x[0], reverse=True)\n",
    "                                    channel_top_posts[channel] = channel_top_posts[channel][:MAX_POSTS_PER_CHANNEL * 4]\n",
    "                                \n",
    "                                # Периодический вывод статистики\n",
    "                                if total_processed % 100000 == 0:\n",
    "                                    # Подсчитываем текущее количество отобранных постов\n",
    "                                    current_selected = sum(min(MAX_POSTS_PER_CHANNEL, len(posts)) for posts in channel_top_posts.values())\n",
    "                                    print(f\"Обработано: {total_processed:,} | Каналов с разрешенной категорией: {allowed_channels_count:,} | Потенциальных отобранных: {current_selected:,}\")\n",
    "                                \n",
    "                                if len(channel_top_posts) > 0:\n",
    "                                    # Подсчитываем примерное количество отобранных постов\n",
    "                                    estimated_selected = sum(min(MAX_POSTS_PER_CHANNEL, len(posts)) for posts in channel_top_posts.values())\n",
    "                                    if estimated_selected >= MAX_TOTAL_RECORDS:\n",
    "                                        print(f\"Достигнут глобальный лимит ({estimated_selected:,} записей) — прерываем\")\n",
    "                                        raise StopIteration\n",
    "                                        \n",
    "                            except json.JSONDecodeError:\n",
    "                                continue\n",
    "                            except Exception as e:\n",
    "                                continue\n",
    "        \n",
    "        # После обработки каждого ZIP — проводим промежуточный отбор\n",
    "        print(\"Промежуточный отбор постов из середины рейтинга...\")\n",
    "        \n",
    "        # Отбираем посты из середины для каждого канала\n",
    "        intermediate_selected = []\n",
    "        for channel, post_list in channel_top_posts.items():\n",
    "            if not post_list:\n",
    "                continue\n",
    "            \n",
    "            # Используем новую функцию для отбора из середины\n",
    "            selected_for_channel = select_mid_range_posts(post_list, MAX_POSTS_PER_CHANNEL)\n",
    "            intermediate_selected.extend(selected_for_channel)\n",
    "            \n",
    "            # Останавливаемся при достижении лимита\n",
    "            if len(intermediate_selected) >= MAX_TOTAL_RECORDS:\n",
    "                intermediate_selected = intermediate_selected[:MAX_TOTAL_RECORDS]\n",
    "                break\n",
    "        \n",
    "        # Сохраняем отобранные посты в файл\n",
    "        with open(selected_posts_file, 'a', encoding='utf-8') as spf:\n",
    "            for post in intermediate_selected:\n",
    "                spf.write(json.dumps(post, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # Обновляем счетчик отобранных постов\n",
    "        selected_posts_count += len(intermediate_selected)\n",
    "        \n",
    "        # Сохраняем прогресс\n",
    "        processed_files.add(filename)\n",
    "        with open(progress_file, 'w', encoding='utf-8') as pf:\n",
    "            json.dump(list(processed_files), pf, indent=2)\n",
    "        \n",
    "        # Сохраняем промежуточную статистику топ-постов\n",
    "        with open(stats_file, 'w', encoding='utf-8') as sf:\n",
    "            json.dump(channel_top_posts, sf, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Сохраняем кэш проверенных каналов\n",
    "        with open(cache_file, 'w', encoding='utf-8') as cf:\n",
    "            json.dump(verified_channels_cache, cf, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Удаляем ZIP, чтобы не занимать место\n",
    "        os.remove(file_path)\n",
    "        print(f\"ZIP удалён: {file_path}\")\n",
    "        \n",
    "        print(f\"Статистика после обработки {filename}:\")\n",
    "        print(f\"  - Обработано записей: {total_processed:,}\")\n",
    "        print(f\"  - Отобрано постов всего: {selected_posts_count:,}\")\n",
    "        print(f\"  - Каналов с разрешенной категорией: {allowed_channels_count:,}\")\n",
    "        \n",
    "        # Проверяем лимит\n",
    "        if selected_posts_count >= MAX_TOTAL_RECORDS:\n",
    "            print(f\"Достигнут глобальный лимит в {MAX_TOTAL_RECORDS:,} записей — завершаем обработку\")\n",
    "            break\n",
    "        \n",
    "    except StopIteration:\n",
    "        print(\"Обработка прервана по достижению лимита\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка {filename}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ==============================================\n",
    "# Финальный отбор и сохранение результатов\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\nФинальный отбор постов...\")\n",
    "print(f\"Всего каналов с разрешенной категорией: {len(channel_top_posts):,}\")\n",
    "print(f\"Уже отобрано постов: {selected_posts_count:,}\")\n",
    "\n",
    "# Загружаем все отобранные посты из временного файла\n",
    "selected_posts = []\n",
    "if os.path.exists(selected_posts_file):\n",
    "    with open(selected_posts_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    selected_posts.append(json.loads(line))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "# Если нужно ограничить по лимиту (на случай если превысили)\n",
    "if len(selected_posts) > MAX_TOTAL_RECORDS:\n",
    "    print(f\"Ограничиваем количество постов с {len(selected_posts):,} до {MAX_TOTAL_RECORDS:,}\")\n",
    "    selected_posts = selected_posts[:MAX_TOTAL_RECORDS]\n",
    "\n",
    "print(f\"\\nФинальное количество постов: {len(selected_posts):,}\")\n",
    "print(f\"Всего обработано записей: {total_processed:,}\")\n",
    "\n",
    "# ==============================================\n",
    "# Сохранение финального результата\n",
    "# ==============================================\n",
    "\n",
    "if selected_posts:\n",
    "    df_final = pd.DataFrame(selected_posts)\n",
    "    \n",
    "    # Сохраняем в CSV\n",
    "    csv_path = os.path.join(output_dir, \"selected_posts.csv\")\n",
    "    df_final.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Финальный CSV: {csv_path} ({len(df_final):,} строк)\")\n",
    "    \n",
    "    # Сохраняем в JSONL (одна строка = один пост)\n",
    "    jsonl_path = os.path.join(output_dir, \"selected_posts.jsonl\")\n",
    "    with open(jsonl_path, 'w', encoding='utf-8') as jf:\n",
    "        for post in selected_posts:\n",
    "            jf.write(json.dumps(post, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Финальный JSONL: {jsonl_path} ({len(selected_posts):,} записей)\")\n",
    "    \n",
    "    # Сохраняем в ДВА JSON формата\n",
    "    \n",
    "    # 1. Красивый JSON с отступами (для чтения)\n",
    "    json_path_pretty = os.path.join(output_dir, \"selected_posts_pretty.json\")\n",
    "    with open(json_path_pretty, 'w', encoding='utf-8') as jf:\n",
    "        json.dump(selected_posts, jf, ensure_ascii=False, indent=2)\n",
    "    print(f\"Форматированный JSON: {json_path_pretty}\")\n",
    "    \n",
    "    # 2. Обычный JSON без отступов (одна строка, компактный)\n",
    "    json_path_compact = os.path.join(output_dir, \"selected_posts.json\")\n",
    "    with open(json_path_compact, 'w', encoding='utf-8') as jf:\n",
    "        json.dump(selected_posts, jf, ensure_ascii=False)\n",
    "    print(f\"Компактный JSON: {json_path_compact} ({len(selected_posts):,} записей)\")\n",
    "    \n",
    "    # Сохраняем статистику обработки\n",
    "    stats_summary = {\n",
    "        \"total_selected_posts\": len(selected_posts),\n",
    "        \"total_processed_records\": total_processed,\n",
    "        \"total_allowed_channels\": len(channel_top_posts),\n",
    "        \"max_posts_per_channel\": MAX_POSTS_PER_CHANNEL,\n",
    "        \"min_text_length\": MIN_TEXT_LEN,\n",
    "        \"max_text_length\": MAX_TEXT_LEN,\n",
    "        \"max_total_records_limit\": MAX_TOTAL_RECORDS,\n",
    "        \"allowed_categories_count\": len(allowed_categories),\n",
    "        \"processed_zip_files\": len(processed_files),\n",
    "        \"selection_strategy\": \"mid_range (not top, not bottom)\",\n",
    "        \"estimated_memory_saved_MB\": (total_processed - len(selected_posts)) * 0.5 / 1024  # примерная оценка\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(output_dir, \"processing_summary.json\")\n",
    "    with open(summary_path, 'w', encoding='utf-8') as sf:\n",
    "        json.dump(stats_summary, sf, ensure_ascii=False, indent=2)\n",
    "    print(f\"Статистика обработки: {summary_path}\")\n",
    "    \n",
    "    # Удаляем временный файл\n",
    "    if os.path.exists(selected_posts_file):\n",
    "        os.remove(selected_posts_file)\n",
    "        print(f\"Временный файл удалён: {selected_posts_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Не удалось отобрать ни одного поста\")\n",
    "\n",
    "print(\"\\nГотово!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
